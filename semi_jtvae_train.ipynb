{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "gradient": {
     "editing": false,
     "id": "18de9aeb-6551-42f6-8c11-a0e30bd207d6",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "JDinHdioUZRH",
    "outputId": "1a824742-d528-46a2-8d89-7697a166c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu116.html\n",
    "!pip install -q dive-into-graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q toolz\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f728113a-ff43-466d-b34d-77c9b2e11478",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "_RKgd8MsYOYh"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pickle \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from molecule_optimizer.externals.fast_jtnn.datautils import SemiMolTreeFolder, SemiMolTreeFolderTest\n",
    "from molecule_optimizer.runner.semi_jtvae import SemiJTVAEGeneratorPredictor\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "\n",
    "import rdkit\n",
    "\n",
    "lg = rdkit.RDLogger.logger() \n",
    "lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6e85f4e2-eab6-4eae-b452-7f089e176039",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "2C4erValhPS-"
   },
   "outputs": [],
   "source": [
    "conf = json.load(open(\"training/configs/rand_gen_zinc250k_config_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": true,
     "id": "30eb7f71-c38e-49f6-8d84-715ff3d80e08",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "SuzGX7ClhKaf",
    "outputId": "ab764f6b-4dd7-4ffc-b6cf-bae47d6cedf7"
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"ZINC_310k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = csv['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiles = smiles[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torch.tensor(csv['LogP'][:60000]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(csv['LogP']).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_TEST = 10000\n",
    "N_TEST = 200\n",
    "VAL_FRAC = 0.05\n",
    "chem_prop = \"LogP\"\n",
    "load_epoch = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:57<00:00,  1.89s/it]\n",
      "100%|██████████| 62/62 [1:06:51<00:00, 64.70s/it] \n"
     ]
    }
   ],
   "source": [
    "# if 'runner.xml' not in os.listdir(\".\"):\n",
    "#     runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "#     processed_smiles, processed_idxs = SemiJTVAEGeneratorPredictor.preprocess(smiles) \n",
    "#     with open('runner.xml', 'wb') as f:\n",
    "#         pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:35<00:00,  2.93s/it]\n",
      "100%|██████████| 12/12 [19:09<00:00, 95.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# if 'runner_20.xml' not in os.listdir(\".\"):\n",
    "#     runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "#     processed_smiles, processed_idxs = SemiJTVAEGeneratorPredictor.preprocess(smiles) \n",
    "#     with open('runner_20.xml', 'wb') as f:\n",
    "#         pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fc02ae3d-696b-4c2e-b557-760e6a1a75a9",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "uTNMpfWD7b7Q"
   },
   "outputs": [],
   "source": [
    "with open('saved/runner_LogP_50_1_iter_5000.xml', 'rb') as f:\n",
    "    runner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "4ccc9136-0e87-470b-90eb-8e0b92da52bc",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "0-1xXVU15z6N",
    "outputId": "c6c328fe-72e4-4401-bce6-8613e7e9319f"
   },
   "outputs": [],
   "source": [
    "runner.get_model(\n",
    "    \"rand_gen\",\n",
    "    {\n",
    "        \"hidden_size\": conf[\"model\"][\"hidden_size\"],\n",
    "        \"latent_size\": conf[\"model\"][\"latent_size\"],\n",
    "        \"depthT\": conf[\"model\"][\"depthT\"],\n",
    "        \"depthG\": conf[\"model\"][\"depthG\"],\n",
    "        \"label_size\": 1,\n",
    "        \"label_mean\": float(torch.mean(labels)),\n",
    "        \"label_var\": float(torch.var(labels)),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c177219a-298a-4994-98b9-de76833e14fe",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "IGnpfkM_KQXi"
   },
   "outputs": [],
   "source": [
    "labels = runner.get_processed_labels(labels, processed_idxs)\n",
    "preprocessed = processed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perm_id=np.random.permutation(len(labels))\n",
    "\n",
    "X_train = preprocessed[perm_id[N_TEST:]]\n",
    "X_train_smiles = smiles[perm_id[N_TEST:]]\n",
    "L_train = torch.tensor(labels.numpy()[perm_id[N_TEST:]])\n",
    "\n",
    "\n",
    "X_test = preprocessed[perm_id[:N_TEST]]\n",
    "X_test_smiles = smiles[perm_id[:N_TEST]]\n",
    "L_test = torch.tensor(labels.numpy()[perm_id[:N_TEST]])\n",
    "\n",
    "val_cut = math.floor(len(X_train) * VAL_FRAC)\n",
    "\n",
    "X_Val = X_train[:val_cut]\n",
    "X_Val_smiles = X_train_smiles[:val_cut]\n",
    "L_Val = L_train[:val_cut]\n",
    "\n",
    "X_train = X_train[val_cut :]\n",
    "X_train_smiles = X_train_smiles[val_cut :]\n",
    "L_train = L_train[val_cut :]\n",
    "\n",
    "with open(\"train_smiles_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_train_smiles)\n",
    "\n",
    "with open(\"test_smiles_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_test_smiles)\n",
    "\n",
    "with open(\"validation_smiles_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_Val_smiles)\n",
    "\n",
    "#save preproccessed\n",
    "\n",
    "with open(\"train_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "\n",
    "with open(\"test_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_test)\n",
    "\n",
    "with open(\"validation_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_Val)\n",
    "\n",
    "#Save labels\n",
    "\n",
    "torch.save(L_train, \"L_train_\" + chem_prop + \"_50_1.pt\")\n",
    "\n",
    "torch.save(L_test, \"L_test_\" + chem_prop + \"_50_1.pt\")\n",
    "\n",
    "torch.save(L_Val, \"L_Val_\" + chem_prop + \"_50_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train = torch.load(\"L_train_\" + chem_prop + \"_50_1.pt\")\n",
    "L_test = torch.load(\"L_test_\" + chem_prop + \"_50_1.pt\")\n",
    "L_Val = torch.load(\"L_Val_\" + chem_prop + \"_50_1.pt\")\n",
    "\n",
    "with open(\"train_\" + chem_prop + \"_50_1.npy\", 'rb') as f:\n",
    "    X_train = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(\"test_\" + chem_prop + \"_50_1.npy\", 'rb') as f:\n",
    "    X_test = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(\"validation_\" + chem_prop + \"_50_1.npy\", 'rb') as f:\n",
    "    X_Val = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7cb9e705-09fa-4075-a487-21887ecaeb95",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "TMxgCK1Y20mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model #Params: 4732K\n",
      "[Train][1100] Alpha: 0.000, Beta: 0.000, Loss: 40.79, KL: 679.81, MAE: 0.85188, Word Loss: 28.13, Topo Loss: 7.37, Assm Loss: 5.29, Pred Loss: 0.84, Word: 75.06, Topo: 94.72, Assm: 75.16, PNorm: 153.08, GNorm: 47.52\n",
      "[Train][1200] Alpha: 0.000, Beta: 0.000, Loss: 39.47, KL: 717.46, MAE: 0.84108, Word Loss: 27.26, Topo Loss: 6.95, Assm Loss: 5.26, Pred Loss: 0.81, Word: 76.19, Topo: 95.11, Assm: 75.32, PNorm: 157.58, GNorm: 35.72\n",
      "[Train][1300] Alpha: 0.000, Beta: 0.000, Loss: 37.01, KL: 750.64, MAE: 0.85575, Word Loss: 25.91, Topo Loss: 6.35, Assm Loss: 4.75, Pred Loss: 0.83, Word: 76.89, Topo: 95.61, Assm: 77.72, PNorm: 161.90, GNorm: 50.00\n",
      "[Train][1400] Alpha: 0.000, Beta: 0.000, Loss: 36.21, KL: 796.35, MAE: 0.81510, Word Loss: 25.03, Topo Loss: 6.43, Assm Loss: 4.75, Pred Loss: 0.78, Word: 77.64, Topo: 95.35, Assm: 77.52, PNorm: 164.86, GNorm: 42.02\n",
      "[Train][1500] Alpha: 0.000, Beta: 0.000, Loss: 34.63, KL: 834.32, MAE: 0.76559, Word Loss: 23.85, Topo Loss: 6.05, Assm Loss: 4.73, Pred Loss: 0.67, Word: 78.38, Topo: 95.80, Assm: 77.69, PNorm: 168.37, GNorm: 42.75\n",
      "[Train][1600] Alpha: 0.000, Beta: 0.000, Loss: 34.58, KL: 861.93, MAE: 0.81207, Word Loss: 23.79, Topo Loss: 6.16, Assm Loss: 4.63, Pred Loss: 0.80, Word: 78.49, Topo: 95.71, Assm: 79.40, PNorm: 170.91, GNorm: 49.97\n",
      "[Train][1700] Alpha: 0.000, Beta: 0.000, Loss: 33.73, KL: 893.15, MAE: 0.80064, Word Loss: 23.19, Topo Loss: 5.97, Assm Loss: 4.56, Pred Loss: 0.74, Word: 79.25, Topo: 95.82, Assm: 78.87, PNorm: 173.61, GNorm: 50.00\n",
      "[Train][1800] Alpha: 0.000, Beta: 0.000, Loss: 32.95, KL: 914.03, MAE: 0.78595, Word Loss: 22.84, Topo Loss: 5.81, Assm Loss: 4.30, Pred Loss: 0.72, Word: 79.55, Topo: 95.93, Assm: 79.65, PNorm: 176.19, GNorm: 50.00\n",
      "[Train][1900] Alpha: 0.000, Beta: 0.000, Loss: 31.73, KL: 948.40, MAE: 0.75512, Word Loss: 21.94, Topo Loss: 5.57, Assm Loss: 4.22, Pred Loss: 0.66, Word: 80.15, Topo: 96.18, Assm: 80.32, PNorm: 178.73, GNorm: 50.00\n",
      "[Train][2000] Alpha: 0.000, Beta: 0.000, Loss: 31.59, KL: 947.83, MAE: 0.81451, Word Loss: 21.71, Topo Loss: 5.49, Assm Loss: 4.39, Pred Loss: 0.75, Word: 80.62, Topo: 96.14, Assm: 80.13, PNorm: 181.34, GNorm: 50.00\n",
      "[Train][2100] Alpha: 0.000, Beta: 0.000, Loss: 29.78, KL: 991.44, MAE: 0.82339, Word Loss: 20.49, Topo Loss: 5.33, Assm Loss: 3.96, Pred Loss: 0.78, Word: 81.06, Topo: 96.22, Assm: 81.89, PNorm: 183.97, GNorm: 50.00\n",
      "[Train][2200] Alpha: 0.000, Beta: 0.000, Loss: 29.85, KL: 1008.41, MAE: 0.84411, Word Loss: 20.66, Topo Loss: 5.21, Assm Loss: 3.98, Pred Loss: 0.81, Word: 81.06, Topo: 96.35, Assm: 81.42, PNorm: 186.66, GNorm: 49.27\n",
      "[Train][2300] Alpha: 0.000, Beta: 0.000, Loss: 27.74, KL: 1033.64, MAE: 0.84545, Word Loss: 19.35, Topo Loss: 4.81, Assm Loss: 3.59, Pred Loss: 0.84, Word: 81.95, Topo: 96.71, Assm: 83.60, PNorm: 188.94, GNorm: 50.00\n",
      "[Train][2400] Alpha: 0.000, Beta: 0.000, Loss: 28.53, KL: 1053.37, MAE: 0.76251, Word Loss: 19.47, Topo Loss: 5.22, Assm Loss: 3.85, Pred Loss: 0.67, Word: 82.29, Topo: 96.35, Assm: 81.99, PNorm: 191.03, GNorm: 50.00\n",
      "[Train][2500] Alpha: 0.000, Beta: 0.000, Loss: 27.39, KL: 1088.39, MAE: 0.79701, Word Loss: 18.73, Topo Loss: 4.73, Assm Loss: 3.94, Pred Loss: 0.74, Word: 82.46, Topo: 96.75, Assm: 82.32, PNorm: 193.14, GNorm: 50.00\n"
     ]
    }
   ],
   "source": [
    "# print(\"Training model...\")\n",
    "# runner.train_gen_pred(\n",
    "#     X_train,\n",
    "#     L_train,\n",
    "#     X_test,\n",
    "#     L_test,\n",
    "#     X_Val,\n",
    "#     L_Val,\n",
    "#     load_epoch= 1000,\n",
    "#     lr=conf[\"lr\"],\n",
    "#     anneal_rate=conf[\"anneal_rate\"],\n",
    "#     clip_norm=conf[\"clip_norm\"],\n",
    "#     num_epochs=conf[\"num_epochs\"],\n",
    "#     alpha=conf[\"alpha\"],\n",
    "#     max_alpha=conf[\"max_alpha\"],\n",
    "#     step_alpha=conf[\"step_alpha\"],\n",
    "#     beta=conf[\"beta\"],\n",
    "#     max_beta=conf[\"max_beta\"],\n",
    "#     step_beta=conf[\"step_beta\"],\n",
    "#     anneal_iter=conf[\"anneal_iter\"],\n",
    "#     alpha_anneal_iter=conf[\"alpha_anneal_iter\"],\n",
    "#     kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "#     print_iter=100,\n",
    "#     save_iter= 1000,\n",
    "#     batch_size=conf[\"batch_size\"],\n",
    "#     num_workers=conf[\"num_workers\"],\n",
    "#     label_pct=0.5,\n",
    "#     chem_prop = \"LogP\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Initializing...\n",
      "Model #Params: 5207K\n",
      "[Train][100] Alpha: 0.000, Beta: 0.000, Loss: 61.27, KL: 185.61, MAE: 1.53639, Word Loss: 45.94, Topo Loss: 10.87, Assm Loss: 4.46, Pred Loss: 2.62, Word: 28.95, Topo: 83.15, Assm: 55.49, PNorm: 103.46, GNorm: 44.32\n",
      "[Train][200] Alpha: 0.000, Beta: 0.000, Loss: 38.47, KL: 169.67, MAE: 1.59289, Word Loss: 28.09, Topo Loss: 6.18, Assm Loss: 4.20, Pred Loss: 2.89, Word: 51.02, Topo: 91.55, Assm: 58.88, PNorm: 107.94, GNorm: 40.76\n",
      "[Train][300] Alpha: 0.000, Beta: 0.000, Loss: 33.83, KL: 172.82, MAE: 1.63952, Word Loss: 24.30, Topo Loss: 5.48, Assm Loss: 4.05, Pred Loss: 2.99, Word: 58.74, Topo: 92.44, Assm: 61.21, PNorm: 111.25, GNorm: 23.04\n",
      "[Train][400] Alpha: 0.000, Beta: 0.000, Loss: 30.26, KL: 199.53, MAE: 1.65205, Word Loss: 21.46, Topo Loss: 4.89, Assm Loss: 3.90, Pred Loss: 3.00, Word: 63.16, Topo: 93.10, Assm: 61.72, PNorm: 114.16, GNorm: 28.80\n",
      "[Train][500] Alpha: 0.000, Beta: 0.000, Loss: 28.41, KL: 214.00, MAE: 1.74623, Word Loss: 20.11, Topo Loss: 4.52, Assm Loss: 3.78, Pred Loss: 3.37, Word: 66.09, Topo: 93.60, Assm: 61.95, PNorm: 116.98, GNorm: 35.25\n",
      "[Train][600] Alpha: 0.000, Beta: 0.000, Loss: 26.85, KL: 239.11, MAE: 1.82902, Word Loss: 18.83, Topo Loss: 4.22, Assm Loss: 3.80, Pred Loss: 3.63, Word: 67.33, Topo: 93.90, Assm: 63.35, PNorm: 119.78, GNorm: 21.73\n",
      "[Train][700] Alpha: 0.000, Beta: 0.000, Loss: 25.03, KL: 263.92, MAE: 2.05277, Word Loss: 17.57, Topo Loss: 4.07, Assm Loss: 3.39, Pred Loss: 4.23, Word: 69.38, Topo: 94.27, Assm: 65.55, PNorm: 122.49, GNorm: 28.30\n",
      "[Train][800] Alpha: 0.000, Beta: 0.000, Loss: 24.45, KL: 273.61, MAE: 2.01037, Word Loss: 17.27, Topo Loss: 4.02, Assm Loss: 3.16, Pred Loss: 4.17, Word: 70.48, Topo: 94.45, Assm: 67.62, PNorm: 124.99, GNorm: 37.93\n",
      "[Train][900] Alpha: 0.000, Beta: 0.000, Loss: 23.43, KL: 304.11, MAE: 2.05457, Word Loss: 16.36, Topo Loss: 3.94, Assm Loss: 3.13, Pred Loss: 4.38, Word: 71.89, Topo: 94.44, Assm: 69.39, PNorm: 127.35, GNorm: 28.81\n",
      "[Train][1000] Alpha: 0.000, Beta: 0.000, Loss: 22.02, KL: 319.22, MAE: 2.15335, Word Loss: 15.36, Topo Loss: 3.71, Assm Loss: 2.96, Pred Loss: 4.72, Word: 73.25, Topo: 94.90, Assm: 71.14, PNorm: 129.59, GNorm: 33.16\n",
      "[Train][1100] Alpha: 0.000, Beta: 0.000, Loss: 21.86, KL: 326.51, MAE: 2.05442, Word Loss: 15.28, Topo Loss: 3.77, Assm Loss: 2.81, Pred Loss: 4.33, Word: 73.51, Topo: 94.68, Assm: 73.54, PNorm: 131.89, GNorm: 35.55\n",
      "[Train][1200] Alpha: 0.000, Beta: 0.000, Loss: 20.83, KL: 329.36, MAE: 2.13046, Word Loss: 14.47, Topo Loss: 3.51, Assm Loss: 2.85, Pred Loss: 4.53, Word: 75.00, Topo: 94.99, Assm: 73.46, PNorm: 134.17, GNorm: 17.32\n",
      "[Train][1300] Alpha: 0.000, Beta: 0.000, Loss: 20.27, KL: 344.66, MAE: 2.26477, Word Loss: 14.09, Topo Loss: 3.48, Assm Loss: 2.70, Pred Loss: 5.10, Word: 75.23, Topo: 95.08, Assm: 74.43, PNorm: 136.33, GNorm: 38.46\n",
      "[Train][1400] Alpha: 0.000, Beta: 0.000, Loss: 21.02, KL: 339.48, MAE: 2.23218, Word Loss: 14.22, Topo Loss: 3.77, Assm Loss: 3.03, Pred Loss: 5.07, Word: 75.20, Topo: 95.15, Assm: 74.47, PNorm: 138.53, GNorm: 31.07\n",
      "[Train][1500] Alpha: 0.000, Beta: 0.000, Loss: 19.46, KL: 354.87, MAE: 2.12829, Word Loss: 13.46, Topo Loss: 3.30, Assm Loss: 2.70, Pred Loss: 4.59, Word: 76.46, Topo: 95.47, Assm: 74.60, PNorm: 140.52, GNorm: 23.33\n",
      "[Train][1600] Alpha: 0.000, Beta: 0.000, Loss: 18.98, KL: 361.07, MAE: 2.18611, Word Loss: 12.91, Topo Loss: 3.40, Assm Loss: 2.68, Pred Loss: 4.82, Word: 76.87, Topo: 95.12, Assm: 75.01, PNorm: 142.44, GNorm: 32.47\n",
      "[Train][1700] Alpha: 0.000, Beta: 0.000, Loss: 18.24, KL: 378.67, MAE: 2.34560, Word Loss: 12.34, Topo Loss: 3.31, Assm Loss: 2.58, Pred Loss: 5.42, Word: 78.05, Topo: 95.33, Assm: 75.36, PNorm: 144.36, GNorm: 19.12\n",
      "[Train][1800] Alpha: 0.000, Beta: 0.000, Loss: 17.43, KL: 383.91, MAE: 2.29407, Word Loss: 12.09, Topo Loss: 2.95, Assm Loss: 2.39, Pred Loss: 5.25, Word: 78.01, Topo: 95.94, Assm: 77.03, PNorm: 146.26, GNorm: 25.65\n",
      "[Train][1900] Alpha: 0.000, Beta: 0.000, Loss: 17.08, KL: 403.95, MAE: 2.44520, Word Loss: 11.84, Topo Loss: 2.88, Assm Loss: 2.36, Pred Loss: 5.87, Word: 78.38, Topo: 96.02, Assm: 77.19, PNorm: 148.03, GNorm: 29.24\n",
      "[Train][2000] Alpha: 0.000, Beta: 0.000, Loss: 16.94, KL: 422.69, MAE: 2.46448, Word Loss: 11.61, Topo Loss: 2.91, Assm Loss: 2.42, Pred Loss: 5.97, Word: 78.84, Topo: 96.00, Assm: 77.07, PNorm: 149.91, GNorm: 24.62\n",
      "[Train][2100] Alpha: 0.000, Beta: 0.000, Loss: 16.10, KL: 427.47, MAE: 2.46994, Word Loss: 11.01, Topo Loss: 2.77, Assm Loss: 2.32, Pred Loss: 6.12, Word: 79.93, Topo: 96.27, Assm: 78.37, PNorm: 151.65, GNorm: 24.70\n",
      "[Train][2200] Alpha: 0.000, Beta: 0.000, Loss: 15.34, KL: 445.95, MAE: 2.62029, Word Loss: 10.60, Topo Loss: 2.60, Assm Loss: 2.14, Pred Loss: 6.68, Word: 80.12, Topo: 96.35, Assm: 79.77, PNorm: 153.33, GNorm: 35.88\n",
      "[Train][2300] Alpha: 0.000, Beta: 0.000, Loss: 15.36, KL: 464.10, MAE: 2.66473, Word Loss: 10.62, Topo Loss: 2.67, Assm Loss: 2.08, Pred Loss: 6.88, Word: 80.21, Topo: 96.29, Assm: 80.39, PNorm: 155.07, GNorm: 32.50\n",
      "[Train][2400] Alpha: 0.000, Beta: 0.000, Loss: 15.46, KL: 468.33, MAE: 2.77981, Word Loss: 10.36, Topo Loss: 2.88, Assm Loss: 2.22, Pred Loss: 7.26, Word: 80.77, Topo: 96.19, Assm: 80.82, PNorm: 156.89, GNorm: 30.52\n",
      "[Train][2500] Alpha: 0.000, Beta: 0.000, Loss: 14.75, KL: 479.46, MAE: 2.85163, Word Loss: 10.13, Topo Loss: 2.49, Assm Loss: 2.13, Pred Loss: 7.52, Word: 81.49, Topo: 96.67, Assm: 81.58, PNorm: 158.61, GNorm: 20.24\n",
      "[Train][2600] Alpha: 0.000, Beta: 0.000, Loss: 14.37, KL: 503.13, MAE: 2.84286, Word Loss: 9.83, Topo Loss: 2.49, Assm Loss: 2.05, Pred Loss: 7.55, Word: 81.68, Topo: 96.55, Assm: 81.43, PNorm: 160.22, GNorm: 28.02\n",
      "[Train][2700] Alpha: 0.000, Beta: 0.000, Loss: 13.98, KL: 501.24, MAE: 2.94666, Word Loss: 9.62, Topo Loss: 2.46, Assm Loss: 1.89, Pred Loss: 8.02, Word: 82.02, Topo: 96.58, Assm: 82.53, PNorm: 161.82, GNorm: 30.09\n",
      "[Train][2800] Alpha: 0.000, Beta: 0.000, Loss: 13.85, KL: 514.66, MAE: 3.05835, Word Loss: 9.56, Topo Loss: 2.46, Assm Loss: 1.83, Pred Loss: 8.39, Word: 82.14, Topo: 96.63, Assm: 82.52, PNorm: 163.38, GNorm: 33.58\n",
      "[Train][2900] Alpha: 0.000, Beta: 0.000, Loss: 13.66, KL: 520.05, MAE: 3.02386, Word Loss: 9.38, Topo Loss: 2.38, Assm Loss: 1.90, Pred Loss: 8.36, Word: 82.08, Topo: 96.75, Assm: 82.78, PNorm: 164.84, GNorm: 26.00\n",
      "[Train][3000] Alpha: 0.000, Beta: 0.000, Loss: 14.00, KL: 525.68, MAE: 3.16365, Word Loss: 9.26, Topo Loss: 2.64, Assm Loss: 2.10, Pred Loss: 9.14, Word: 82.66, Topo: 96.58, Assm: 82.65, PNorm: 166.32, GNorm: 25.36\n",
      "[Train][3100] Alpha: 0.000, Beta: 0.000, Loss: 13.31, KL: 538.43, MAE: 3.13836, Word Loss: 9.10, Topo Loss: 2.37, Assm Loss: 1.83, Pred Loss: 8.86, Word: 82.70, Topo: 96.78, Assm: 83.44, PNorm: 167.81, GNorm: 20.87\n",
      "[Train][3200] Alpha: 0.000, Beta: 0.000, Loss: 12.66, KL: 551.48, MAE: 3.03987, Word Loss: 8.62, Topo Loss: 2.26, Assm Loss: 1.78, Pred Loss: 8.55, Word: 83.41, Topo: 96.94, Assm: 83.83, PNorm: 169.25, GNorm: 26.16\n",
      "[Train][3300] Alpha: 0.000, Beta: 0.000, Loss: 12.14, KL: 551.05, MAE: 3.07129, Word Loss: 8.37, Topo Loss: 2.12, Assm Loss: 1.65, Pred Loss: 8.73, Word: 83.57, Topo: 97.06, Assm: 83.96, PNorm: 170.58, GNorm: 24.93\n",
      "[Train][3400] Alpha: 0.000, Beta: 0.000, Loss: 12.29, KL: 563.62, MAE: 3.03928, Word Loss: 8.43, Topo Loss: 2.24, Assm Loss: 1.63, Pred Loss: 8.58, Word: 83.89, Topo: 96.96, Assm: 84.86, PNorm: 172.05, GNorm: 19.75\n",
      "[Train][3500] Alpha: 0.000, Beta: 0.000, Loss: 12.17, KL: 561.52, MAE: 2.96655, Word Loss: 8.32, Topo Loss: 2.16, Assm Loss: 1.69, Pred Loss: 8.20, Word: 84.05, Topo: 97.02, Assm: 84.42, PNorm: 173.47, GNorm: 30.26\n",
      "[Train][3600] Alpha: 0.000, Beta: 0.000, Loss: 12.07, KL: 585.04, MAE: 3.10465, Word Loss: 8.26, Topo Loss: 2.13, Assm Loss: 1.68, Pred Loss: 8.72, Word: 84.01, Topo: 97.12, Assm: 84.57, PNorm: 174.81, GNorm: 28.66\n",
      "[Train][3700] Alpha: 0.000, Beta: 0.000, Loss: 11.09, KL: 589.03, MAE: 2.99940, Word Loss: 7.52, Topo Loss: 2.00, Assm Loss: 1.56, Pred Loss: 8.20, Word: 85.66, Topo: 97.32, Assm: 84.77, PNorm: 175.99, GNorm: 22.75\n",
      "[Train][3800] Alpha: 0.000, Beta: 0.000, Loss: 11.27, KL: 607.06, MAE: 3.02927, Word Loss: 7.74, Topo Loss: 1.97, Assm Loss: 1.56, Pred Loss: 8.45, Word: 84.86, Topo: 97.38, Assm: 85.13, PNorm: 177.23, GNorm: 29.13\n",
      "[Train][3900] Alpha: 0.000, Beta: 0.000, Loss: 11.01, KL: 606.08, MAE: 3.03183, Word Loss: 7.50, Topo Loss: 1.91, Assm Loss: 1.61, Pred Loss: 8.39, Word: 85.35, Topo: 97.48, Assm: 85.82, PNorm: 178.46, GNorm: 31.42\n",
      "[Train][4000] Alpha: 0.000, Beta: 0.000, Loss: 11.36, KL: 622.56, MAE: 2.89679, Word Loss: 7.64, Topo Loss: 2.15, Assm Loss: 1.57, Pred Loss: 7.92, Word: 85.31, Topo: 97.21, Assm: 85.45, PNorm: 179.80, GNorm: 27.96\n",
      "[Train][4100] Alpha: 0.000, Beta: 0.000, Loss: 10.72, KL: 608.21, MAE: 2.81484, Word Loss: 7.24, Topo Loss: 1.97, Assm Loss: 1.50, Pred Loss: 7.54, Word: 85.76, Topo: 97.43, Assm: 86.20, PNorm: 181.03, GNorm: 33.44\n",
      "[Train][4200] Alpha: 0.000, Beta: 0.000, Loss: 10.47, KL: 623.72, MAE: 2.78265, Word Loss: 7.07, Topo Loss: 1.86, Assm Loss: 1.53, Pred Loss: 7.34, Word: 86.41, Topo: 97.55, Assm: 85.87, PNorm: 182.20, GNorm: 27.82\n",
      "[Train][4300] Alpha: 0.000, Beta: 0.000, Loss: 11.08, KL: 631.67, MAE: 2.89370, Word Loss: 7.73, Topo Loss: 1.94, Assm Loss: 1.42, Pred Loss: 7.88, Word: 84.89, Topo: 97.46, Assm: 86.79, PNorm: 183.51, GNorm: 29.29\n",
      "[Train][4400] Alpha: 0.000, Beta: 0.000, Loss: 10.10, KL: 640.94, MAE: 2.89048, Word Loss: 6.93, Topo Loss: 1.71, Assm Loss: 1.46, Pred Loss: 7.96, Word: 86.10, Topo: 97.59, Assm: 86.43, PNorm: 184.71, GNorm: 27.54\n",
      "[Train][4500] Alpha: 0.000, Beta: 0.000, Loss: 10.51, KL: 664.52, MAE: 3.02265, Word Loss: 7.12, Topo Loss: 1.87, Assm Loss: 1.52, Pred Loss: 8.56, Word: 86.14, Topo: 97.51, Assm: 86.93, PNorm: 185.97, GNorm: 32.82\n",
      "[Train][4600] Alpha: 0.000, Beta: 0.000, Loss: 10.33, KL: 668.59, MAE: 2.93604, Word Loss: 6.95, Topo Loss: 1.89, Assm Loss: 1.49, Pred Loss: 8.05, Word: 86.37, Topo: 97.38, Assm: 85.96, PNorm: 187.20, GNorm: 28.82\n",
      "[Train][4700] Alpha: 0.000, Beta: 0.000, Loss: 9.93, KL: 665.43, MAE: 2.85328, Word Loss: 6.76, Topo Loss: 1.75, Assm Loss: 1.41, Pred Loss: 7.78, Word: 86.66, Topo: 97.63, Assm: 87.06, PNorm: 188.35, GNorm: 24.36\n",
      "[Train][4800] Alpha: 0.000, Beta: 0.000, Loss: 9.71, KL: 682.57, MAE: 2.95436, Word Loss: 6.48, Topo Loss: 1.86, Assm Loss: 1.38, Pred Loss: 8.21, Word: 87.07, Topo: 97.54, Assm: 87.21, PNorm: 189.55, GNorm: 24.90\n",
      "[Train][4900] Alpha: 0.000, Beta: 0.000, Loss: 9.53, KL: 679.48, MAE: 3.02138, Word Loss: 6.42, Topo Loss: 1.72, Assm Loss: 1.39, Pred Loss: 8.68, Word: 87.27, Topo: 97.61, Assm: 86.85, PNorm: 190.62, GNorm: 24.69\n",
      "[Train][5000] Alpha: 0.000, Beta: 0.000, Loss: 9.51, KL: 692.56, MAE: 2.80141, Word Loss: 6.48, Topo Loss: 1.66, Assm Loss: 1.38, Pred Loss: 7.54, Word: 86.88, Topo: 97.81, Assm: 87.36, PNorm: 191.78, GNorm: 30.98\n",
      "[Train][5100] Alpha: 0.000, Beta: 0.000, Loss: 9.43, KL: 698.47, MAE: 2.77415, Word Loss: 6.47, Topo Loss: 1.62, Assm Loss: 1.34, Pred Loss: 7.34, Word: 87.29, Topo: 97.89, Assm: 87.71, PNorm: 192.88, GNorm: 26.69\n",
      "[Train][5200] Alpha: 0.000, Beta: 0.000, Loss: 9.11, KL: 698.60, MAE: 2.75731, Word Loss: 6.28, Topo Loss: 1.59, Assm Loss: 1.24, Pred Loss: 7.36, Word: 87.37, Topo: 97.87, Assm: 88.64, PNorm: 194.00, GNorm: 29.79\n",
      "[Train][5300] Alpha: 0.000, Beta: 0.000, Loss: 9.49, KL: 702.20, MAE: 2.78562, Word Loss: 6.59, Topo Loss: 1.62, Assm Loss: 1.28, Pred Loss: 7.56, Word: 87.12, Topo: 97.90, Assm: 87.97, PNorm: 195.19, GNorm: 29.00\n",
      "[Train][5400] Alpha: 0.000, Beta: 0.000, Loss: 9.33, KL: 699.87, MAE: 2.82438, Word Loss: 6.35, Topo Loss: 1.65, Assm Loss: 1.33, Pred Loss: 7.64, Word: 87.17, Topo: 97.81, Assm: 87.71, PNorm: 196.36, GNorm: 35.04\n",
      "[Train][5500] Alpha: 0.000, Beta: 0.000, Loss: 8.74, KL: 701.69, MAE: 2.87748, Word Loss: 6.03, Topo Loss: 1.51, Assm Loss: 1.21, Pred Loss: 7.87, Word: 87.74, Topo: 97.99, Assm: 88.81, PNorm: 197.45, GNorm: 26.74\n",
      "[Train][5600] Alpha: 0.000, Beta: 0.000, Loss: 9.06, KL: 710.88, MAE: 2.97493, Word Loss: 6.16, Topo Loss: 1.65, Assm Loss: 1.24, Pred Loss: 8.31, Word: 87.41, Topo: 97.87, Assm: 88.73, PNorm: 198.60, GNorm: 23.33\n",
      "[Train][5700] Alpha: 0.000, Beta: 0.000, Loss: 8.85, KL: 712.34, MAE: 2.90929, Word Loss: 6.12, Topo Loss: 1.56, Assm Loss: 1.17, Pred Loss: 8.13, Word: 87.34, Topo: 97.90, Assm: 89.03, PNorm: 199.70, GNorm: 22.92\n",
      "[Train][5800] Alpha: 0.000, Beta: 0.000, Loss: 8.63, KL: 719.74, MAE: 2.87403, Word Loss: 5.86, Topo Loss: 1.62, Assm Loss: 1.16, Pred Loss: 7.95, Word: 88.16, Topo: 97.78, Assm: 89.24, PNorm: 200.82, GNorm: 26.33\n",
      "[Train][5900] Alpha: 0.000, Beta: 0.000, Loss: 8.61, KL: 718.92, MAE: 2.89626, Word Loss: 5.92, Topo Loss: 1.56, Assm Loss: 1.13, Pred Loss: 7.93, Word: 87.90, Topo: 97.92, Assm: 90.02, PNorm: 201.91, GNorm: 25.66\n",
      "[Train][6000] Alpha: 0.000, Beta: 0.000, Loss: 8.76, KL: 727.56, MAE: 2.92410, Word Loss: 5.97, Topo Loss: 1.58, Assm Loss: 1.21, Pred Loss: 8.13, Word: 87.91, Topo: 97.87, Assm: 88.71, PNorm: 203.02, GNorm: 27.32\n",
      "[Train][6100] Alpha: 0.000, Beta: 0.000, Loss: 8.24, KL: 726.62, MAE: 2.85078, Word Loss: 5.68, Topo Loss: 1.47, Assm Loss: 1.09, Pred Loss: 7.80, Word: 88.74, Topo: 98.01, Assm: 90.24, PNorm: 204.11, GNorm: 32.73\n",
      "[Train][6200] Alpha: 0.000, Beta: 0.000, Loss: 8.87, KL: 728.02, MAE: 2.86315, Word Loss: 5.75, Topo Loss: 1.47, Assm Loss: 1.65, Pred Loss: 7.74, Word: 88.22, Topo: 98.03, Assm: 88.69, PNorm: 205.17, GNorm: 37.48\n",
      "[Train][6300] Alpha: 0.000, Beta: 0.000, Loss: 8.55, KL: 743.56, MAE: 2.82315, Word Loss: 5.92, Topo Loss: 1.46, Assm Loss: 1.17, Pred Loss: 7.72, Word: 88.08, Topo: 98.10, Assm: 89.53, PNorm: 206.24, GNorm: 28.45\n",
      "[Train][6400] Alpha: 0.000, Beta: 0.000, Loss: 8.20, KL: 743.21, MAE: 2.73369, Word Loss: 5.71, Topo Loss: 1.42, Assm Loss: 1.06, Pred Loss: 7.20, Word: 88.41, Topo: 98.10, Assm: 90.34, PNorm: 207.34, GNorm: 21.37\n",
      "[Train][6500] Alpha: 0.000, Beta: 0.000, Loss: 8.16, KL: 748.95, MAE: 2.89168, Word Loss: 5.51, Topo Loss: 1.46, Assm Loss: 1.19, Pred Loss: 7.95, Word: 88.53, Topo: 98.07, Assm: 89.98, PNorm: 208.33, GNorm: 25.67\n",
      "[Train][6600] Alpha: 0.000, Beta: 0.000, Loss: 7.98, KL: 755.76, MAE: 2.89821, Word Loss: 5.47, Topo Loss: 1.37, Assm Loss: 1.14, Pred Loss: 8.07, Word: 88.87, Topo: 98.17, Assm: 89.78, PNorm: 209.37, GNorm: 21.97\n",
      "[Train][6700] Alpha: 0.000, Beta: 0.000, Loss: 7.99, KL: 764.79, MAE: 2.94003, Word Loss: 5.54, Topo Loss: 1.37, Assm Loss: 1.08, Pred Loss: 8.23, Word: 88.69, Topo: 98.16, Assm: 90.57, PNorm: 210.46, GNorm: 37.01\n",
      "[Train][6800] Alpha: 0.000, Beta: 0.000, Loss: 7.93, KL: 775.78, MAE: 2.81093, Word Loss: 5.36, Topo Loss: 1.40, Assm Loss: 1.16, Pred Loss: 7.65, Word: 88.97, Topo: 98.10, Assm: 89.52, PNorm: 211.58, GNorm: 26.39\n",
      "[Train][6900] Alpha: 0.000, Beta: 0.000, Loss: 8.28, KL: 765.44, MAE: 2.81583, Word Loss: 5.62, Topo Loss: 1.55, Assm Loss: 1.11, Pred Loss: 7.69, Word: 88.61, Topo: 97.94, Assm: 90.45, PNorm: 212.72, GNorm: 33.79\n",
      "[Train][7000] Alpha: 0.000, Beta: 0.000, Loss: 7.60, KL: 772.39, MAE: 2.93242, Word Loss: 5.22, Topo Loss: 1.39, Assm Loss: 0.98, Pred Loss: 8.17, Word: 89.36, Topo: 98.13, Assm: 91.02, PNorm: 213.81, GNorm: 22.14\n",
      "[Train][7100] Alpha: 0.000, Beta: 0.000, Loss: 7.44, KL: 770.25, MAE: 2.98831, Word Loss: 5.18, Topo Loss: 1.29, Assm Loss: 0.97, Pred Loss: 8.52, Word: 89.14, Topo: 98.22, Assm: 90.80, PNorm: 214.74, GNorm: 23.66\n",
      "[Train][7200] Alpha: 0.000, Beta: 0.000, Loss: 7.29, KL: 772.99, MAE: 3.00795, Word Loss: 5.00, Topo Loss: 1.24, Assm Loss: 1.05, Pred Loss: 8.67, Word: 89.47, Topo: 98.36, Assm: 90.67, PNorm: 215.75, GNorm: 28.02\n",
      "[Train][7300] Alpha: 0.000, Beta: 0.000, Loss: 7.47, KL: 792.48, MAE: 2.98549, Word Loss: 5.16, Topo Loss: 1.26, Assm Loss: 1.04, Pred Loss: 8.51, Word: 89.28, Topo: 98.32, Assm: 90.65, PNorm: 216.78, GNorm: 25.00\n",
      "[Train][7400] Alpha: 0.000, Beta: 0.000, Loss: 7.57, KL: 796.43, MAE: 3.14405, Word Loss: 5.28, Topo Loss: 1.30, Assm Loss: 1.00, Pred Loss: 9.27, Word: 89.11, Topo: 98.28, Assm: 90.97, PNorm: 217.74, GNorm: 36.43\n",
      "[Train][7500] Alpha: 0.000, Beta: 0.000, Loss: 7.28, KL: 799.69, MAE: 2.95492, Word Loss: 5.05, Topo Loss: 1.29, Assm Loss: 0.95, Pred Loss: 8.27, Word: 89.51, Topo: 98.32, Assm: 91.36, PNorm: 218.79, GNorm: 23.79\n",
      "[Train][7600] Alpha: 0.000, Beta: 0.000, Loss: 7.62, KL: 814.64, MAE: 3.05296, Word Loss: 5.15, Topo Loss: 1.31, Assm Loss: 1.17, Pred Loss: 8.75, Word: 89.60, Topo: 98.30, Assm: 90.71, PNorm: 219.90, GNorm: 25.17\n",
      "[Train][7700] Alpha: 0.000, Beta: 0.000, Loss: 7.63, KL: 815.86, MAE: 3.14121, Word Loss: 5.18, Topo Loss: 1.38, Assm Loss: 1.07, Pred Loss: 9.19, Word: 89.38, Topo: 98.14, Assm: 90.85, PNorm: 220.83, GNorm: 24.64\n",
      "[Train][7800] Alpha: 0.000, Beta: 0.000, Loss: 7.57, KL: 819.45, MAE: 3.05853, Word Loss: 5.27, Topo Loss: 1.30, Assm Loss: 1.00, Pred Loss: 8.79, Word: 89.30, Topo: 98.27, Assm: 91.49, PNorm: 221.82, GNorm: 32.70\n",
      "[Train][7900] Alpha: 0.000, Beta: 0.000, Loss: 7.77, KL: 807.16, MAE: 3.05572, Word Loss: 5.23, Topo Loss: 1.46, Assm Loss: 1.07, Pred Loss: 8.74, Word: 89.62, Topo: 98.21, Assm: 91.11, PNorm: 222.85, GNorm: 29.57\n",
      "[Train][8000] Alpha: 0.000, Beta: 0.000, Loss: 6.97, KL: 826.78, MAE: 3.00749, Word Loss: 4.80, Topo Loss: 1.26, Assm Loss: 0.90, Pred Loss: 8.64, Word: 90.08, Topo: 98.36, Assm: 91.99, PNorm: 223.83, GNorm: 26.44\n",
      "[Train][8100] Alpha: 0.000, Beta: 0.000, Loss: 6.96, KL: 827.76, MAE: 3.02413, Word Loss: 4.74, Topo Loss: 1.27, Assm Loss: 0.96, Pred Loss: 8.61, Word: 90.03, Topo: 98.29, Assm: 91.41, PNorm: 224.80, GNorm: 25.06\n",
      "[Train][8200] Alpha: 0.000, Beta: 0.000, Loss: 6.80, KL: 849.30, MAE: 3.09290, Word Loss: 4.75, Topo Loss: 1.15, Assm Loss: 0.89, Pred Loss: 8.95, Word: 89.80, Topo: 98.48, Assm: 91.80, PNorm: 225.70, GNorm: 25.80\n",
      "[Train][8300] Alpha: 0.000, Beta: 0.000, Loss: 7.12, KL: 825.49, MAE: 2.91503, Word Loss: 4.94, Topo Loss: 1.19, Assm Loss: 0.99, Pred Loss: 8.35, Word: 89.81, Topo: 98.42, Assm: 91.33, PNorm: 226.65, GNorm: 25.17\n",
      "[Train][8400] Alpha: 0.000, Beta: 0.000, Loss: 7.58, KL: 834.35, MAE: 3.03569, Word Loss: 5.31, Topo Loss: 1.31, Assm Loss: 0.96, Pred Loss: 8.67, Word: 89.35, Topo: 98.27, Assm: 91.60, PNorm: 227.65, GNorm: 32.54\n",
      "[Train][8500] Alpha: 0.000, Beta: 0.000, Loss: 6.82, KL: 823.62, MAE: 2.85161, Word Loss: 4.70, Topo Loss: 1.14, Assm Loss: 0.97, Pred Loss: 7.76, Word: 90.21, Topo: 98.47, Assm: 91.23, PNorm: 228.58, GNorm: 40.08\n",
      "[Train][8600] Alpha: 0.000, Beta: 0.000, Loss: 7.21, KL: 831.16, MAE: 2.81826, Word Loss: 5.02, Topo Loss: 1.24, Assm Loss: 0.96, Pred Loss: 7.63, Word: 89.40, Topo: 98.32, Assm: 91.84, PNorm: 229.58, GNorm: 20.42\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred_supervised(\n",
    "    X_train,\n",
    "    L_train,\n",
    "    X_test,\n",
    "    L_test,\n",
    "    X_Val,\n",
    "    L_Val,\n",
    "    load_epoch= 0,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=conf[\"alpha\"],\n",
    "    max_alpha=conf[\"max_alpha\"],\n",
    "    step_alpha=conf[\"step_alpha\"],\n",
    "    beta=conf[\"beta\"],\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    alpha_anneal_iter=conf[\"alpha_anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter= conf[\"save_iter\"],\n",
    "    batch_size=conf[\"batch_size\"],\n",
    "    num_workers=conf[\"num_workers\"],\n",
    "    label_pct=0.5,\n",
    "    chem_prop = \"LogP\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of google_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
