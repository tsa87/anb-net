{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "gradient": {
     "editing": false,
     "id": "18de9aeb-6551-42f6-8c11-a0e30bd207d6",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "JDinHdioUZRH",
    "outputId": "1a824742-d528-46a2-8d89-7697a166c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu116.html\n",
    "!pip install -q dive-into-graphs\n",
    "!pip install -q toolz\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f728113a-ff43-466d-b34d-77c9b2e11478",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "_RKgd8MsYOYh"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pickle \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from molecule_optimizer.externals.fast_jtnn.datautils import SemiMolTreeFolder, SemiMolTreeFolderTest\n",
    "from molecule_optimizer.runner.semi_jtvae import SemiJTVAEGeneratorPredictor\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "\n",
    "import rdkit\n",
    "\n",
    "lg = rdkit.RDLogger.logger() \n",
    "lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6e85f4e2-eab6-4eae-b452-7f089e176039",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "2C4erValhPS-"
   },
   "outputs": [],
   "source": [
    "conf = json.load(open(\"training/configs/rand_gen_zinc250k_config_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": true,
     "id": "30eb7f71-c38e-49f6-8d84-715ff3d80e08",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "SuzGX7ClhKaf",
    "outputId": "ab764f6b-4dd7-4ffc-b6cf-bae47d6cedf7"
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"ZINC_310k.csv\")\n",
    "smiles = csv['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TEST = 10000\n",
    "VAL_FRAC = 0.05\n",
    "chem_prop = \"MolWt\"\n",
    "load_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(csv[chem_prop]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "processed_smiles, processed_idxs = SemiJTVAEGeneratorPredictor.preprocess(smiles) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "4ccc9136-0e87-470b-90eb-8e0b92da52bc",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "0-1xXVU15z6N",
    "outputId": "c6c328fe-72e4-4401-bce6-8613e7e9319f"
   },
   "outputs": [],
   "source": [
    "runner.get_model(\n",
    "    \"rand_gen\",\n",
    "    {\n",
    "        \"hidden_size\": conf[\"model\"][\"hidden_size\"],\n",
    "        \"latent_size\": conf[\"model\"][\"latent_size\"],\n",
    "        \"depthT\": conf[\"model\"][\"depthT\"],\n",
    "        \"depthG\": conf[\"model\"][\"depthG\"],\n",
    "        \"label_size\": 1,\n",
    "        \"label_mean\": float(torch.mean(labels)),\n",
    "        \"label_var\": float(torch.var(labels)),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c177219a-298a-4994-98b9-de76833e14fe",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "IGnpfkM_KQXi"
   },
   "outputs": [],
   "source": [
    "labels = runner.get_processed_labels(labels, processed_idxs)\n",
    "preprocessed = processed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perm_id=np.random.permutation(len(labels))\n",
    "\n",
    "X_train = preprocessed[perm_id[N_TEST:]]\n",
    "X_train_smiles = smiles[perm_id[N_TEST:]]\n",
    "L_train = torch.tensor(labels.numpy()[perm_id[N_TEST:]])\n",
    "\n",
    "\n",
    "X_test = preprocessed[perm_id[:N_TEST]]\n",
    "X_test_smiles = smiles[perm_id[:N_TEST]]\n",
    "L_test = torch.tensor(labels.numpy()[perm_id[:N_TEST]])\n",
    "\n",
    "val_cut = math.floor(len(X_train) * VAL_FRAC)\n",
    "\n",
    "X_Val = X_train[:val_cut]\n",
    "X_Val_smiles = X_train_smiles[:val_cut]\n",
    "L_Val = L_train[:val_cut]\n",
    "\n",
    "X_train = X_train[val_cut :]\n",
    "X_train_smiles = X_train_smiles[val_cut :]\n",
    "L_train = L_train[val_cut :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model #Params: 5207K\n",
      "[Train][9100] Alpha: 0.000, Beta: 0.000, Loss: 6.59, KL: 789.27, MAE: 90.13103, Word Loss: 4.55, Topo Loss: 1.20, Assm Loss: 0.84, Pred Loss: 2.76, Word: 90.45, Topo: 98.37, Assm: 92.49, PNorm: 235.40, GNorm: 21.08\n",
      "[Train][9200] Alpha: 0.000, Beta: 0.000, Loss: 6.39, KL: 806.50, MAE: 91.06080, Word Loss: 4.47, Topo Loss: 1.13, Assm Loss: 0.80, Pred Loss: 2.78, Word: 90.38, Topo: 98.49, Assm: 92.93, PNorm: 237.90, GNorm: 22.20\n",
      "[Train][9300] Alpha: 0.000, Beta: 0.000, Loss: 6.44, KL: 806.42, MAE: 91.96536, Word Loss: 4.33, Topo Loss: 1.25, Assm Loss: 0.86, Pred Loss: 2.92, Word: 90.80, Topo: 98.35, Assm: 92.68, PNorm: 240.23, GNorm: 23.92\n",
      "[Train][9400] Alpha: 0.000, Beta: 0.000, Loss: 6.35, KL: 805.82, MAE: 89.43684, Word Loss: 4.38, Topo Loss: 1.14, Assm Loss: 0.84, Pred Loss: 2.73, Word: 90.73, Topo: 98.40, Assm: 92.54, PNorm: 242.54, GNorm: 23.66\n",
      "[Train][9500] Alpha: 0.000, Beta: 0.000, Loss: 6.24, KL: 793.20, MAE: 92.61533, Word Loss: 4.34, Topo Loss: 1.09, Assm Loss: 0.81, Pred Loss: 2.94, Word: 90.98, Topo: 98.53, Assm: 92.98, PNorm: 244.10, GNorm: 20.61\n",
      "[Train][9600] Alpha: 0.000, Beta: 0.000, Loss: 6.17, KL: 807.64, MAE: 93.01664, Word Loss: 4.28, Topo Loss: 1.11, Assm Loss: 0.78, Pred Loss: 2.89, Word: 90.80, Topo: 98.54, Assm: 93.13, PNorm: 245.44, GNorm: 26.83\n",
      "[Train][9700] Alpha: 0.000, Beta: 0.000, Loss: 6.26, KL: 820.70, MAE: 90.46890, Word Loss: 4.36, Topo Loss: 1.12, Assm Loss: 0.78, Pred Loss: 2.82, Word: 90.92, Topo: 98.51, Assm: 93.24, PNorm: 246.77, GNorm: 19.76\n",
      "[Train][9800] Alpha: 0.000, Beta: 0.000, Loss: 6.45, KL: 823.61, MAE: 91.93339, Word Loss: 4.44, Topo Loss: 1.14, Assm Loss: 0.87, Pred Loss: 2.84, Word: 90.46, Topo: 98.52, Assm: 92.15, PNorm: 248.08, GNorm: 27.22\n",
      "[Train][9900] Alpha: 0.000, Beta: 0.000, Loss: 6.45, KL: 835.43, MAE: 94.08676, Word Loss: 4.45, Topo Loss: 1.20, Assm Loss: 0.80, Pred Loss: 2.94, Word: 90.69, Topo: 98.46, Assm: 93.03, PNorm: 249.40, GNorm: 26.37\n",
      "[Train][10000] Alpha: 0.000, Beta: 0.000, Loss: 6.25, KL: 824.05, MAE: 86.59737, Word Loss: 4.29, Topo Loss: 1.17, Assm Loss: 0.79, Pred Loss: 2.58, Word: 90.84, Topo: 98.39, Assm: 92.68, PNorm: 250.67, GNorm: 40.68\n",
      "[Train][10100] Alpha: 0.000, Beta: 0.000, Loss: 6.15, KL: 838.42, MAE: 87.11761, Word Loss: 4.21, Topo Loss: 1.09, Assm Loss: 0.84, Pred Loss: 2.54, Word: 91.11, Topo: 98.56, Assm: 92.76, PNorm: 251.92, GNorm: 18.36\n",
      "[Train][10200] Alpha: 0.000, Beta: 0.000, Loss: 5.91, KL: 841.17, MAE: 90.44930, Word Loss: 4.01, Topo Loss: 1.06, Assm Loss: 0.83, Pred Loss: 2.81, Word: 91.52, Topo: 98.63, Assm: 92.88, PNorm: 253.29, GNorm: 19.93\n",
      "[Train][10300] Alpha: 0.000, Beta: 0.000, Loss: 6.31, KL: 820.85, MAE: 91.02972, Word Loss: 4.37, Topo Loss: 1.08, Assm Loss: 0.87, Pred Loss: 2.81, Word: 90.93, Topo: 98.56, Assm: 92.22, PNorm: 254.89, GNorm: 21.96\n",
      "[Train][10400] Alpha: 0.000, Beta: 0.000, Loss: 6.23, KL: 831.87, MAE: 89.66507, Word Loss: 4.20, Topo Loss: 1.16, Assm Loss: 0.87, Pred Loss: 2.74, Word: 91.63, Topo: 98.48, Assm: 92.57, PNorm: 256.31, GNorm: 30.50\n",
      "[Train][10500] Alpha: 0.000, Beta: 0.000, Loss: 6.48, KL: 849.17, MAE: 90.56752, Word Loss: 4.33, Topo Loss: 1.25, Assm Loss: 0.90, Pred Loss: 2.77, Word: 90.89, Topo: 98.32, Assm: 92.09, PNorm: 257.86, GNorm: 21.34\n",
      "[Train][10600] Alpha: 0.000, Beta: 0.000, Loss: 5.83, KL: 844.58, MAE: 88.29939, Word Loss: 4.09, Topo Loss: 0.97, Assm Loss: 0.77, Pred Loss: 2.60, Word: 91.06, Topo: 98.66, Assm: 93.06, PNorm: 259.24, GNorm: 21.74\n",
      "[Train][10700] Alpha: 0.000, Beta: 0.000, Loss: 6.46, KL: 860.41, MAE: 89.59854, Word Loss: 4.42, Topo Loss: 1.19, Assm Loss: 0.86, Pred Loss: 2.68, Word: 90.90, Topo: 98.45, Assm: 92.71, PNorm: 260.54, GNorm: 21.40\n",
      "[Train][10800] Alpha: 0.000, Beta: 0.000, Loss: 6.05, KL: 860.23, MAE: 90.79441, Word Loss: 4.14, Topo Loss: 1.09, Assm Loss: 0.82, Pred Loss: 2.84, Word: 91.33, Topo: 98.53, Assm: 93.16, PNorm: 261.80, GNorm: 22.25\n",
      "[Train][10900] Alpha: 0.000, Beta: 0.000, Loss: 6.26, KL: 866.88, MAE: 89.33646, Word Loss: 4.38, Topo Loss: 1.07, Assm Loss: 0.82, Pred Loss: 2.73, Word: 90.79, Topo: 98.60, Assm: 93.02, PNorm: 262.78, GNorm: 21.35\n",
      "[Train][11000] Alpha: 0.000, Beta: 0.000, Loss: 5.74, KL: 834.52, MAE: 84.99166, Word Loss: 3.94, Topo Loss: 1.06, Assm Loss: 0.74, Pred Loss: 2.50, Word: 91.63, Topo: 98.56, Assm: 93.34, PNorm: 264.04, GNorm: 22.66\n",
      "[Train][11100] Alpha: 0.000, Beta: 0.000, Loss: 6.25, KL: 866.87, MAE: 88.59902, Word Loss: 4.35, Topo Loss: 1.08, Assm Loss: 0.82, Pred Loss: 2.64, Word: 90.79, Topo: 98.53, Assm: 93.06, PNorm: 265.04, GNorm: 21.65\n",
      "[Train][11200] Alpha: 0.000, Beta: 0.000, Loss: 6.24, KL: 867.47, MAE: 92.64374, Word Loss: 4.36, Topo Loss: 1.15, Assm Loss: 0.73, Pred Loss: 2.85, Word: 90.98, Topo: 98.43, Assm: 93.75, PNorm: 266.60, GNorm: 22.84\n",
      "[Train][11300] Alpha: 0.000, Beta: 0.000, Loss: 5.77, KL: 859.86, MAE: 87.30187, Word Loss: 3.96, Topo Loss: 1.10, Assm Loss: 0.71, Pred Loss: 2.60, Word: 91.53, Topo: 98.59, Assm: 93.72, PNorm: 267.41, GNorm: 22.21\n",
      "[Train][11400] Alpha: 0.000, Beta: 0.000, Loss: 5.98, KL: 859.97, MAE: 87.50690, Word Loss: 4.08, Topo Loss: 1.21, Assm Loss: 0.70, Pred Loss: 2.56, Word: 91.46, Topo: 98.46, Assm: 93.69, PNorm: 268.48, GNorm: 20.34\n",
      "[Train][11500] Alpha: 0.000, Beta: 0.000, Loss: 5.58, KL: 856.17, MAE: 85.89194, Word Loss: 3.79, Topo Loss: 1.03, Assm Loss: 0.76, Pred Loss: 2.47, Word: 91.92, Topo: 98.59, Assm: 93.35, PNorm: 269.42, GNorm: 34.52\n",
      "[Train][11600] Alpha: 0.000, Beta: 0.000, Loss: 5.98, KL: 865.95, MAE: 91.37879, Word Loss: 4.10, Topo Loss: 1.06, Assm Loss: 0.82, Pred Loss: 2.82, Word: 91.50, Topo: 98.55, Assm: 93.16, PNorm: 270.40, GNorm: 16.86\n",
      "[Train][11700] Alpha: 0.000, Beta: 0.000, Loss: 5.52, KL: 860.06, MAE: 88.17024, Word Loss: 3.87, Topo Loss: 0.98, Assm Loss: 0.67, Pred Loss: 2.60, Word: 91.85, Topo: 98.66, Assm: 94.17, PNorm: 271.33, GNorm: 18.29\n",
      "[Train][11800] Alpha: 0.000, Beta: 0.000, Loss: 5.91, KL: 861.50, MAE: 89.00288, Word Loss: 4.13, Topo Loss: 1.05, Assm Loss: 0.73, Pred Loss: 2.67, Word: 91.39, Topo: 98.66, Assm: 93.93, PNorm: 272.61, GNorm: 28.10\n",
      "[Train][11900] Alpha: 0.000, Beta: 0.000, Loss: 5.61, KL: 876.03, MAE: 87.12038, Word Loss: 3.97, Topo Loss: 0.98, Assm Loss: 0.66, Pred Loss: 2.61, Word: 91.59, Topo: 98.71, Assm: 94.52, PNorm: 273.85, GNorm: 20.61\n",
      "[Train][12000] Alpha: 0.000, Beta: 0.000, Loss: 5.80, KL: 890.52, MAE: 88.87934, Word Loss: 3.95, Topo Loss: 1.11, Assm Loss: 0.74, Pred Loss: 2.64, Word: 91.64, Topo: 98.54, Assm: 93.32, PNorm: 274.92, GNorm: 17.58\n",
      "[Train][12100] Alpha: 0.000, Beta: 0.000, Loss: 5.71, KL: 891.35, MAE: 89.98426, Word Loss: 3.96, Topo Loss: 1.03, Assm Loss: 0.72, Pred Loss: 2.72, Word: 91.77, Topo: 98.66, Assm: 93.72, PNorm: 275.83, GNorm: 23.21\n",
      "[Train][12200] Alpha: 0.000, Beta: 0.000, Loss: 5.53, KL: 879.08, MAE: 93.14242, Word Loss: 3.80, Topo Loss: 0.96, Assm Loss: 0.77, Pred Loss: 2.87, Word: 91.96, Topo: 98.70, Assm: 93.50, PNorm: 276.84, GNorm: 24.22\n",
      "[Train][12300] Alpha: 0.000, Beta: 0.000, Loss: 5.45, KL: 882.16, MAE: 88.71458, Word Loss: 3.81, Topo Loss: 0.97, Assm Loss: 0.66, Pred Loss: 2.71, Word: 91.93, Topo: 98.71, Assm: 94.07, PNorm: 277.87, GNorm: 29.43\n",
      "[Train][12400] Alpha: 0.000, Beta: 0.000, Loss: 5.75, KL: 879.03, MAE: 87.49010, Word Loss: 4.09, Topo Loss: 0.94, Assm Loss: 0.72, Pred Loss: 2.57, Word: 91.71, Topo: 98.76, Assm: 93.70, PNorm: 279.16, GNorm: 18.83\n",
      "[Train][12500] Alpha: 0.000, Beta: 0.000, Loss: 5.51, KL: 875.20, MAE: 87.38582, Word Loss: 3.81, Topo Loss: 1.04, Assm Loss: 0.65, Pred Loss: 2.58, Word: 92.04, Topo: 98.60, Assm: 94.36, PNorm: 280.08, GNorm: 28.01\n",
      "[Train][12600] Alpha: 0.000, Beta: 0.000, Loss: 5.91, KL: 888.17, MAE: 87.74915, Word Loss: 4.09, Topo Loss: 1.09, Assm Loss: 0.73, Pred Loss: 2.62, Word: 91.56, Topo: 98.57, Assm: 93.44, PNorm: 280.89, GNorm: 22.31\n",
      "[Train][12700] Alpha: 0.000, Beta: 0.000, Loss: 5.57, KL: 891.14, MAE: 85.93274, Word Loss: 3.84, Topo Loss: 1.04, Assm Loss: 0.70, Pred Loss: 2.52, Word: 91.74, Topo: 98.65, Assm: 94.01, PNorm: 281.66, GNorm: 25.35\n",
      "[Train][12800] Alpha: 0.000, Beta: 0.000, Loss: 5.69, KL: 880.92, MAE: 88.75294, Word Loss: 3.93, Topo Loss: 1.03, Assm Loss: 0.73, Pred Loss: 2.61, Word: 91.90, Topo: 98.64, Assm: 93.80, PNorm: 282.81, GNorm: 23.55\n",
      "[Train][12900] Alpha: 0.000, Beta: 0.000, Loss: 5.67, KL: 891.29, MAE: 89.59539, Word Loss: 4.00, Topo Loss: 1.03, Assm Loss: 0.64, Pred Loss: 2.73, Word: 91.61, Topo: 98.60, Assm: 94.40, PNorm: 283.66, GNorm: 24.87\n",
      "[Train][13000] Alpha: 0.000, Beta: 0.000, Loss: 5.70, KL: 893.43, MAE: 86.22130, Word Loss: 3.97, Topo Loss: 0.99, Assm Loss: 0.74, Pred Loss: 2.53, Word: 91.83, Topo: 98.70, Assm: 93.83, PNorm: 284.58, GNorm: 20.74\n",
      "[Train][13100] Alpha: 0.000, Beta: 0.000, Loss: 5.47, KL: 887.54, MAE: 86.14330, Word Loss: 3.79, Topo Loss: 0.96, Assm Loss: 0.73, Pred Loss: 2.57, Word: 91.96, Topo: 98.72, Assm: 94.09, PNorm: 285.54, GNorm: 23.47\n",
      "[Train][13200] Alpha: 0.000, Beta: 0.000, Loss: 5.66, KL: 898.22, MAE: 87.14898, Word Loss: 3.83, Topo Loss: 1.04, Assm Loss: 0.78, Pred Loss: 2.56, Word: 91.70, Topo: 98.62, Assm: 94.10, PNorm: 286.54, GNorm: 22.26\n",
      "[Train][13300] Alpha: 0.000, Beta: 0.000, Loss: 6.00, KL: 889.34, MAE: 88.15382, Word Loss: 4.06, Topo Loss: 1.15, Assm Loss: 0.79, Pred Loss: 2.65, Word: 91.43, Topo: 98.63, Assm: 93.92, PNorm: 287.55, GNorm: 20.00\n",
      "[Train][13400] Alpha: 0.000, Beta: 0.000, Loss: 5.54, KL: 888.07, MAE: 90.54321, Word Loss: 3.78, Topo Loss: 0.99, Assm Loss: 0.76, Pred Loss: 2.85, Word: 91.84, Topo: 98.70, Assm: 93.51, PNorm: 288.51, GNorm: 21.13\n",
      "[Train][13500] Alpha: 0.000, Beta: 0.000, Loss: 5.19, KL: 900.74, MAE: 91.26116, Word Loss: 3.60, Topo Loss: 0.95, Assm Loss: 0.64, Pred Loss: 2.83, Word: 92.00, Topo: 98.75, Assm: 94.44, PNorm: 289.56, GNorm: 25.56\n",
      "[Train][13600] Alpha: 0.000, Beta: 0.000, Loss: 5.39, KL: 894.83, MAE: 90.58923, Word Loss: 3.74, Topo Loss: 0.96, Assm Loss: 0.68, Pred Loss: 2.78, Word: 92.04, Topo: 98.71, Assm: 93.99, PNorm: 290.54, GNorm: 25.41\n",
      "[Train][13700] Alpha: 0.000, Beta: 0.000, Loss: 5.24, KL: 897.72, MAE: 93.28857, Word Loss: 3.71, Topo Loss: 0.92, Assm Loss: 0.61, Pred Loss: 2.93, Word: 92.23, Topo: 98.72, Assm: 94.86, PNorm: 291.68, GNorm: 24.72\n",
      "[Train][13800] Alpha: 0.000, Beta: 0.000, Loss: 5.52, KL: 891.59, MAE: 90.10435, Word Loss: 3.92, Topo Loss: 0.90, Assm Loss: 0.69, Pred Loss: 2.72, Word: 91.65, Topo: 98.77, Assm: 94.44, PNorm: 292.82, GNorm: 22.98\n",
      "[Train][13900] Alpha: 0.000, Beta: 0.000, Loss: 5.30, KL: 904.61, MAE: 94.56769, Word Loss: 3.72, Topo Loss: 0.92, Assm Loss: 0.65, Pred Loss: 2.99, Word: 92.31, Topo: 98.79, Assm: 94.56, PNorm: 293.62, GNorm: 18.60\n",
      "[Train][14000] Alpha: 0.000, Beta: 0.000, Loss: 5.40, KL: 902.79, MAE: 95.61926, Word Loss: 3.74, Topo Loss: 0.96, Assm Loss: 0.70, Pred Loss: 3.07, Word: 92.02, Topo: 98.78, Assm: 93.75, PNorm: 294.78, GNorm: 25.05\n",
      "[Train][14100] Alpha: 0.000, Beta: 0.000, Loss: 5.19, KL: 918.09, MAE: 95.56069, Word Loss: 3.65, Topo Loss: 0.94, Assm Loss: 0.60, Pred Loss: 3.09, Word: 92.40, Topo: 98.77, Assm: 95.08, PNorm: 295.58, GNorm: 22.40\n",
      "[Train][14200] Alpha: 0.000, Beta: 0.000, Loss: 5.31, KL: 907.46, MAE: 89.22849, Word Loss: 3.76, Topo Loss: 0.91, Assm Loss: 0.64, Pred Loss: 2.68, Word: 92.14, Topo: 98.81, Assm: 94.55, PNorm: 296.41, GNorm: 24.35\n",
      "[Train][14300] Alpha: 0.000, Beta: 0.000, Loss: 5.27, KL: 930.18, MAE: 89.04165, Word Loss: 3.72, Topo Loss: 0.94, Assm Loss: 0.61, Pred Loss: 2.66, Word: 92.04, Topo: 98.76, Assm: 94.95, PNorm: 297.71, GNorm: 22.55\n",
      "[Train][14400] Alpha: 0.000, Beta: 0.000, Loss: 5.50, KL: 916.32, MAE: 90.89220, Word Loss: 3.82, Topo Loss: 1.03, Assm Loss: 0.65, Pred Loss: 2.80, Word: 91.77, Topo: 98.66, Assm: 94.68, PNorm: 298.50, GNorm: 29.91\n",
      "[Train][14500] Alpha: 0.000, Beta: 0.000, Loss: 4.84, KL: 908.48, MAE: 88.31416, Word Loss: 3.39, Topo Loss: 0.87, Assm Loss: 0.58, Pred Loss: 2.61, Word: 92.46, Topo: 98.79, Assm: 95.03, PNorm: 299.17, GNorm: 18.13\n",
      "[Train][14600] Alpha: 0.000, Beta: 0.000, Loss: 5.22, KL: 916.05, MAE: 88.95972, Word Loss: 3.76, Topo Loss: 0.86, Assm Loss: 0.60, Pred Loss: 2.67, Word: 91.92, Topo: 98.89, Assm: 94.93, PNorm: 299.92, GNorm: 22.18\n",
      "[Train][14700] Alpha: 0.000, Beta: 0.000, Loss: 5.59, KL: 916.24, MAE: 90.90694, Word Loss: 3.93, Topo Loss: 0.96, Assm Loss: 0.70, Pred Loss: 2.77, Word: 91.70, Topo: 98.71, Assm: 94.35, PNorm: 300.81, GNorm: 28.71\n",
      "[Train][14800] Alpha: 0.000, Beta: 0.000, Loss: 5.17, KL: 918.13, MAE: 87.73899, Word Loss: 3.60, Topo Loss: 0.97, Assm Loss: 0.60, Pred Loss: 2.61, Word: 92.30, Topo: 98.75, Assm: 94.63, PNorm: 301.60, GNorm: 23.65\n",
      "[Train][14900] Alpha: 0.000, Beta: 0.000, Loss: 5.12, KL: 926.53, MAE: 90.11020, Word Loss: 3.57, Topo Loss: 0.88, Assm Loss: 0.67, Pred Loss: 2.76, Word: 92.51, Topo: 98.83, Assm: 94.25, PNorm: 302.25, GNorm: 24.56\n",
      "[Train][15000] Alpha: 0.000, Beta: 0.000, Loss: 5.38, KL: 918.04, MAE: 88.55649, Word Loss: 3.83, Topo Loss: 0.91, Assm Loss: 0.64, Pred Loss: 2.64, Word: 91.85, Topo: 98.82, Assm: 94.62, PNorm: 303.06, GNorm: 19.33\n",
      "[Train][15100] Alpha: 0.000, Beta: 0.000, Loss: 5.04, KL: 918.90, MAE: 90.79414, Word Loss: 3.44, Topo Loss: 0.92, Assm Loss: 0.68, Pred Loss: 2.81, Word: 92.68, Topo: 98.80, Assm: 94.00, PNorm: 303.93, GNorm: 21.82\n",
      "[Train][15200] Alpha: 0.000, Beta: 0.000, Loss: 4.99, KL: 931.25, MAE: 87.67215, Word Loss: 3.42, Topo Loss: 0.89, Assm Loss: 0.69, Pred Loss: 2.60, Word: 92.80, Topo: 98.86, Assm: 94.06, PNorm: 304.62, GNorm: 25.94\n",
      "[Train][15300] Alpha: 0.000, Beta: 0.000, Loss: 4.96, KL: 915.02, MAE: 89.39279, Word Loss: 3.42, Topo Loss: 0.91, Assm Loss: 0.62, Pred Loss: 2.70, Word: 92.33, Topo: 98.74, Assm: 94.68, PNorm: 305.37, GNorm: 19.91\n",
      "[Train][15400] Alpha: 0.000, Beta: 0.000, Loss: 5.33, KL: 932.09, MAE: 84.52401, Word Loss: 3.73, Topo Loss: 0.92, Assm Loss: 0.68, Pred Loss: 2.42, Word: 92.18, Topo: 98.83, Assm: 94.05, PNorm: 306.16, GNorm: 21.29\n",
      "[Train][15500] Alpha: 0.000, Beta: 0.000, Loss: 5.03, KL: 923.37, MAE: 88.25483, Word Loss: 3.50, Topo Loss: 0.91, Assm Loss: 0.63, Pred Loss: 2.59, Word: 92.61, Topo: 98.77, Assm: 94.69, PNorm: 307.13, GNorm: 33.36\n",
      "[Train][15600] Alpha: 0.000, Beta: 0.000, Loss: 5.18, KL: 918.65, MAE: 87.26560, Word Loss: 3.67, Topo Loss: 0.90, Assm Loss: 0.62, Pred Loss: 2.57, Word: 92.12, Topo: 98.80, Assm: 94.79, PNorm: 308.04, GNorm: 20.85\n",
      "[Train][15700] Alpha: 0.000, Beta: 0.000, Loss: 5.26, KL: 923.71, MAE: 89.93345, Word Loss: 3.71, Topo Loss: 0.92, Assm Loss: 0.62, Pred Loss: 2.70, Word: 92.15, Topo: 98.79, Assm: 94.87, PNorm: 308.72, GNorm: 21.99\n",
      "[Train][15800] Alpha: 0.000, Beta: 0.000, Loss: 5.07, KL: 933.36, MAE: 87.94695, Word Loss: 3.57, Topo Loss: 0.88, Assm Loss: 0.62, Pred Loss: 2.61, Word: 92.62, Topo: 98.75, Assm: 94.75, PNorm: 309.48, GNorm: 20.09\n",
      "[Train][15900] Alpha: 0.000, Beta: 0.000, Loss: 5.09, KL: 937.44, MAE: 86.61211, Word Loss: 3.62, Topo Loss: 0.91, Assm Loss: 0.56, Pred Loss: 2.55, Word: 92.39, Topo: 98.79, Assm: 95.09, PNorm: 310.44, GNorm: 21.79\n",
      "[Train][16000] Alpha: 0.000, Beta: 0.000, Loss: 4.84, KL: 920.69, MAE: 85.53165, Word Loss: 3.38, Topo Loss: 0.86, Assm Loss: 0.59, Pred Loss: 2.49, Word: 92.60, Topo: 98.85, Assm: 94.80, PNorm: 311.18, GNorm: 20.66\n",
      "[Train][16100] Alpha: 0.000, Beta: 0.000, Loss: 4.97, KL: 932.45, MAE: 88.46612, Word Loss: 3.49, Topo Loss: 0.86, Assm Loss: 0.62, Pred Loss: 2.61, Word: 92.49, Topo: 98.80, Assm: 94.76, PNorm: 312.13, GNorm: 22.01\n",
      "[Train][16200] Alpha: 0.000, Beta: 0.000, Loss: 5.13, KL: 936.30, MAE: 85.01167, Word Loss: 3.63, Topo Loss: 0.90, Assm Loss: 0.60, Pred Loss: 2.49, Word: 92.45, Topo: 98.84, Assm: 94.55, PNorm: 313.54, GNorm: 30.08\n",
      "[Train][16300] Alpha: 0.000, Beta: 0.000, Loss: 5.20, KL: 935.15, MAE: 85.30071, Word Loss: 3.69, Topo Loss: 0.91, Assm Loss: 0.61, Pred Loss: 2.45, Word: 92.20, Topo: 98.87, Assm: 95.20, PNorm: 314.76, GNorm: 24.96\n",
      "[Train][16400] Alpha: 0.000, Beta: 0.000, Loss: 5.11, KL: 934.78, MAE: 90.30355, Word Loss: 3.60, Topo Loss: 0.91, Assm Loss: 0.60, Pred Loss: 2.68, Word: 92.22, Topo: 98.77, Assm: 95.03, PNorm: 315.52, GNorm: 24.34\n",
      "[Train][16500] Alpha: 0.000, Beta: 0.000, Loss: 5.12, KL: 934.58, MAE: 84.71055, Word Loss: 3.61, Topo Loss: 0.97, Assm Loss: 0.54, Pred Loss: 2.45, Word: 92.23, Topo: 98.72, Assm: 95.76, PNorm: 316.30, GNorm: 22.88\n",
      "[Train][16600] Alpha: 0.000, Beta: 0.000, Loss: 5.26, KL: 945.18, MAE: 89.87688, Word Loss: 3.64, Topo Loss: 0.98, Assm Loss: 0.65, Pred Loss: 2.77, Word: 92.39, Topo: 98.71, Assm: 94.30, PNorm: 317.12, GNorm: 30.89\n",
      "[Train][16700] Alpha: 0.000, Beta: 0.000, Loss: 5.21, KL: 930.07, MAE: 85.75568, Word Loss: 3.68, Topo Loss: 0.95, Assm Loss: 0.58, Pred Loss: 2.49, Word: 92.33, Topo: 98.72, Assm: 95.45, PNorm: 318.51, GNorm: 25.27\n",
      "[Train][16800] Alpha: 0.000, Beta: 0.000, Loss: 4.85, KL: 926.61, MAE: 87.50010, Word Loss: 3.51, Topo Loss: 0.81, Assm Loss: 0.53, Pred Loss: 2.60, Word: 92.48, Topo: 98.97, Assm: 95.36, PNorm: 319.42, GNorm: 24.71\n",
      "[Train][16900] Alpha: 0.000, Beta: 0.000, Loss: 5.21, KL: 933.86, MAE: 84.86234, Word Loss: 3.70, Topo Loss: 0.89, Assm Loss: 0.61, Pred Loss: 2.45, Word: 92.11, Topo: 98.81, Assm: 94.69, PNorm: 320.42, GNorm: 24.23\n",
      "[Train][17000] Alpha: 0.000, Beta: 0.000, Loss: 4.72, KL: 917.02, MAE: 86.92626, Word Loss: 3.26, Topo Loss: 0.86, Assm Loss: 0.59, Pred Loss: 2.59, Word: 93.07, Topo: 98.89, Assm: 95.03, PNorm: 320.99, GNorm: 21.02\n",
      "[Train][17100] Alpha: 0.000, Beta: 0.000, Loss: 4.87, KL: 932.59, MAE: 83.97271, Word Loss: 3.46, Topo Loss: 0.82, Assm Loss: 0.59, Pred Loss: 2.40, Word: 92.90, Topo: 98.97, Assm: 95.12, PNorm: 321.63, GNorm: 19.97\n",
      "[Train][17200] Alpha: 0.000, Beta: 0.000, Loss: 4.99, KL: 955.51, MAE: 88.67495, Word Loss: 3.45, Topo Loss: 0.98, Assm Loss: 0.56, Pred Loss: 2.71, Word: 92.47, Topo: 98.72, Assm: 95.21, PNorm: 322.41, GNorm: 19.30\n",
      "[Train][17300] Alpha: 0.000, Beta: 0.000, Loss: 4.84, KL: 956.19, MAE: 88.53922, Word Loss: 3.37, Topo Loss: 0.87, Assm Loss: 0.61, Pred Loss: 2.63, Word: 92.96, Topo: 98.87, Assm: 94.75, PNorm: 323.67, GNorm: 27.54\n",
      "[Train][17400] Alpha: 0.000, Beta: 0.000, Loss: 4.70, KL: 945.11, MAE: 87.41860, Word Loss: 3.34, Topo Loss: 0.78, Assm Loss: 0.58, Pred Loss: 2.58, Word: 93.04, Topo: 98.98, Assm: 94.95, PNorm: 324.33, GNorm: 23.14\n",
      "[Train][17500] Alpha: 0.000, Beta: 0.000, Loss: 4.92, KL: 963.39, MAE: 89.96501, Word Loss: 3.47, Topo Loss: 0.87, Assm Loss: 0.58, Pred Loss: 2.77, Word: 92.54, Topo: 98.89, Assm: 95.32, PNorm: 325.14, GNorm: 25.18\n",
      "[Train][17600] Alpha: 0.000, Beta: 0.000, Loss: 4.72, KL: 953.01, MAE: 85.36454, Word Loss: 3.30, Topo Loss: 0.88, Assm Loss: 0.54, Pred Loss: 2.45, Word: 92.92, Topo: 98.82, Assm: 95.57, PNorm: 325.96, GNorm: 12.55\n",
      "[Train][17700] Alpha: 0.000, Beta: 0.000, Loss: 4.73, KL: 968.41, MAE: 86.79459, Word Loss: 3.35, Topo Loss: 0.81, Assm Loss: 0.57, Pred Loss: 2.56, Word: 92.67, Topo: 98.95, Assm: 95.16, PNorm: 326.92, GNorm: 25.92\n",
      "[Train][17800] Alpha: 0.000, Beta: 0.000, Loss: 4.89, KL: 957.52, MAE: 86.46663, Word Loss: 3.51, Topo Loss: 0.83, Assm Loss: 0.55, Pred Loss: 2.52, Word: 92.47, Topo: 98.91, Assm: 95.54, PNorm: 327.55, GNorm: 15.22\n",
      "[Train][17900] Alpha: 0.000, Beta: 0.000, Loss: 4.84, KL: 946.60, MAE: 87.04666, Word Loss: 3.43, Topo Loss: 0.82, Assm Loss: 0.58, Pred Loss: 2.56, Word: 92.68, Topo: 98.90, Assm: 95.29, PNorm: 328.20, GNorm: 22.02\n",
      "[Train][18000] Alpha: 0.000, Beta: 0.000, Loss: 4.72, KL: 956.78, MAE: 88.79803, Word Loss: 3.34, Topo Loss: 0.86, Assm Loss: 0.52, Pred Loss: 2.60, Word: 92.69, Topo: 98.90, Assm: 95.55, PNorm: 328.92, GNorm: 19.30\n",
      "[Train][18100] Alpha: 0.000, Beta: 0.000, Loss: 4.77, KL: 948.25, MAE: 85.40268, Word Loss: 3.40, Topo Loss: 0.84, Assm Loss: 0.53, Pred Loss: 2.50, Word: 92.92, Topo: 98.88, Assm: 95.64, PNorm: 329.87, GNorm: 25.94\n",
      "[Train][18200] Alpha: 0.000, Beta: 0.000, Loss: 4.85, KL: 952.96, MAE: 85.87533, Word Loss: 3.47, Topo Loss: 0.85, Assm Loss: 0.54, Pred Loss: 2.49, Word: 92.71, Topo: 98.90, Assm: 95.90, PNorm: 330.86, GNorm: 20.80\n",
      "[Train][18300] Alpha: 0.000, Beta: 0.000, Loss: 4.43, KL: 951.17, MAE: 87.26405, Word Loss: 3.13, Topo Loss: 0.81, Assm Loss: 0.50, Pred Loss: 2.57, Word: 93.12, Topo: 98.95, Assm: 95.85, PNorm: 331.49, GNorm: 29.41\n",
      "[Train][18400] Alpha: 0.000, Beta: 0.000, Loss: 4.98, KL: 971.78, MAE: 86.92752, Word Loss: 3.55, Topo Loss: 0.83, Assm Loss: 0.60, Pred Loss: 2.58, Word: 92.76, Topo: 98.93, Assm: 94.99, PNorm: 332.41, GNorm: 19.00\n",
      "[Train][18500] Alpha: 0.000, Beta: 0.000, Loss: 4.59, KL: 965.98, MAE: 83.19876, Word Loss: 3.21, Topo Loss: 0.82, Assm Loss: 0.55, Pred Loss: 2.35, Word: 93.21, Topo: 98.93, Assm: 95.44, PNorm: 333.20, GNorm: 16.84\n",
      "[Train][18600] Alpha: 0.000, Beta: 0.000, Loss: 4.90, KL: 963.12, MAE: 83.51327, Word Loss: 3.47, Topo Loss: 0.89, Assm Loss: 0.53, Pred Loss: 2.38, Word: 92.78, Topo: 98.87, Assm: 95.32, PNorm: 334.06, GNorm: 11.92\n",
      "[Train][18700] Alpha: 0.000, Beta: 0.000, Loss: 4.67, KL: 962.87, MAE: 85.31746, Word Loss: 3.30, Topo Loss: 0.81, Assm Loss: 0.55, Pred Loss: 2.50, Word: 92.67, Topo: 98.92, Assm: 95.47, PNorm: 334.87, GNorm: 27.70\n",
      "[Train][18800] Alpha: 0.000, Beta: 0.000, Loss: 4.47, KL: 976.54, MAE: 88.04989, Word Loss: 3.14, Topo Loss: 0.82, Assm Loss: 0.52, Pred Loss: 2.60, Word: 93.26, Topo: 98.92, Assm: 95.70, PNorm: 335.58, GNorm: 21.23\n",
      "[Train][18900] Alpha: 0.000, Beta: 0.000, Loss: 4.67, KL: 975.04, MAE: 89.16598, Word Loss: 3.29, Topo Loss: 0.87, Assm Loss: 0.51, Pred Loss: 2.69, Word: 93.26, Topo: 98.88, Assm: 95.96, PNorm: 336.27, GNorm: 19.08\n",
      "[Train][19000] Alpha: 0.000, Beta: 0.000, Loss: 4.87, KL: 976.81, MAE: 88.20351, Word Loss: 3.41, Topo Loss: 0.84, Assm Loss: 0.62, Pred Loss: 2.65, Word: 92.70, Topo: 98.87, Assm: 94.78, PNorm: 337.09, GNorm: 22.14\n",
      "[Train][19100] Alpha: 0.000, Beta: 0.000, Loss: 4.55, KL: 961.94, MAE: 85.13748, Word Loss: 3.22, Topo Loss: 0.81, Assm Loss: 0.52, Pred Loss: 2.48, Word: 93.21, Topo: 98.90, Assm: 95.70, PNorm: 337.75, GNorm: 19.34\n",
      "[Train][19200] Alpha: 0.000, Beta: 0.000, Loss: 4.85, KL: 962.53, MAE: 90.34429, Word Loss: 3.35, Topo Loss: 0.91, Assm Loss: 0.59, Pred Loss: 2.82, Word: 92.82, Topo: 98.91, Assm: 95.33, PNorm: 338.50, GNorm: 16.43\n",
      "[Train][19300] Alpha: 0.000, Beta: 0.000, Loss: 4.49, KL: 967.26, MAE: 89.84071, Word Loss: 3.23, Topo Loss: 0.78, Assm Loss: 0.48, Pred Loss: 2.75, Word: 93.25, Topo: 98.97, Assm: 96.22, PNorm: 339.18, GNorm: 20.45\n",
      "[Train][19400] Alpha: 0.000, Beta: 0.000, Loss: 5.26, KL: 971.19, MAE: 86.55167, Word Loss: 3.42, Topo Loss: 1.01, Assm Loss: 0.83, Pred Loss: 2.54, Word: 92.77, Topo: 98.68, Assm: 95.63, PNorm: 339.95, GNorm: 24.08\n",
      "[Train][19500] Alpha: 0.000, Beta: 0.000, Loss: 4.58, KL: 964.99, MAE: 88.44690, Word Loss: 3.37, Topo Loss: 0.79, Assm Loss: 0.42, Pred Loss: 2.64, Word: 92.72, Topo: 98.93, Assm: 96.32, PNorm: 340.66, GNorm: 20.37\n",
      "[Train][19600] Alpha: 0.000, Beta: 0.000, Loss: 4.48, KL: 952.85, MAE: 87.90662, Word Loss: 3.18, Topo Loss: 0.78, Assm Loss: 0.51, Pred Loss: 2.61, Word: 93.14, Topo: 98.97, Assm: 95.74, PNorm: 341.47, GNorm: 23.25\n",
      "[Train][19700] Alpha: 0.000, Beta: 0.000, Loss: 4.69, KL: 983.43, MAE: 88.60777, Word Loss: 3.31, Topo Loss: 0.83, Assm Loss: 0.55, Pred Loss: 2.71, Word: 93.03, Topo: 98.92, Assm: 95.42, PNorm: 342.11, GNorm: 22.88\n",
      "[Train][19800] Alpha: 0.000, Beta: 0.000, Loss: 4.53, KL: 975.25, MAE: 90.51267, Word Loss: 3.24, Topo Loss: 0.81, Assm Loss: 0.48, Pred Loss: 2.81, Word: 93.07, Topo: 99.00, Assm: 95.86, PNorm: 342.72, GNorm: 20.41\n",
      "[Train][19900] Alpha: 0.000, Beta: 0.000, Loss: 4.60, KL: 957.49, MAE: 86.93157, Word Loss: 3.28, Topo Loss: 0.80, Assm Loss: 0.51, Pred Loss: 2.56, Word: 92.77, Topo: 98.93, Assm: 95.64, PNorm: 343.48, GNorm: 24.30\n",
      "[Train][20000] Alpha: 0.000, Beta: 0.000, Loss: 4.45, KL: 977.79, MAE: 86.93429, Word Loss: 3.11, Topo Loss: 0.78, Assm Loss: 0.56, Pred Loss: 2.55, Word: 93.19, Topo: 98.99, Assm: 95.45, PNorm: 344.08, GNorm: 17.76\n",
      "[Train][20100] Alpha: 0.000, Beta: 0.000, Loss: 4.91, KL: 967.93, MAE: 87.03121, Word Loss: 3.43, Topo Loss: 0.94, Assm Loss: 0.54, Pred Loss: 2.59, Word: 93.04, Topo: 98.80, Assm: 95.33, PNorm: 344.83, GNorm: 21.97\n",
      "[Train][20200] Alpha: 0.000, Beta: 0.000, Loss: 4.55, KL: 988.27, MAE: 88.12347, Word Loss: 3.23, Topo Loss: 0.83, Assm Loss: 0.50, Pred Loss: 2.61, Word: 92.90, Topo: 98.93, Assm: 95.76, PNorm: 345.86, GNorm: 19.70\n",
      "[Train][20300] Alpha: 0.000, Beta: 0.000, Loss: 4.88, KL: 973.24, MAE: 84.98180, Word Loss: 3.55, Topo Loss: 0.81, Assm Loss: 0.53, Pred Loss: 2.46, Word: 92.54, Topo: 98.89, Assm: 95.96, PNorm: 346.56, GNorm: 13.91\n",
      "[Train][20400] Alpha: 0.000, Beta: 0.000, Loss: 4.51, KL: 966.37, MAE: 86.92896, Word Loss: 3.20, Topo Loss: 0.82, Assm Loss: 0.50, Pred Loss: 2.58, Word: 92.90, Topo: 98.94, Assm: 96.09, PNorm: 347.28, GNorm: 24.27\n",
      "[Train][20500] Alpha: 0.000, Beta: 0.000, Loss: 5.03, KL: 976.74, MAE: 85.87034, Word Loss: 3.55, Topo Loss: 0.86, Assm Loss: 0.61, Pred Loss: 2.52, Word: 92.62, Topo: 98.86, Assm: 95.31, PNorm: 348.06, GNorm: 21.38\n",
      "[Train][20600] Alpha: 0.000, Beta: 0.000, Loss: 4.42, KL: 976.34, MAE: 86.46545, Word Loss: 3.14, Topo Loss: 0.76, Assm Loss: 0.51, Pred Loss: 2.53, Word: 93.33, Topo: 99.01, Assm: 95.55, PNorm: 348.97, GNorm: 24.55\n",
      "[Train][20700] Alpha: 0.000, Beta: 0.000, Loss: 4.72, KL: 1003.35, MAE: 86.39326, Word Loss: 3.35, Topo Loss: 0.77, Assm Loss: 0.61, Pred Loss: 2.48, Word: 92.97, Topo: 99.01, Assm: 95.04, PNorm: 349.78, GNorm: 24.53\n",
      "[Train][20800] Alpha: 0.000, Beta: 0.000, Loss: 4.48, KL: 994.11, MAE: 83.02879, Word Loss: 3.19, Topo Loss: 0.79, Assm Loss: 0.51, Pred Loss: 2.35, Word: 92.89, Topo: 99.01, Assm: 95.45, PNorm: 350.37, GNorm: 28.58\n",
      "[Train][20900] Alpha: 0.000, Beta: 0.000, Loss: 4.57, KL: 1007.56, MAE: 84.12524, Word Loss: 3.25, Topo Loss: 0.81, Assm Loss: 0.52, Pred Loss: 2.43, Word: 92.93, Topo: 98.88, Assm: 95.54, PNorm: 350.97, GNorm: 26.03\n",
      "[Train][21000] Alpha: 0.000, Beta: 0.000, Loss: 4.44, KL: 995.82, MAE: 82.51181, Word Loss: 3.11, Topo Loss: 0.77, Assm Loss: 0.55, Pred Loss: 2.35, Word: 93.17, Topo: 98.98, Assm: 95.21, PNorm: 351.58, GNorm: 22.45\n",
      "[Train][21100] Alpha: 0.000, Beta: 0.000, Loss: 4.65, KL: 1003.12, MAE: 85.12848, Word Loss: 3.27, Topo Loss: 0.80, Assm Loss: 0.58, Pred Loss: 2.45, Word: 92.89, Topo: 98.98, Assm: 95.01, PNorm: 352.25, GNorm: 28.00\n",
      "[Train][21200] Alpha: 0.000, Beta: 0.000, Loss: 4.33, KL: 1003.08, MAE: 88.56103, Word Loss: 3.10, Topo Loss: 0.73, Assm Loss: 0.51, Pred Loss: 2.60, Word: 93.38, Topo: 99.02, Assm: 95.58, PNorm: 352.83, GNorm: 30.49\n",
      "[Train][21300] Alpha: 0.000, Beta: 0.000, Loss: 4.59, KL: 999.90, MAE: 83.75108, Word Loss: 3.20, Topo Loss: 0.91, Assm Loss: 0.48, Pred Loss: 2.38, Word: 93.20, Topo: 98.85, Assm: 95.85, PNorm: 353.43, GNorm: 17.72\n",
      "[Train][21400] Alpha: 0.000, Beta: 0.000, Loss: 4.44, KL: 996.34, MAE: 83.82425, Word Loss: 3.11, Topo Loss: 0.84, Assm Loss: 0.49, Pred Loss: 2.36, Word: 93.16, Topo: 98.89, Assm: 95.72, PNorm: 354.04, GNorm: 24.30\n",
      "[Train][21500] Alpha: 0.000, Beta: 0.000, Loss: 4.42, KL: 1021.67, MAE: 86.37087, Word Loss: 3.19, Topo Loss: 0.76, Assm Loss: 0.48, Pred Loss: 2.56, Word: 93.09, Topo: 99.03, Assm: 96.11, PNorm: 354.65, GNorm: 29.03\n",
      "[Train][21600] Alpha: 0.000, Beta: 0.000, Loss: 4.50, KL: 1003.37, MAE: 85.52964, Word Loss: 3.24, Topo Loss: 0.71, Assm Loss: 0.55, Pred Loss: 2.44, Word: 93.09, Topo: 99.07, Assm: 95.42, PNorm: 355.26, GNorm: 24.49\n",
      "[Train][21700] Alpha: 0.000, Beta: 0.000, Loss: 4.39, KL: 998.77, MAE: 86.17419, Word Loss: 3.07, Topo Loss: 0.80, Assm Loss: 0.52, Pred Loss: 2.51, Word: 93.35, Topo: 98.93, Assm: 96.05, PNorm: 355.84, GNorm: 16.96\n",
      "[Train][21800] Alpha: 0.000, Beta: 0.000, Loss: 4.42, KL: 1008.73, MAE: 84.27089, Word Loss: 3.15, Topo Loss: 0.75, Assm Loss: 0.52, Pred Loss: 2.42, Word: 93.15, Topo: 99.02, Assm: 95.60, PNorm: 356.44, GNorm: 20.33\n",
      "[Train][21900] Alpha: 0.000, Beta: 0.000, Loss: 4.48, KL: 1012.14, MAE: 88.30748, Word Loss: 3.24, Topo Loss: 0.76, Assm Loss: 0.48, Pred Loss: 2.64, Word: 93.05, Topo: 99.00, Assm: 95.96, PNorm: 357.28, GNorm: 21.03\n",
      "[Train][22000] Alpha: 0.000, Beta: 0.000, Loss: 4.29, KL: 1012.78, MAE: 86.19848, Word Loss: 3.06, Topo Loss: 0.73, Assm Loss: 0.50, Pred Loss: 2.49, Word: 93.39, Topo: 99.04, Assm: 95.56, PNorm: 357.86, GNorm: 27.52\n",
      "[Train][22100] Alpha: 0.000, Beta: 0.000, Loss: 4.19, KL: 1020.32, MAE: 84.16418, Word Loss: 3.04, Topo Loss: 0.71, Assm Loss: 0.44, Pred Loss: 2.39, Word: 93.54, Topo: 99.09, Assm: 96.24, PNorm: 358.67, GNorm: 17.06\n",
      "[Train][22200] Alpha: 0.000, Beta: 0.000, Loss: 4.28, KL: 1000.07, MAE: 82.96387, Word Loss: 3.02, Topo Loss: 0.73, Assm Loss: 0.53, Pred Loss: 2.38, Word: 93.65, Topo: 99.06, Assm: 95.50, PNorm: 359.27, GNorm: 21.78\n",
      "[Train][22300] Alpha: 0.000, Beta: 0.000, Loss: 4.47, KL: 1030.16, MAE: 87.84624, Word Loss: 3.17, Topo Loss: 0.83, Assm Loss: 0.47, Pred Loss: 2.63, Word: 93.23, Topo: 98.90, Assm: 96.03, PNorm: 359.84, GNorm: 22.61\n",
      "[Train][22400] Alpha: 0.000, Beta: 0.000, Loss: 4.43, KL: 1022.77, MAE: 84.11107, Word Loss: 3.16, Topo Loss: 0.80, Assm Loss: 0.48, Pred Loss: 2.44, Word: 93.42, Topo: 98.98, Assm: 96.19, PNorm: 360.62, GNorm: 25.84\n",
      "[Train][22500] Alpha: 0.000, Beta: 0.000, Loss: 4.30, KL: 1012.36, MAE: 82.61193, Word Loss: 3.06, Topo Loss: 0.71, Assm Loss: 0.53, Pred Loss: 2.36, Word: 93.44, Topo: 99.03, Assm: 95.73, PNorm: 361.34, GNorm: 18.54\n",
      "[Train][22600] Alpha: 0.000, Beta: 0.000, Loss: 4.40, KL: 1013.38, MAE: 84.83244, Word Loss: 3.16, Topo Loss: 0.77, Assm Loss: 0.47, Pred Loss: 2.43, Word: 93.34, Topo: 99.00, Assm: 96.08, PNorm: 361.93, GNorm: 16.28\n",
      "[Train][22700] Alpha: 0.000, Beta: 0.000, Loss: 4.45, KL: 1020.80, MAE: 82.72530, Word Loss: 3.17, Topo Loss: 0.81, Assm Loss: 0.47, Pred Loss: 2.33, Word: 93.13, Topo: 98.98, Assm: 95.89, PNorm: 362.82, GNorm: 13.51\n",
      "[Train][22800] Alpha: 0.000, Beta: 0.000, Loss: 4.46, KL: 1033.67, MAE: 83.53462, Word Loss: 3.22, Topo Loss: 0.73, Assm Loss: 0.52, Pred Loss: 2.35, Word: 93.13, Topo: 99.01, Assm: 95.93, PNorm: 363.43, GNorm: 19.67\n",
      "[Train][22900] Alpha: 0.000, Beta: 0.000, Loss: 4.34, KL: 1016.92, MAE: 82.01553, Word Loss: 3.07, Topo Loss: 0.80, Assm Loss: 0.47, Pred Loss: 2.25, Word: 93.46, Topo: 99.03, Assm: 95.98, PNorm: 364.14, GNorm: 19.69\n",
      "[Train][23000] Alpha: 0.000, Beta: 0.000, Loss: 4.32, KL: 1036.24, MAE: 83.59908, Word Loss: 3.05, Topo Loss: 0.79, Assm Loss: 0.48, Pred Loss: 2.34, Word: 93.62, Topo: 98.99, Assm: 96.01, PNorm: 364.80, GNorm: 16.67\n",
      "[Train][23100] Alpha: 0.000, Beta: 0.000, Loss: 4.30, KL: 1035.72, MAE: 82.93384, Word Loss: 3.10, Topo Loss: 0.76, Assm Loss: 0.44, Pred Loss: 2.33, Word: 93.27, Topo: 99.00, Assm: 96.31, PNorm: 365.54, GNorm: 20.38\n",
      "[Train][23200] Alpha: 0.000, Beta: 0.000, Loss: 4.64, KL: 1030.21, MAE: 82.15997, Word Loss: 3.42, Topo Loss: 0.76, Assm Loss: 0.46, Pred Loss: 2.32, Word: 92.79, Topo: 98.98, Assm: 96.33, PNorm: 366.95, GNorm: 28.78\n",
      "[Train][23300] Alpha: 0.000, Beta: 0.000, Loss: 4.29, KL: 1046.11, MAE: 84.54559, Word Loss: 3.15, Topo Loss: 0.73, Assm Loss: 0.40, Pred Loss: 2.41, Word: 93.34, Topo: 99.06, Assm: 96.90, PNorm: 367.88, GNorm: 30.09\n",
      "[Train][23400] Alpha: 0.000, Beta: 0.000, Loss: 4.34, KL: 1034.23, MAE: 83.39151, Word Loss: 3.14, Topo Loss: 0.75, Assm Loss: 0.45, Pred Loss: 2.36, Word: 93.26, Topo: 99.00, Assm: 96.23, PNorm: 368.82, GNorm: 17.62\n",
      "[Train][23500] Alpha: 0.000, Beta: 0.000, Loss: 4.15, KL: 1041.78, MAE: 82.56049, Word Loss: 2.98, Topo Loss: 0.73, Assm Loss: 0.44, Pred Loss: 2.32, Word: 93.74, Topo: 99.08, Assm: 96.24, PNorm: 369.48, GNorm: 19.65\n",
      "[Train][23600] Alpha: 0.000, Beta: 0.000, Loss: 4.54, KL: 1033.58, MAE: 81.95129, Word Loss: 3.27, Topo Loss: 0.78, Assm Loss: 0.49, Pred Loss: 2.30, Word: 93.21, Topo: 98.96, Assm: 95.97, PNorm: 370.27, GNorm: 21.93\n",
      "[Train][23700] Alpha: 0.000, Beta: 0.000, Loss: 4.46, KL: 1035.55, MAE: 82.93107, Word Loss: 3.17, Topo Loss: 0.77, Assm Loss: 0.51, Pred Loss: 2.34, Word: 93.01, Topo: 99.00, Assm: 95.60, PNorm: 371.10, GNorm: 28.57\n",
      "[Train][23800] Alpha: 0.000, Beta: 0.000, Loss: 4.35, KL: 1008.02, MAE: 83.95144, Word Loss: 3.14, Topo Loss: 0.78, Assm Loss: 0.44, Pred Loss: 2.42, Word: 93.34, Topo: 99.00, Assm: 96.32, PNorm: 371.77, GNorm: 18.75\n",
      "[Train][23900] Alpha: 0.000, Beta: 0.000, Loss: 4.59, KL: 1030.72, MAE: 82.54945, Word Loss: 3.23, Topo Loss: 0.87, Assm Loss: 0.49, Pred Loss: 2.32, Word: 93.26, Topo: 98.83, Assm: 96.35, PNorm: 372.92, GNorm: 26.05\n",
      "[Train][24000] Alpha: 0.000, Beta: 0.000, Loss: 4.35, KL: 1036.29, MAE: 85.66860, Word Loss: 3.14, Topo Loss: 0.73, Assm Loss: 0.47, Pred Loss: 2.48, Word: 93.20, Topo: 99.05, Assm: 96.30, PNorm: 373.66, GNorm: 20.38\n",
      "[Train][24100] Alpha: 0.000, Beta: 0.000, Loss: 4.25, KL: 1004.45, MAE: 81.79605, Word Loss: 2.98, Topo Loss: 0.74, Assm Loss: 0.53, Pred Loss: 2.26, Word: 93.61, Topo: 99.03, Assm: 95.56, PNorm: 374.36, GNorm: 24.75\n",
      "[Train][24200] Alpha: 0.000, Beta: 0.000, Loss: 4.30, KL: 1021.24, MAE: 83.43850, Word Loss: 3.08, Topo Loss: 0.76, Assm Loss: 0.46, Pred Loss: 2.43, Word: 93.48, Topo: 99.03, Assm: 96.14, PNorm: 375.15, GNorm: 25.51\n",
      "[Train][24300] Alpha: 0.000, Beta: 0.000, Loss: 4.17, KL: 1026.38, MAE: 81.97326, Word Loss: 3.05, Topo Loss: 0.70, Assm Loss: 0.42, Pred Loss: 2.22, Word: 93.15, Topo: 99.08, Assm: 96.42, PNorm: 375.65, GNorm: 21.39\n",
      "[Train][24400] Alpha: 0.000, Beta: 0.000, Loss: 4.38, KL: 1027.56, MAE: 81.89043, Word Loss: 3.19, Topo Loss: 0.72, Assm Loss: 0.47, Pred Loss: 2.28, Word: 93.23, Topo: 99.08, Assm: 96.09, PNorm: 376.28, GNorm: 14.19\n",
      "[Train][24500] Alpha: 0.000, Beta: 0.000, Loss: 4.05, KL: 1025.80, MAE: 84.33803, Word Loss: 2.86, Topo Loss: 0.73, Assm Loss: 0.45, Pred Loss: 2.39, Word: 93.86, Topo: 99.00, Assm: 96.38, PNorm: 376.95, GNorm: 13.64\n",
      "[Train][24600] Alpha: 0.000, Beta: 0.000, Loss: 4.29, KL: 1041.29, MAE: 84.09783, Word Loss: 3.16, Topo Loss: 0.70, Assm Loss: 0.43, Pred Loss: 2.42, Word: 93.48, Topo: 99.11, Assm: 96.57, PNorm: 377.81, GNorm: 25.11\n",
      "[Train][24700] Alpha: 0.000, Beta: 0.000, Loss: 4.32, KL: 1044.46, MAE: 81.36816, Word Loss: 3.14, Topo Loss: 0.74, Assm Loss: 0.44, Pred Loss: 2.28, Word: 93.23, Topo: 99.02, Assm: 96.29, PNorm: 378.55, GNorm: 26.76\n",
      "[Train][24800] Alpha: 0.000, Beta: 0.000, Loss: 4.28, KL: 1034.25, MAE: 87.06939, Word Loss: 3.08, Topo Loss: 0.75, Assm Loss: 0.45, Pred Loss: 2.54, Word: 93.11, Topo: 99.03, Assm: 96.21, PNorm: 379.15, GNorm: 18.60\n",
      "[Train][24900] Alpha: 0.000, Beta: 0.000, Loss: 4.01, KL: 1034.08, MAE: 85.34988, Word Loss: 2.92, Topo Loss: 0.70, Assm Loss: 0.40, Pred Loss: 2.47, Word: 93.82, Topo: 99.11, Assm: 96.47, PNorm: 379.67, GNorm: 27.02\n",
      "[Train][25000] Alpha: 0.000, Beta: 0.000, Loss: 4.31, KL: 1044.33, MAE: 84.45357, Word Loss: 3.11, Topo Loss: 0.70, Assm Loss: 0.50, Pred Loss: 2.40, Word: 93.30, Topo: 99.10, Assm: 96.05, PNorm: 380.23, GNorm: 15.68\n",
      "[Train][25100] Alpha: 0.000, Beta: 0.000, Loss: 4.12, KL: 1026.00, MAE: 82.86988, Word Loss: 2.97, Topo Loss: 0.69, Assm Loss: 0.46, Pred Loss: 2.37, Word: 93.55, Topo: 99.09, Assm: 96.12, PNorm: 380.82, GNorm: 21.83\n",
      "[Train][25200] Alpha: 0.000, Beta: 0.000, Loss: 4.09, KL: 1022.62, MAE: 86.70787, Word Loss: 2.98, Topo Loss: 0.69, Assm Loss: 0.41, Pred Loss: 2.56, Word: 93.42, Topo: 99.08, Assm: 96.59, PNorm: 381.29, GNorm: 13.99\n",
      "[Train][25300] Alpha: 0.000, Beta: 0.000, Loss: 4.25, KL: 1021.09, MAE: 85.33332, Word Loss: 3.09, Topo Loss: 0.69, Assm Loss: 0.47, Pred Loss: 2.44, Word: 93.39, Topo: 99.09, Assm: 96.43, PNorm: 382.11, GNorm: 20.08\n",
      "[Train][25400] Alpha: 0.000, Beta: 0.000, Loss: 3.87, KL: 1013.87, MAE: 86.34751, Word Loss: 2.71, Topo Loss: 0.69, Assm Loss: 0.47, Pred Loss: 2.46, Word: 94.03, Topo: 99.10, Assm: 96.07, PNorm: 382.61, GNorm: 19.43\n",
      "[Train][25500] Alpha: 0.000, Beta: 0.000, Loss: 4.66, KL: 1040.31, MAE: 83.18217, Word Loss: 3.23, Topo Loss: 0.87, Assm Loss: 0.56, Pred Loss: 2.31, Word: 93.04, Topo: 98.91, Assm: 96.12, PNorm: 383.35, GNorm: 19.54\n",
      "[Train][25600] Alpha: 0.000, Beta: 0.000, Loss: 4.13, KL: 1029.47, MAE: 86.10323, Word Loss: 2.94, Topo Loss: 0.76, Assm Loss: 0.43, Pred Loss: 2.47, Word: 93.54, Topo: 99.05, Assm: 96.34, PNorm: 383.95, GNorm: 32.04\n",
      "[Train][25700] Alpha: 0.000, Beta: 0.000, Loss: 4.11, KL: 1034.62, MAE: 84.55227, Word Loss: 2.92, Topo Loss: 0.70, Assm Loss: 0.49, Pred Loss: 2.45, Word: 93.80, Topo: 99.09, Assm: 96.09, PNorm: 384.70, GNorm: 33.41\n",
      "[Validation][881] Alpha: 0.000, Beta: 0.000, Loss: 4.35, KL: 1031.72, MAE: 84.16603, Word Loss: 3.05, Topo Loss: 0.83, Assm Loss: 0.47, Pred Loss: 2.395667, Word: 93.46, Topo: 98.91, Assm: 96.13\n",
      "[Train][25800] Alpha: 0.000, Beta: 0.000, Loss: 4.11, KL: 1036.68, MAE: 83.38351, Word Loss: 2.96, Topo Loss: 0.72, Assm Loss: 0.43, Pred Loss: 2.35, Word: 93.57, Topo: 99.10, Assm: 96.31, PNorm: 385.31, GNorm: 22.99\n",
      "[Train][25900] Alpha: 0.000, Beta: 0.000, Loss: 3.64, KL: 1028.02, MAE: 85.63126, Word Loss: 2.60, Topo Loss: 0.68, Assm Loss: 0.35, Pred Loss: 2.51, Word: 94.13, Topo: 99.07, Assm: 96.95, PNorm: 385.94, GNorm: 21.57\n",
      "[Train][26000] Alpha: 0.000, Beta: 0.000, Loss: 3.63, KL: 1039.58, MAE: 87.12415, Word Loss: 2.59, Topo Loss: 0.63, Assm Loss: 0.42, Pred Loss: 2.56, Word: 94.37, Topo: 99.17, Assm: 96.48, PNorm: 386.75, GNorm: 22.72\n",
      "[Train][26100] Alpha: 0.000, Beta: 0.000, Loss: 3.73, KL: 1047.39, MAE: 83.80994, Word Loss: 2.69, Topo Loss: 0.67, Assm Loss: 0.38, Pred Loss: 2.41, Word: 93.97, Topo: 99.13, Assm: 96.75, PNorm: 387.50, GNorm: 16.11\n",
      "[Train][26200] Alpha: 0.000, Beta: 0.000, Loss: 3.87, KL: 1057.15, MAE: 83.20028, Word Loss: 2.75, Topo Loss: 0.71, Assm Loss: 0.41, Pred Loss: 2.37, Word: 93.91, Topo: 99.08, Assm: 96.60, PNorm: 388.08, GNorm: 24.86\n",
      "[Train][26300] Alpha: 0.000, Beta: 0.000, Loss: 3.80, KL: 1051.63, MAE: 83.94132, Word Loss: 2.79, Topo Loss: 0.61, Assm Loss: 0.41, Pred Loss: 2.43, Word: 93.83, Topo: 99.23, Assm: 96.57, PNorm: 388.68, GNorm: 22.54\n",
      "[Train][26400] Alpha: 0.000, Beta: 0.000, Loss: 3.94, KL: 1048.08, MAE: 82.85550, Word Loss: 2.82, Topo Loss: 0.70, Assm Loss: 0.43, Pred Loss: 2.32, Word: 93.77, Topo: 99.07, Assm: 96.59, PNorm: 389.36, GNorm: 25.05\n",
      "[Train][26500] Alpha: 0.000, Beta: 0.000, Loss: 3.67, KL: 1036.35, MAE: 83.29509, Word Loss: 2.60, Topo Loss: 0.67, Assm Loss: 0.41, Pred Loss: 2.32, Word: 94.27, Topo: 99.11, Assm: 96.43, PNorm: 389.86, GNorm: 22.97\n",
      "[Train][26600] Alpha: 0.000, Beta: 0.000, Loss: 3.96, KL: 1064.71, MAE: 84.27778, Word Loss: 2.94, Topo Loss: 0.66, Assm Loss: 0.36, Pred Loss: 2.46, Word: 93.58, Topo: 99.16, Assm: 97.05, PNorm: 390.47, GNorm: 17.63\n",
      "[Train][26700] Alpha: 0.000, Beta: 0.000, Loss: 3.73, KL: 1049.24, MAE: 80.69401, Word Loss: 2.70, Topo Loss: 0.62, Assm Loss: 0.41, Pred Loss: 2.22, Word: 93.82, Topo: 99.19, Assm: 96.57, PNorm: 390.96, GNorm: 18.51\n",
      "[Train][26800] Alpha: 0.000, Beta: 0.000, Loss: 3.78, KL: 1054.43, MAE: 84.70242, Word Loss: 2.74, Topo Loss: 0.59, Assm Loss: 0.44, Pred Loss: 2.44, Word: 93.96, Topo: 99.19, Assm: 96.70, PNorm: 391.88, GNorm: 21.24\n",
      "[Train][26900] Alpha: 0.000, Beta: 0.000, Loss: 3.86, KL: 1052.97, MAE: 82.04135, Word Loss: 2.76, Topo Loss: 0.68, Assm Loss: 0.42, Pred Loss: 2.27, Word: 93.89, Topo: 99.08, Assm: 96.47, PNorm: 392.49, GNorm: 18.30\n",
      "[Train][27000] Alpha: 0.000, Beta: 0.000, Loss: 3.88, KL: 1052.71, MAE: 80.65867, Word Loss: 2.76, Topo Loss: 0.68, Assm Loss: 0.45, Pred Loss: 2.27, Word: 93.65, Topo: 99.12, Assm: 96.51, PNorm: 393.03, GNorm: 21.01\n",
      "[Train][27100] Alpha: 0.000, Beta: 0.000, Loss: 3.62, KL: 1062.58, MAE: 84.22236, Word Loss: 2.59, Topo Loss: 0.65, Assm Loss: 0.37, Pred Loss: 2.38, Word: 94.17, Topo: 99.14, Assm: 96.97, PNorm: 393.64, GNorm: 21.57\n",
      "[Train][27200] Alpha: 0.000, Beta: 0.000, Loss: 3.91, KL: 1049.00, MAE: 82.86452, Word Loss: 2.82, Topo Loss: 0.66, Assm Loss: 0.43, Pred Loss: 2.32, Word: 93.69, Topo: 99.16, Assm: 96.78, PNorm: 394.48, GNorm: 19.67\n",
      "[Train][27300] Alpha: 0.000, Beta: 0.000, Loss: 4.09, KL: 1034.93, MAE: 84.53171, Word Loss: 2.91, Topo Loss: 0.79, Assm Loss: 0.39, Pred Loss: 2.39, Word: 93.67, Topo: 99.04, Assm: 96.76, PNorm: 395.04, GNorm: 21.49\n",
      "[Train][27400] Alpha: 0.000, Beta: 0.000, Loss: 4.05, KL: 1033.79, MAE: 82.31346, Word Loss: 2.86, Topo Loss: 0.71, Assm Loss: 0.49, Pred Loss: 2.32, Word: 93.81, Topo: 99.04, Assm: 96.40, PNorm: 395.80, GNorm: 19.16\n",
      "[Train][27500] Alpha: 0.000, Beta: 0.000, Loss: 3.99, KL: 1034.29, MAE: 83.49840, Word Loss: 2.88, Topo Loss: 0.69, Assm Loss: 0.42, Pred Loss: 2.35, Word: 93.69, Topo: 99.08, Assm: 96.43, PNorm: 396.52, GNorm: 24.31\n",
      "[Train][27600] Alpha: 0.000, Beta: 0.000, Loss: 3.92, KL: 1054.33, MAE: 82.19572, Word Loss: 2.81, Topo Loss: 0.68, Assm Loss: 0.43, Pred Loss: 2.31, Word: 93.97, Topo: 99.15, Assm: 96.55, PNorm: 397.18, GNorm: 17.98\n",
      "[Train][27700] Alpha: 0.000, Beta: 0.000, Loss: 3.79, KL: 1032.18, MAE: 83.49088, Word Loss: 2.73, Topo Loss: 0.64, Assm Loss: 0.42, Pred Loss: 2.39, Word: 93.82, Topo: 99.17, Assm: 96.71, PNorm: 397.62, GNorm: 12.95\n",
      "[Train][27800] Alpha: 0.000, Beta: 0.000, Loss: 3.75, KL: 1042.68, MAE: 81.12758, Word Loss: 2.70, Topo Loss: 0.69, Assm Loss: 0.36, Pred Loss: 2.28, Word: 94.06, Topo: 99.09, Assm: 96.90, PNorm: 398.13, GNorm: 16.50\n",
      "[Train][27900] Alpha: 0.000, Beta: 0.000, Loss: 3.79, KL: 1050.18, MAE: 82.63199, Word Loss: 2.77, Topo Loss: 0.64, Assm Loss: 0.38, Pred Loss: 2.29, Word: 94.01, Topo: 99.18, Assm: 96.93, PNorm: 398.72, GNorm: 22.84\n",
      "[Train][28000] Alpha: 0.000, Beta: 0.000, Loss: 3.78, KL: 1042.00, MAE: 78.48707, Word Loss: 2.71, Topo Loss: 0.64, Assm Loss: 0.43, Pred Loss: 2.09, Word: 93.96, Topo: 99.13, Assm: 96.66, PNorm: 399.22, GNorm: 21.27\n",
      "[Train][28100] Alpha: 0.000, Beta: 0.000, Loss: 3.86, KL: 1056.33, MAE: 82.96660, Word Loss: 2.71, Topo Loss: 0.73, Assm Loss: 0.42, Pred Loss: 2.37, Word: 94.07, Topo: 99.12, Assm: 96.64, PNorm: 399.86, GNorm: 16.73\n",
      "[Train][28200] Alpha: 0.000, Beta: 0.000, Loss: 3.83, KL: 1060.33, MAE: 82.17544, Word Loss: 2.72, Topo Loss: 0.72, Assm Loss: 0.39, Pred Loss: 2.31, Word: 93.97, Topo: 99.03, Assm: 96.86, PNorm: 400.52, GNorm: 37.09\n",
      "[Train][28300] Alpha: 0.000, Beta: 0.000, Loss: 3.77, KL: 1051.04, MAE: 82.13314, Word Loss: 2.76, Topo Loss: 0.65, Assm Loss: 0.36, Pred Loss: 2.33, Word: 94.02, Topo: 99.10, Assm: 96.98, PNorm: 401.04, GNorm: 17.92\n",
      "[Train][28400] Alpha: 0.000, Beta: 0.000, Loss: 3.88, KL: 1068.23, MAE: 84.72135, Word Loss: 2.82, Topo Loss: 0.65, Assm Loss: 0.41, Pred Loss: 2.48, Word: 93.81, Topo: 99.14, Assm: 96.62, PNorm: 401.56, GNorm: 18.11\n",
      "[Train][28500] Alpha: 0.000, Beta: 0.000, Loss: 3.66, KL: 1050.23, MAE: 82.67638, Word Loss: 2.73, Topo Loss: 0.59, Assm Loss: 0.34, Pred Loss: 2.34, Word: 93.89, Topo: 99.25, Assm: 97.05, PNorm: 402.18, GNorm: 19.26\n",
      "[Train][28600] Alpha: 0.000, Beta: 0.000, Loss: 3.87, KL: 1052.16, MAE: 85.17983, Word Loss: 2.87, Topo Loss: 0.66, Assm Loss: 0.34, Pred Loss: 2.44, Word: 93.65, Topo: 99.10, Assm: 97.17, PNorm: 402.64, GNorm: 20.10\n",
      "[Train][28700] Alpha: 0.000, Beta: 0.000, Loss: 3.86, KL: 1041.98, MAE: 83.14776, Word Loss: 2.74, Topo Loss: 0.70, Assm Loss: 0.42, Pred Loss: 2.36, Word: 93.95, Topo: 99.06, Assm: 96.56, PNorm: 403.18, GNorm: 12.78\n",
      "[Train][38400] Alpha: 0.000, Beta: 0.000, Loss: 3.76, KL: 1104.45, MAE: 83.83333, Word Loss: 2.82, Topo Loss: 0.62, Assm Loss: 0.32, Pred Loss: 2.36, Word: 93.88, Topo: 99.22, Assm: 97.42, PNorm: 459.69, GNorm: 18.21\n",
      "[Train][38500] Alpha: 0.000, Beta: 0.000, Loss: 3.50, KL: 1099.30, MAE: 83.22139, Word Loss: 2.53, Topo Loss: 0.63, Assm Loss: 0.34, Pred Loss: 2.30, Word: 94.42, Topo: 99.21, Assm: 97.45, PNorm: 460.24, GNorm: 29.37\n",
      "[Train][39100] Alpha: 0.000, Beta: 0.000, Loss: 3.51, KL: 1141.96, MAE: 80.74493, Word Loss: 2.63, Topo Loss: 0.57, Assm Loss: 0.30, Pred Loss: 2.24, Word: 94.21, Topo: 99.25, Assm: 97.29, PNorm: 463.08, GNorm: 25.22\n",
      "[Train][39200] Alpha: 0.000, Beta: 0.000, Loss: 3.46, KL: 1127.87, MAE: 81.50694, Word Loss: 2.56, Topo Loss: 0.57, Assm Loss: 0.33, Pred Loss: 2.23, Word: 94.26, Topo: 99.23, Assm: 97.26, PNorm: 463.46, GNorm: 15.67\n",
      "[Train][39300] Alpha: 0.000, Beta: 0.000, Loss: 3.75, KL: 1130.40, MAE: 82.58232, Word Loss: 2.78, Topo Loss: 0.63, Assm Loss: 0.35, Pred Loss: 2.27, Word: 94.14, Topo: 99.16, Assm: 96.98, PNorm: 463.94, GNorm: 18.42\n",
      "[Train][39400] Alpha: 0.000, Beta: 0.000, Loss: 3.48, KL: 1141.11, MAE: 80.30544, Word Loss: 2.50, Topo Loss: 0.64, Assm Loss: 0.35, Pred Loss: 2.21, Word: 94.28, Topo: 99.14, Assm: 96.87, PNorm: 464.30, GNorm: 20.91\n",
      "[Train][39500] Alpha: 0.000, Beta: 0.000, Loss: 3.56, KL: 1143.14, MAE: 82.55460, Word Loss: 2.58, Topo Loss: 0.60, Assm Loss: 0.38, Pred Loss: 2.27, Word: 94.18, Topo: 99.20, Assm: 97.14, PNorm: 464.80, GNorm: 20.62\n",
      "[Train][39600] Alpha: 0.000, Beta: 0.000, Loss: 3.65, KL: 1122.37, MAE: 81.69499, Word Loss: 2.64, Topo Loss: 0.68, Assm Loss: 0.33, Pred Loss: 2.30, Word: 94.16, Topo: 99.14, Assm: 97.45, PNorm: 465.27, GNorm: 25.89\n",
      "[Train][39700] Alpha: 0.000, Beta: 0.000, Loss: 3.51, KL: 1109.65, MAE: 81.92058, Word Loss: 2.55, Topo Loss: 0.61, Assm Loss: 0.35, Pred Loss: 2.32, Word: 94.50, Topo: 99.24, Assm: 97.02, PNorm: 465.70, GNorm: 21.45\n",
      "[Train][39800] Alpha: 0.000, Beta: 0.000, Loss: 3.63, KL: 1118.59, MAE: 80.91456, Word Loss: 2.64, Topo Loss: 0.61, Assm Loss: 0.38, Pred Loss: 2.21, Word: 94.12, Topo: 99.24, Assm: 96.83, PNorm: 466.28, GNorm: 26.35\n",
      "[Train][39900] Alpha: 0.000, Beta: 0.000, Loss: 3.48, KL: 1114.23, MAE: 81.01663, Word Loss: 2.51, Topo Loss: 0.62, Assm Loss: 0.35, Pred Loss: 2.24, Word: 94.50, Topo: 99.17, Assm: 97.10, PNorm: 466.71, GNorm: 21.22\n",
      "[Train][40000] Alpha: 0.000, Beta: 0.000, Loss: 3.49, KL: 1103.00, MAE: 83.26848, Word Loss: 2.62, Topo Loss: 0.52, Assm Loss: 0.34, Pred Loss: 2.34, Word: 94.30, Topo: 99.33, Assm: 97.16, PNorm: 467.19, GNorm: 18.03\n",
      "learning rate: 0.000729\n",
      "[Train][40100] Alpha: 0.000, Beta: 0.002, Loss: 5.00, KL: 713.67, MAE: 74.99573, Word Loss: 2.61, Topo Loss: 0.60, Assm Loss: 0.37, Pred Loss: 1.93, Word: 94.26, Topo: 99.24, Assm: 97.35, PNorm: 466.76, GNorm: 16.46\n",
      "[Train][40200] Alpha: 0.000, Beta: 0.002, Loss: 4.63, KL: 594.54, MAE: 73.97872, Word Loss: 2.52, Topo Loss: 0.57, Assm Loss: 0.35, Pred Loss: 1.85, Word: 94.58, Topo: 99.22, Assm: 97.30, PNorm: 467.01, GNorm: 12.33\n",
      "[Train][40300] Alpha: 0.000, Beta: 0.002, Loss: 4.53, KL: 562.37, MAE: 74.77490, Word Loss: 2.47, Topo Loss: 0.59, Assm Loss: 0.35, Pred Loss: 1.91, Word: 94.64, Topo: 99.24, Assm: 97.11, PNorm: 467.21, GNorm: 13.80\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred_supervised(\n",
    "    X_train,\n",
    "    L_train,\n",
    "    X_test,\n",
    "    L_test,\n",
    "    X_Val,\n",
    "    L_Val,\n",
    "    load_epoch= load_epoch,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=0.0,\n",
    "    max_alpha=conf[\"max_alpha\"],\n",
    "    step_alpha=conf[\"step_alpha\"],\n",
    "    beta=0.0,\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    alpha_anneal_iter=conf[\"alpha_anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter= 1000,\n",
    "    batch_size=conf[\"batch_size\"],\n",
    "    num_workers=conf[\"num_workers\"],\n",
    "    label_pct=0.5,\n",
    "    chem_prop = chem_prop\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of google_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
