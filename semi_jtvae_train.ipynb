{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr:0.001\n",
    "# anneal_rate:0.9\n",
    "# batch_size:32\n",
    "# clip_norm:50\n",
    "# num_epochs:5\n",
    "# alpha:250\n",
    "# beta:0\n",
    "# max_beta:1\n",
    "# step_beta:0.002\n",
    "# anneal_iter:40000\n",
    "# kl_anneal_iter:2000\n",
    "# print_iter:100\n",
    "# save_iter:5000\n",
    "# num_workers:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "gradient": {
     "editing": false,
     "id": "18de9aeb-6551-42f6-8c11-a0e30bd207d6",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "JDinHdioUZRH",
    "outputId": "1a824742-d528-46a2-8d89-7697a166c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu116.html\n",
    "!pip install -q dive-into-graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q toolz\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f728113a-ff43-466d-b34d-77c9b2e11478",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "_RKgd8MsYOYh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pickle \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from molecule_optimizer.externals.fast_jtnn.datautils import SemiMolTreeFolder, SemiMolTreeFolderTest\n",
    "from molecule_optimizer.runner.semi_jtvae import SemiJTVAEGeneratorPredictor\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "\n",
    "import rdkit\n",
    "\n",
    "lg = rdkit.RDLogger.logger() \n",
    "lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6e85f4e2-eab6-4eae-b452-7f089e176039",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "2C4erValhPS-"
   },
   "outputs": [],
   "source": [
    "conf = json.load(open(\"training/configs/rand_gen_zinc250k_config_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": true,
     "id": "30eb7f71-c38e-49f6-8d84-715ff3d80e08",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "SuzGX7ClhKaf",
    "outputId": "ab764f6b-4dd7-4ffc-b6cf-bae47d6cedf7"
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"ZINC_310k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = csv['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = smiles[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(csv['QED'][:60000]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'runner.xml' not in os.listdir(\".\"):\n",
    "#     runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "#     with open('runner.xml', 'wb') as f:\n",
    "#         pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'runner_20.xml' not in os.listdir(\".\"):\n",
    "    runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "    with open('runner_20.xml', 'wb') as f:\n",
    "        pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fc02ae3d-696b-4c2e-b557-760e6a1a75a9",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "uTNMpfWD7b7Q"
   },
   "outputs": [],
   "source": [
    "# with open('runner.xml', 'rb') as f:\n",
    "#     runner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fc02ae3d-696b-4c2e-b557-760e6a1a75a9",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "uTNMpfWD7b7Q"
   },
   "outputs": [],
   "source": [
    "with open('runner_20.xml', 'rb') as f:\n",
    "    runner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "4ccc9136-0e87-470b-90eb-8e0b92da52bc",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "0-1xXVU15z6N",
    "outputId": "c6c328fe-72e4-4401-bce6-8613e7e9319f"
   },
   "outputs": [],
   "source": [
    "runner.get_model(\n",
    "    \"rand_gen\",\n",
    "    {\n",
    "        \"hidden_size\": conf[\"model\"][\"hidden_size\"],\n",
    "        \"latent_size\": conf[\"model\"][\"latent_size\"],\n",
    "        \"depthT\": conf[\"model\"][\"depthT\"],\n",
    "        \"depthG\": conf[\"model\"][\"depthG\"],\n",
    "        \"label_size\": 1,\n",
    "        \"label_mean\": float(torch.mean(labels)),\n",
    "        \"label_var\": float(torch.var(labels)),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c177219a-298a-4994-98b9-de76833e14fe",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "IGnpfkM_KQXi"
   },
   "outputs": [],
   "source": [
    "labels = runner.get_processed_labels(labels)\n",
    "preprocessed = runner.processed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_TEST = 10000\n",
    "N_TEST = 200\n",
    "VAL_FRAC = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_id=np.random.permutation(len(labels))\n",
    "X_train = preprocessed[perm_id[N_TEST:]]\n",
    "L_train = torch.tensor(labels.numpy()[perm_id[N_TEST:]])\n",
    "\n",
    "X_test = preprocessed[perm_id[:N_TEST]]\n",
    "L_test = torch.tensor(labels.numpy()[perm_id[:N_TEST]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cut = math.floor(len(X_train) * VAL_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Val = X_train[:val_cut]\n",
    "L_Val = L_train[:val_cut]\n",
    "\n",
    "X_train = X_train[val_cut :]\n",
    "L_train = L_train[val_cut :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7cb9e705-09fa-4075-a487-21887ecaeb95",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "TMxgCK1Y20mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model #Params: 4732K\n",
      "[Train][100] Alpha: 0.000, Beta: 0.000, Loss: 110.60, KL: 373.92, MAE: 0.53291, Word Loss: 82.77, Topo Loss: 19.30, Assm Loss: 8.53, Pred Loss: 14.03, Word: 32.38, Topo: 85.16, Assm: 57.06, PNorm: 102.09, GNorm: 50.00\n",
      "[Train][200] Alpha: 0.000, Beta: 0.000, Loss: 68.58, KL: 406.83, MAE: 0.26997, Word Loss: 50.04, Topo Loss: 10.68, Assm Loss: 7.86, Pred Loss: 3.98, Word: 56.29, Topo: 92.66, Assm: 60.93, PNorm: 107.18, GNorm: 42.09\n",
      "[Train][300] Alpha: 0.000, Beta: 0.000, Loss: 57.56, KL: 501.76, MAE: 0.18050, Word Loss: 41.02, Topo Loss: 9.15, Assm Loss: 7.40, Pred Loss: 1.89, Word: 64.55, Topo: 93.61, Assm: 63.43, PNorm: 110.76, GNorm: 42.52\n",
      "[Train][400] Alpha: 0.000, Beta: 0.000, Loss: 50.76, KL: 604.05, MAE: 0.17801, Word Loss: 35.98, Topo Loss: 7.92, Assm Loss: 6.85, Pred Loss: 1.93, Word: 68.50, Topo: 94.40, Assm: 65.57, PNorm: 114.45, GNorm: 50.00\n",
      "learning rate: 0.000729\n",
      "[Train][500] Alpha: 0.000, Beta: 0.010, Loss: 47.54, KL: 366.21, MAE: 0.17088, Word Loss: 32.32, Topo Loss: 7.22, Assm Loss: 6.10, Pred Loss: 1.82, Word: 71.02, Topo: 94.89, Assm: 69.98, PNorm: 117.54, GNorm: 49.03\n",
      "[Train][600] Alpha: 0.000, Beta: 0.020, Loss: 45.04, KL: 192.83, MAE: 0.16554, Word Loss: 29.68, Topo Loss: 6.83, Assm Loss: 5.51, Pred Loss: 1.69, Word: 73.02, Topo: 95.18, Assm: 73.44, PNorm: 120.85, GNorm: 50.00\n",
      "[Train][700] Alpha: 25.000, Beta: 0.030, Loss: 53.41, KL: 143.32, MAE: 0.11002, Word Loss: 27.60, Topo Loss: 6.76, Assm Loss: 5.07, Pred Loss: 0.77, Word: 74.50, Topo: 95.13, Assm: 76.25, PNorm: 123.31, GNorm: 50.00\n",
      "[Train][800] Alpha: 50.000, Beta: 0.040, Loss: 57.36, KL: 122.83, MAE: 0.08040, Word Loss: 26.36, Topo Loss: 5.65, Assm Loss: 4.57, Pred Loss: 0.41, Word: 75.37, Topo: 95.95, Assm: 78.90, PNorm: 125.35, GNorm: 50.00\n",
      "learning rate: 0.000656\n",
      "[Train][900] Alpha: 75.000, Beta: 0.050, Loss: 60.01, KL: 111.48, MAE: 0.06811, Word Loss: 25.67, Topo Loss: 5.45, Assm Loss: 4.12, Pred Loss: 0.30, Word: 75.82, Topo: 96.21, Assm: 81.15, PNorm: 126.89, GNorm: 50.00\n",
      "[Train][1000] Alpha: 100.000, Beta: 0.060, Loss: 61.50, KL: 102.10, MAE: 0.06137, Word Loss: 25.17, Topo Loss: 5.23, Assm Loss: 3.88, Pred Loss: 0.24, Word: 76.15, Topo: 96.31, Assm: 82.47, PNorm: 128.45, GNorm: 50.00\n",
      "[Train][1100] Alpha: 125.000, Beta: 0.070, Loss: 63.47, KL: 93.95, MAE: 0.05636, Word Loss: 24.75, Topo Loss: 5.04, Assm Loss: 3.71, Pred Loss: 0.21, Word: 76.30, Topo: 96.42, Assm: 83.48, PNorm: 129.94, GNorm: 50.00\n",
      "[Train][1200] Alpha: 150.000, Beta: 0.080, Loss: 64.65, KL: 88.54, MAE: 0.05326, Word Loss: 24.32, Topo Loss: 4.82, Assm Loss: 3.49, Pred Loss: 0.18, Word: 76.72, Topo: 96.63, Assm: 84.79, PNorm: 131.31, GNorm: 50.00\n",
      "learning rate: 0.000590\n",
      "[Train][1300] Alpha: 175.000, Beta: 0.090, Loss: 60.54, KL: 83.62, MAE: 0.04520, Word Loss: 23.84, Topo Loss: 4.50, Assm Loss: 3.24, Pred Loss: 0.13, Word: 76.99, Topo: 96.87, Assm: 85.83, PNorm: 132.45, GNorm: 50.00\n",
      "[Train][1400] Alpha: 200.000, Beta: 0.100, Loss: 63.70, KL: 77.91, MAE: 0.04484, Word Loss: 23.73, Topo Loss: 4.40, Assm Loss: 3.15, Pred Loss: 0.13, Word: 77.06, Topo: 96.96, Assm: 86.23, PNorm: 133.56, GNorm: 50.00\n",
      "[Train][1500] Alpha: 225.000, Beta: 0.110, Loss: 61.44, KL: 74.69, MAE: 0.03998, Word Loss: 23.50, Topo Loss: 4.26, Assm Loss: 3.05, Pred Loss: 0.11, Word: 76.95, Topo: 97.06, Assm: 86.82, PNorm: 134.73, GNorm: 50.00\n",
      "[Validation][87] Alpha: 230.000, Beta: 0.112, Loss: 105.65, KL: 35.60, MAE: 0.07012, Word Loss: 17.21, Topo Loss: 4.47, Assm Loss: 3.63, Pred Loss: 0.33, Word: 70.79, Topo: 94.46, Assm: 72.06\n",
      "[Train][1600] Alpha: 250.000, Beta: 0.120, Loss: 62.06, KL: 71.42, MAE: 0.03927, Word Loss: 23.04, Topo Loss: 4.05, Assm Loss: 2.89, Pred Loss: 0.10, Word: 77.47, Topo: 97.23, Assm: 87.31, PNorm: 135.65, GNorm: 50.00\n",
      "learning rate: 0.000531\n",
      "[Train][1700] Alpha: 275.000, Beta: 0.130, Loss: 56.27, KL: 68.51, MAE: 0.03312, Word Loss: 22.54, Topo Loss: 3.85, Assm Loss: 2.79, Pred Loss: 0.07, Word: 77.81, Topo: 97.36, Assm: 88.13, PNorm: 136.59, GNorm: 50.00\n",
      "[Train][1800] Alpha: 300.000, Beta: 0.140, Loss: 55.10, KL: 65.28, MAE: 0.03014, Word Loss: 22.33, Topo Loss: 3.69, Assm Loss: 2.58, Pred Loss: 0.06, Word: 77.80, Topo: 97.49, Assm: 88.83, PNorm: 137.62, GNorm: 50.00\n",
      "[Train][1900] Alpha: 325.000, Beta: 0.150, Loss: 54.33, KL: 62.61, MAE: 0.02900, Word Loss: 21.84, Topo Loss: 3.61, Assm Loss: 2.45, Pred Loss: 0.06, Word: 78.32, Topo: 97.57, Assm: 89.75, PNorm: 138.57, GNorm: 50.00\n",
      "[Train][2000] Alpha: 350.000, Beta: 0.160, Loss: 58.75, KL: 59.10, MAE: 0.03197, Word Loss: 21.49, Topo Loss: 3.31, Assm Loss: 2.31, Pred Loss: 0.07, Word: 78.58, Topo: 97.80, Assm: 90.77, PNorm: 139.22, GNorm: 50.00\n",
      "learning rate: 0.000478\n",
      "[Train][2100] Alpha: 375.000, Beta: 0.170, Loss: 54.85, KL: 58.25, MAE: 0.02846, Word Loss: 20.93, Topo Loss: 3.26, Assm Loss: 2.23, Pred Loss: 0.05, Word: 79.35, Topo: 97.80, Assm: 91.07, PNorm: 139.87, GNorm: 50.00\n",
      "[Train][2200] Alpha: 400.000, Beta: 0.180, Loss: 49.73, KL: 56.83, MAE: 0.02271, Word Loss: 21.12, Topo Loss: 3.27, Assm Loss: 2.18, Pred Loss: 0.03, Word: 78.83, Topo: 97.81, Assm: 91.49, PNorm: 140.87, GNorm: 50.00\n",
      "[Train][2300] Alpha: 425.000, Beta: 0.190, Loss: 52.53, KL: 54.32, MAE: 0.02476, Word Loss: 20.72, Topo Loss: 3.12, Assm Loss: 2.13, Pred Loss: 0.04, Word: 79.18, Topo: 97.92, Assm: 91.51, PNorm: 141.64, GNorm: 50.00\n",
      "[Train][2400] Alpha: 450.000, Beta: 0.200, Loss: 51.01, KL: 52.92, MAE: 0.02307, Word Loss: 20.30, Topo Loss: 2.94, Assm Loss: 2.06, Pred Loss: 0.03, Word: 79.56, Topo: 98.06, Assm: 91.68, PNorm: 142.53, GNorm: 50.00\n",
      "learning rate: 0.000430\n",
      "[Train][2500] Alpha: 475.000, Beta: 0.210, Loss: 51.79, KL: 50.90, MAE: 0.02321, Word Loss: 20.07, Topo Loss: 2.81, Assm Loss: 1.89, Pred Loss: 0.04, Word: 79.72, Topo: 98.14, Assm: 92.56, PNorm: 143.26, GNorm: 50.00\n",
      "[Train][2600] Alpha: 500.000, Beta: 0.220, Loss: 51.26, KL: 50.40, MAE: 0.02280, Word Loss: 19.65, Topo Loss: 2.61, Assm Loss: 1.64, Pred Loss: 0.03, Word: 80.09, Topo: 98.29, Assm: 94.12, PNorm: 143.92, GNorm: 50.00\n",
      "[Train][2700] Alpha: 500.000, Beta: 0.230, Loss: 56.66, KL: 47.71, MAE: 0.02605, Word Loss: 19.41, Topo Loss: 2.56, Assm Loss: 1.76, Pred Loss: 0.04, Word: 80.37, Topo: 98.32, Assm: 93.39, PNorm: 144.53, GNorm: 50.00\n",
      "[Train][2800] Alpha: 500.000, Beta: 0.240, Loss: 51.26, KL: 47.50, MAE: 0.02228, Word Loss: 19.56, Topo Loss: 2.43, Assm Loss: 1.72, Pred Loss: 0.03, Word: 80.08, Topo: 98.43, Assm: 93.18, PNorm: 145.33, GNorm: 50.00\n",
      "learning rate: 0.000387\n",
      "[Train][2900] Alpha: 500.000, Beta: 0.250, Loss: 51.85, KL: 46.15, MAE: 0.02283, Word Loss: 19.08, Topo Loss: 2.30, Assm Loss: 1.61, Pred Loss: 0.04, Word: 80.34, Topo: 98.54, Assm: 94.12, PNorm: 146.09, GNorm: 50.00\n",
      "[Train][3000] Alpha: 500.000, Beta: 0.260, Loss: 53.18, KL: 44.32, MAE: 0.02322, Word Loss: 18.72, Topo Loss: 3.78, Assm Loss: 1.57, Pred Loss: 0.04, Word: 80.89, Topo: 97.57, Assm: 94.26, PNorm: 146.83, GNorm: 50.00\n",
      "[Validation][87] Alpha: 500.000, Beta: 0.262, Loss: 191.37, KL: 21.65, MAE: 0.06561, Word Loss: 19.77, Topo Loss: 9.64, Assm Loss: 5.77, Pred Loss: 0.30, Word: 68.89, Topo: 91.18, Assm: 69.90\n",
      "[Train][3100] Alpha: 500.000, Beta: 0.270, Loss: 53.42, KL: 43.09, MAE: 0.02285, Word Loss: 18.89, Topo Loss: 3.87, Assm Loss: 1.61, Pred Loss: 0.04, Word: 80.35, Topo: 97.51, Assm: 93.80, PNorm: 147.44, GNorm: 50.00\n",
      "[Train][3200] Alpha: 500.000, Beta: 0.280, Loss: 45.25, KL: 42.23, MAE: 0.01890, Word Loss: 18.25, Topo Loss: 2.15, Assm Loss: 1.78, Pred Loss: 0.02, Word: 81.20, Topo: 98.62, Assm: 93.05, PNorm: 148.10, GNorm: 50.00\n",
      "learning rate: 0.000349\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred(\n",
    "    X_train,\n",
    "    L_train,\n",
    "    X_Val,\n",
    "    L_Val,\n",
    "    load_epoch=0,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=conf[\"alpha\"],\n",
    "    max_alpha=conf[\"max_alpha\"],\n",
    "    step_alpha=conf[\"step_alpha\"],\n",
    "    beta=conf[\"beta\"],\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    alpha_anneal_iter=conf[\"alpha_anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter=conf[\"save_iter\"],\n",
    "    batch_size=conf[\"batch_size\"],\n",
    "    num_workers=conf[\"num_workers\"],\n",
    "    label_pct=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred_supervised(\n",
    "    loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    load_epoch=0,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=conf[\"alpha\"],\n",
    "    max_alpha=conf[\"max_alpha\"],\n",
    "    step_alpha=conf[\"step_alpha\"],\n",
    "    beta=conf[\"beta\"],\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    alpha_anneal_iter=conf[\"alpha_anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter=conf[\"save_iter\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of google_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
