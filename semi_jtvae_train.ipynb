{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "gradient": {
     "editing": false,
     "id": "18de9aeb-6551-42f6-8c11-a0e30bd207d6",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "JDinHdioUZRH",
    "outputId": "1a824742-d528-46a2-8d89-7697a166c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu116.html\n",
    "!pip install -q dive-into-graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q toolz\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f728113a-ff43-466d-b34d-77c9b2e11478",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "_RKgd8MsYOYh"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pickle \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from molecule_optimizer.externals.fast_jtnn.datautils import SemiMolTreeFolder, SemiMolTreeFolderTest\n",
    "from molecule_optimizer.runner.semi_jtvae import SemiJTVAEGeneratorPredictor\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "\n",
    "import rdkit\n",
    "\n",
    "lg = rdkit.RDLogger.logger() \n",
    "lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6e85f4e2-eab6-4eae-b452-7f089e176039",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "2C4erValhPS-"
   },
   "outputs": [],
   "source": [
    "conf = json.load(open(\"training/configs/rand_gen_zinc250k_config_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": true,
     "id": "30eb7f71-c38e-49f6-8d84-715ff3d80e08",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "SuzGX7ClhKaf",
    "outputId": "ab764f6b-4dd7-4ffc-b6cf-bae47d6cedf7"
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"ZINC_310k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = csv['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smiles = smiles[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torch.tensor(csv['LogP'][:60000]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(csv['LogP']).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_TEST = 10000\n",
    "N_TEST = 200\n",
    "VAL_FRAC = 0.05\n",
    "chem_prop = \"LogP\"\n",
    "load_epoch = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [01:57<00:00,  1.89s/it]\n",
      "100%|██████████| 62/62 [1:06:51<00:00, 64.70s/it] \n"
     ]
    }
   ],
   "source": [
    "# if 'runner.xml' not in os.listdir(\".\"):\n",
    "#     runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "#     processed_smiles, processed_idxs = SemiJTVAEGeneratorPredictor.preprocess(smiles) \n",
    "#     with open('runner.xml', 'wb') as f:\n",
    "#         pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:35<00:00,  2.93s/it]\n",
      "100%|██████████| 12/12 [19:09<00:00, 95.82s/it]\n"
     ]
    }
   ],
   "source": [
    "# if 'runner_20.xml' not in os.listdir(\".\"):\n",
    "#     runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "#     processed_smiles, processed_idxs = SemiJTVAEGeneratorPredictor.preprocess(smiles) \n",
    "#     with open('runner_20.xml', 'wb') as f:\n",
    "#         pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fc02ae3d-696b-4c2e-b557-760e6a1a75a9",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "uTNMpfWD7b7Q"
   },
   "outputs": [],
   "source": [
    "with open('saved/runner_LogP_50_1_iter_60000.xml', 'rb') as f:\n",
    "    runner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "4ccc9136-0e87-470b-90eb-8e0b92da52bc",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "0-1xXVU15z6N",
    "outputId": "c6c328fe-72e4-4401-bce6-8613e7e9319f"
   },
   "outputs": [],
   "source": [
    "runner.get_model(\n",
    "    \"rand_gen\",\n",
    "    {\n",
    "        \"hidden_size\": conf[\"model\"][\"hidden_size\"],\n",
    "        \"latent_size\": conf[\"model\"][\"latent_size\"],\n",
    "        \"depthT\": conf[\"model\"][\"depthT\"],\n",
    "        \"depthG\": conf[\"model\"][\"depthG\"],\n",
    "        \"label_size\": 1,\n",
    "        \"label_mean\": float(torch.mean(labels)),\n",
    "        \"label_var\": float(torch.var(labels)),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c177219a-298a-4994-98b9-de76833e14fe",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "IGnpfkM_KQXi"
   },
   "outputs": [],
   "source": [
    "labels = runner.get_processed_labels(labels, processed_idxs)\n",
    "preprocessed = processed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perm_id=np.random.permutation(len(labels))\n",
    "\n",
    "X_train = preprocessed[perm_id[N_TEST:]]\n",
    "X_train_smiles = smiles[perm_id[N_TEST:]]\n",
    "L_train = torch.tensor(labels.numpy()[perm_id[N_TEST:]])\n",
    "\n",
    "\n",
    "X_test = preprocessed[perm_id[:N_TEST]]\n",
    "X_test_smiles = smiles[perm_id[:N_TEST]]\n",
    "L_test = torch.tensor(labels.numpy()[perm_id[:N_TEST]])\n",
    "\n",
    "val_cut = math.floor(len(X_train) * VAL_FRAC)\n",
    "\n",
    "X_Val = X_train[:val_cut]\n",
    "X_Val_smiles = X_train_smiles[:val_cut]\n",
    "L_Val = L_train[:val_cut]\n",
    "\n",
    "X_train = X_train[val_cut :]\n",
    "X_train_smiles = X_train_smiles[val_cut :]\n",
    "L_train = L_train[val_cut :]\n",
    "\n",
    "with open(\"train_smiles_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_train_smiles)\n",
    "\n",
    "with open(\"test_smiles_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_test_smiles)\n",
    "\n",
    "with open(\"validation_smiles_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_Val_smiles)\n",
    "\n",
    "#save preproccessed\n",
    "\n",
    "with open(\"train_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "\n",
    "with open(\"test_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_test)\n",
    "\n",
    "with open(\"validation_\" + chem_prop + \"_50_1.npy\", 'wb') as f:\n",
    "    np.save(f, X_Val)\n",
    "\n",
    "#Save labels\n",
    "\n",
    "torch.save(L_train, \"L_train_\" + chem_prop + \"_50_1.pt\")\n",
    "\n",
    "torch.save(L_test, \"L_test_\" + chem_prop + \"_50_1.pt\")\n",
    "\n",
    "torch.save(L_Val, \"L_Val_\" + chem_prop + \"_50_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_train = torch.load(\"L_train_\" + chem_prop + \"_50_1.pt\")\n",
    "L_test = torch.load(\"L_test_\" + chem_prop + \"_50_1.pt\")\n",
    "L_Val = torch.load(\"L_Val_\" + chem_prop + \"_50_1.pt\")\n",
    "\n",
    "with open(\"train_\" + chem_prop + \"_50_1.npy\", 'rb') as f:\n",
    "    X_train = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(\"test_\" + chem_prop + \"_50_1.npy\", 'rb') as f:\n",
    "    X_test = np.load(f, allow_pickle=True)\n",
    "\n",
    "with open(\"validation_\" + chem_prop + \"_50_1.npy\", 'rb') as f:\n",
    "    X_Val = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7cb9e705-09fa-4075-a487-21887ecaeb95",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "TMxgCK1Y20mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model #Params: 4732K\n",
      "[Train][1100] Alpha: 0.000, Beta: 0.000, Loss: 40.79, KL: 679.81, MAE: 0.85188, Word Loss: 28.13, Topo Loss: 7.37, Assm Loss: 5.29, Pred Loss: 0.84, Word: 75.06, Topo: 94.72, Assm: 75.16, PNorm: 153.08, GNorm: 47.52\n",
      "[Train][1200] Alpha: 0.000, Beta: 0.000, Loss: 39.47, KL: 717.46, MAE: 0.84108, Word Loss: 27.26, Topo Loss: 6.95, Assm Loss: 5.26, Pred Loss: 0.81, Word: 76.19, Topo: 95.11, Assm: 75.32, PNorm: 157.58, GNorm: 35.72\n",
      "[Train][1300] Alpha: 0.000, Beta: 0.000, Loss: 37.01, KL: 750.64, MAE: 0.85575, Word Loss: 25.91, Topo Loss: 6.35, Assm Loss: 4.75, Pred Loss: 0.83, Word: 76.89, Topo: 95.61, Assm: 77.72, PNorm: 161.90, GNorm: 50.00\n",
      "[Train][1400] Alpha: 0.000, Beta: 0.000, Loss: 36.21, KL: 796.35, MAE: 0.81510, Word Loss: 25.03, Topo Loss: 6.43, Assm Loss: 4.75, Pred Loss: 0.78, Word: 77.64, Topo: 95.35, Assm: 77.52, PNorm: 164.86, GNorm: 42.02\n",
      "[Train][1500] Alpha: 0.000, Beta: 0.000, Loss: 34.63, KL: 834.32, MAE: 0.76559, Word Loss: 23.85, Topo Loss: 6.05, Assm Loss: 4.73, Pred Loss: 0.67, Word: 78.38, Topo: 95.80, Assm: 77.69, PNorm: 168.37, GNorm: 42.75\n",
      "[Train][1600] Alpha: 0.000, Beta: 0.000, Loss: 34.58, KL: 861.93, MAE: 0.81207, Word Loss: 23.79, Topo Loss: 6.16, Assm Loss: 4.63, Pred Loss: 0.80, Word: 78.49, Topo: 95.71, Assm: 79.40, PNorm: 170.91, GNorm: 49.97\n",
      "[Train][1700] Alpha: 0.000, Beta: 0.000, Loss: 33.73, KL: 893.15, MAE: 0.80064, Word Loss: 23.19, Topo Loss: 5.97, Assm Loss: 4.56, Pred Loss: 0.74, Word: 79.25, Topo: 95.82, Assm: 78.87, PNorm: 173.61, GNorm: 50.00\n",
      "[Train][1800] Alpha: 0.000, Beta: 0.000, Loss: 32.95, KL: 914.03, MAE: 0.78595, Word Loss: 22.84, Topo Loss: 5.81, Assm Loss: 4.30, Pred Loss: 0.72, Word: 79.55, Topo: 95.93, Assm: 79.65, PNorm: 176.19, GNorm: 50.00\n",
      "[Train][1900] Alpha: 0.000, Beta: 0.000, Loss: 31.73, KL: 948.40, MAE: 0.75512, Word Loss: 21.94, Topo Loss: 5.57, Assm Loss: 4.22, Pred Loss: 0.66, Word: 80.15, Topo: 96.18, Assm: 80.32, PNorm: 178.73, GNorm: 50.00\n",
      "[Train][2000] Alpha: 0.000, Beta: 0.000, Loss: 31.59, KL: 947.83, MAE: 0.81451, Word Loss: 21.71, Topo Loss: 5.49, Assm Loss: 4.39, Pred Loss: 0.75, Word: 80.62, Topo: 96.14, Assm: 80.13, PNorm: 181.34, GNorm: 50.00\n",
      "[Train][2100] Alpha: 0.000, Beta: 0.000, Loss: 29.78, KL: 991.44, MAE: 0.82339, Word Loss: 20.49, Topo Loss: 5.33, Assm Loss: 3.96, Pred Loss: 0.78, Word: 81.06, Topo: 96.22, Assm: 81.89, PNorm: 183.97, GNorm: 50.00\n",
      "[Train][2200] Alpha: 0.000, Beta: 0.000, Loss: 29.85, KL: 1008.41, MAE: 0.84411, Word Loss: 20.66, Topo Loss: 5.21, Assm Loss: 3.98, Pred Loss: 0.81, Word: 81.06, Topo: 96.35, Assm: 81.42, PNorm: 186.66, GNorm: 49.27\n",
      "[Train][2300] Alpha: 0.000, Beta: 0.000, Loss: 27.74, KL: 1033.64, MAE: 0.84545, Word Loss: 19.35, Topo Loss: 4.81, Assm Loss: 3.59, Pred Loss: 0.84, Word: 81.95, Topo: 96.71, Assm: 83.60, PNorm: 188.94, GNorm: 50.00\n",
      "[Train][2400] Alpha: 0.000, Beta: 0.000, Loss: 28.53, KL: 1053.37, MAE: 0.76251, Word Loss: 19.47, Topo Loss: 5.22, Assm Loss: 3.85, Pred Loss: 0.67, Word: 82.29, Topo: 96.35, Assm: 81.99, PNorm: 191.03, GNorm: 50.00\n",
      "[Train][2500] Alpha: 0.000, Beta: 0.000, Loss: 27.39, KL: 1088.39, MAE: 0.79701, Word Loss: 18.73, Topo Loss: 4.73, Assm Loss: 3.94, Pred Loss: 0.74, Word: 82.46, Topo: 96.75, Assm: 82.32, PNorm: 193.14, GNorm: 50.00\n"
     ]
    }
   ],
   "source": [
    "# print(\"Training model...\")\n",
    "# runner.train_gen_pred(\n",
    "#     X_train,\n",
    "#     L_train,\n",
    "#     X_test,\n",
    "#     L_test,\n",
    "#     X_Val,\n",
    "#     L_Val,\n",
    "#     load_epoch= 1000,\n",
    "#     lr=conf[\"lr\"],\n",
    "#     anneal_rate=conf[\"anneal_rate\"],\n",
    "#     clip_norm=conf[\"clip_norm\"],\n",
    "#     num_epochs=conf[\"num_epochs\"],\n",
    "#     alpha=conf[\"alpha\"],\n",
    "#     max_alpha=conf[\"max_alpha\"],\n",
    "#     step_alpha=conf[\"step_alpha\"],\n",
    "#     beta=conf[\"beta\"],\n",
    "#     max_beta=conf[\"max_beta\"],\n",
    "#     step_beta=conf[\"step_beta\"],\n",
    "#     anneal_iter=conf[\"anneal_iter\"],\n",
    "#     alpha_anneal_iter=conf[\"alpha_anneal_iter\"],\n",
    "#     kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "#     print_iter=100,\n",
    "#     save_iter= 1000,\n",
    "#     batch_size=conf[\"batch_size\"],\n",
    "#     num_workers=conf[\"num_workers\"],\n",
    "#     label_pct=0.5,\n",
    "#     chem_prop = \"LogP\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model #Params: 5207K\n",
      "[Train][60100] Alpha: 10.000, Beta: 0.000, Loss: 11.58, KL: 162.05, MAE: 0.83144, Word Loss: 2.36, Topo Loss: 0.56, Assm Loss: 0.37, Pred Loss: 0.83, Word: 95.02, Topo: 99.30, Assm: 97.57, PNorm: 576.94, GNorm: 46.43\n",
      "[Train][60200] Alpha: 10.000, Beta: 0.000, Loss: 5.69, KL: 216.03, MAE: 0.45162, Word Loss: 2.35, Topo Loss: 0.54, Assm Loss: 0.42, Pred Loss: 0.24, Word: 94.80, Topo: 99.32, Assm: 97.20, PNorm: 578.20, GNorm: 50.00\n",
      "[Train][60300] Alpha: 10.000, Beta: 0.000, Loss: 4.60, KL: 223.31, MAE: 0.34493, Word Loss: 2.27, Topo Loss: 0.52, Assm Loss: 0.40, Pred Loss: 0.14, Word: 94.88, Topo: 99.31, Assm: 97.10, PNorm: 579.17, GNorm: 49.28\n",
      "[Train][60400] Alpha: 10.000, Beta: 0.000, Loss: 4.11, KL: 232.05, MAE: 0.29693, Word Loss: 2.13, Topo Loss: 0.52, Assm Loss: 0.39, Pred Loss: 0.11, Word: 95.38, Topo: 99.34, Assm: 97.39, PNorm: 579.99, GNorm: 23.14\n",
      "[Train][60500] Alpha: 10.000, Beta: 0.000, Loss: 4.02, KL: 234.51, MAE: 0.27260, Word Loss: 2.23, Topo Loss: 0.48, Assm Loss: 0.41, Pred Loss: 0.09, Word: 95.13, Topo: 99.37, Assm: 97.00, PNorm: 580.96, GNorm: 29.99\n",
      "[Train][60600] Alpha: 10.000, Beta: 0.000, Loss: 3.98, KL: 234.18, MAE: 0.25605, Word Loss: 2.32, Topo Loss: 0.52, Assm Loss: 0.33, Pred Loss: 0.08, Word: 94.95, Topo: 99.36, Assm: 97.33, PNorm: 582.06, GNorm: 34.70\n",
      "[Train][60700] Alpha: 10.000, Beta: 0.000, Loss: 3.52, KL: 239.60, MAE: 0.22116, Word Loss: 2.15, Topo Loss: 0.48, Assm Loss: 0.26, Pred Loss: 0.06, Word: 95.16, Topo: 99.35, Assm: 97.93, PNorm: 583.40, GNorm: 31.44\n",
      "[Train][60800] Alpha: 10.000, Beta: 0.000, Loss: 3.74, KL: 245.74, MAE: 0.22573, Word Loss: 2.29, Topo Loss: 0.47, Assm Loss: 0.35, Pred Loss: 0.06, Word: 94.91, Topo: 99.39, Assm: 97.41, PNorm: 584.09, GNorm: 20.47\n",
      "[Train][60900] Alpha: 10.000, Beta: 0.000, Loss: 3.53, KL: 245.49, MAE: 0.21461, Word Loss: 2.20, Topo Loss: 0.46, Assm Loss: 0.28, Pred Loss: 0.06, Word: 95.14, Topo: 99.36, Assm: 97.71, PNorm: 585.09, GNorm: 20.58\n",
      "[Train][61000] Alpha: 10.000, Beta: 0.000, Loss: 3.67, KL: 251.84, MAE: 0.21166, Word Loss: 2.26, Topo Loss: 0.53, Assm Loss: 0.33, Pred Loss: 0.06, Word: 95.06, Topo: 99.30, Assm: 97.37, PNorm: 586.03, GNorm: 46.39\n",
      "[Train][61100] Alpha: 10.000, Beta: 0.000, Loss: 3.49, KL: 256.02, MAE: 0.20098, Word Loss: 2.15, Topo Loss: 0.51, Assm Loss: 0.34, Pred Loss: 0.05, Word: 95.25, Topo: 99.35, Assm: 97.81, PNorm: 586.91, GNorm: 29.96\n",
      "[Train][61200] Alpha: 10.000, Beta: 0.000, Loss: 3.63, KL: 263.57, MAE: 0.19868, Word Loss: 2.31, Topo Loss: 0.48, Assm Loss: 0.32, Pred Loss: 0.05, Word: 94.97, Topo: 99.36, Assm: 97.43, PNorm: 588.02, GNorm: 26.04\n",
      "[Train][61300] Alpha: 10.000, Beta: 0.000, Loss: 3.24, KL: 263.77, MAE: 0.18630, Word Loss: 2.06, Topo Loss: 0.47, Assm Loss: 0.29, Pred Loss: 0.04, Word: 95.37, Topo: 99.41, Assm: 97.73, PNorm: 588.83, GNorm: 37.44\n",
      "[Train][61400] Alpha: 10.000, Beta: 0.000, Loss: 3.22, KL: 266.19, MAE: 0.18321, Word Loss: 2.07, Topo Loss: 0.49, Assm Loss: 0.26, Pred Loss: 0.04, Word: 95.52, Topo: 99.33, Assm: 97.80, PNorm: 589.43, GNorm: 36.43\n",
      "[Train][61500] Alpha: 10.000, Beta: 0.000, Loss: 3.48, KL: 269.68, MAE: 0.18183, Word Loss: 2.26, Topo Loss: 0.50, Assm Loss: 0.30, Pred Loss: 0.04, Word: 95.21, Topo: 99.33, Assm: 97.66, PNorm: 590.36, GNorm: 30.31\n",
      "[Train][61600] Alpha: 10.000, Beta: 0.000, Loss: 3.37, KL: 272.33, MAE: 0.18751, Word Loss: 2.18, Topo Loss: 0.48, Assm Loss: 0.26, Pred Loss: 0.04, Word: 95.28, Topo: 99.38, Assm: 97.79, PNorm: 590.91, GNorm: 25.88\n",
      "[Train][61700] Alpha: 10.000, Beta: 0.000, Loss: 3.26, KL: 275.05, MAE: 0.17937, Word Loss: 2.12, Topo Loss: 0.44, Assm Loss: 0.31, Pred Loss: 0.04, Word: 95.38, Topo: 99.42, Assm: 97.53, PNorm: 591.52, GNorm: 23.06\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred_supervised(\n",
    "    X_train,\n",
    "    L_train,\n",
    "    X_test,\n",
    "    L_test,\n",
    "    X_Val,\n",
    "    L_Val,\n",
    "    load_epoch= load_epoch,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=10.0,\n",
    "    max_alpha=conf[\"max_alpha\"],\n",
    "    step_alpha=conf[\"step_alpha\"],\n",
    "    beta=conf[\"beta\"],\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    alpha_anneal_iter=conf[\"alpha_anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter= 1000,\n",
    "    batch_size=conf[\"batch_size\"],\n",
    "    num_workers=conf[\"num_workers\"],\n",
    "    label_pct=0.5,\n",
    "    chem_prop = chem_prop\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of google_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
