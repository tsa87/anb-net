{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "gradient": {
     "editing": false,
     "id": "18de9aeb-6551-42f6-8c11-a0e30bd207d6",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "JDinHdioUZRH",
    "outputId": "1a824742-d528-46a2-8d89-7697a166c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu116.html\n",
    "!pip install -q dive-into-graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q toolz\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f728113a-ff43-466d-b34d-77c9b2e11478",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "_RKgd8MsYOYh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pickle \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from molecule_optimizer.externals.fast_jtnn.datautils import SemiMolTreeFolder, SemiMolTreeFolderTest\n",
    "from molecule_optimizer.runner.semi_jtvae import SemiJTVAEGeneratorPredictor\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "\n",
    "import rdkit\n",
    "\n",
    "lg = rdkit.RDLogger.logger() \n",
    "lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6e85f4e2-eab6-4eae-b452-7f089e176039",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "2C4erValhPS-"
   },
   "outputs": [],
   "source": [
    "conf = json.load(open(\"training/configs/rand_gen_zinc250k_config_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": true,
     "id": "30eb7f71-c38e-49f6-8d84-715ff3d80e08",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "SuzGX7ClhKaf",
    "outputId": "ab764f6b-4dd7-4ffc-b6cf-bae47d6cedf7"
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"ZINC_310k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = csv['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = smiles[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(csv['LogP'][:60000]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = torch.tensor(csv['LogP']).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'runner.xml' not in os.listdir(\".\"):\n",
    "#     runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "#     with open('runner.xml', 'wb') as f:\n",
    "#         pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:41<00:00,  3.46s/it]\n",
      "100%|██████████| 12/12 [21:37<00:00, 108.16s/it]\n"
     ]
    }
   ],
   "source": [
    "if 'runner_20.xml' not in os.listdir(\".\"):\n",
    "    runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "    with open('runner_20.xml', 'wb') as f:\n",
    "        pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fc02ae3d-696b-4c2e-b557-760e6a1a75a9",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "uTNMpfWD7b7Q"
   },
   "outputs": [],
   "source": [
    "# with open('runner.xml', 'rb') as f:\n",
    "#     runner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fc02ae3d-696b-4c2e-b557-760e6a1a75a9",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "uTNMpfWD7b7Q"
   },
   "outputs": [],
   "source": [
    "with open('runner_20.xml', 'rb') as f:\n",
    "    runner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "4ccc9136-0e87-470b-90eb-8e0b92da52bc",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "0-1xXVU15z6N",
    "outputId": "c6c328fe-72e4-4401-bce6-8613e7e9319f"
   },
   "outputs": [],
   "source": [
    "runner.get_model(\n",
    "    \"rand_gen\",\n",
    "    {\n",
    "        \"hidden_size\": conf[\"model\"][\"hidden_size\"],\n",
    "        \"latent_size\": conf[\"model\"][\"latent_size\"],\n",
    "        \"depthT\": conf[\"model\"][\"depthT\"],\n",
    "        \"depthG\": conf[\"model\"][\"depthG\"],\n",
    "        \"label_size\": 1,\n",
    "        \"label_mean\": float(torch.mean(labels)),\n",
    "        \"label_var\": float(torch.var(labels)),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c177219a-298a-4994-98b9-de76833e14fe",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "IGnpfkM_KQXi"
   },
   "outputs": [],
   "source": [
    "labels = runner.get_processed_labels(labels)\n",
    "preprocessed = runner.processed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_TEST = 10000\n",
    "N_TEST = 200\n",
    "VAL_FRAC = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_id=np.random.permutation(len(labels))\n",
    "X_train = preprocessed[perm_id[N_TEST:]]\n",
    "L_train = torch.tensor(labels.numpy()[perm_id[N_TEST:]])\n",
    "\n",
    "X_test = preprocessed[perm_id[:N_TEST]]\n",
    "L_test = torch.tensor(labels.numpy()[perm_id[:N_TEST]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cut = math.floor(len(X_train) * VAL_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Val = X_train[:val_cut]\n",
    "L_Val = L_train[:val_cut]\n",
    "\n",
    "X_train = X_train[val_cut :]\n",
    "L_train = L_train[val_cut :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.npy', 'wb') as f:\n",
    "    np.save(f, X_train)\n",
    "    \n",
    "with open('test.npy', 'wb') as f:\n",
    "    np.save(f, X_test)\n",
    "    \n",
    "with open('validation.npy', 'wb') as f:\n",
    "    np.save(f, X_Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('train.npy', 'rb') as f:\n",
    "#     X_train = np.load(f, allow_pickle=True)\n",
    "\n",
    "# with open('test.npy', 'rb') as f:\n",
    "#     X_test = np.load(f, allow_pickle=True)\n",
    "\n",
    "# with open('validation.npy', 'rb') as f:\n",
    "#     X_Val = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7cb9e705-09fa-4075-a487-21887ecaeb95",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "TMxgCK1Y20mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model #Params: 4732K\n",
      "[Train][100] Alpha: 265.000, Beta: 0.126, Loss: 322.77, KL: 23.94, MAE: 0.78348, Word Loss: 97.17, Topo Loss: 27.58, Assm Loss: 9.39, Pred Loss: 0.70, Word: 25.67, Topo: 78.03, Assm: 52.75, PNorm: 101.06, GNorm: 50.00\n",
      "[Train][200] Alpha: 265.000, Beta: 0.126, Loss: 186.78, KL: 23.87, MAE: 0.56106, Word Loss: 64.40, Topo Loss: 16.86, Assm Loss: 8.65, Pred Loss: 0.35, Word: 42.44, Topo: 87.43, Assm: 56.29, PNorm: 104.97, GNorm: 50.00\n",
      "[Train][300] Alpha: 265.000, Beta: 0.126, Loss: 184.16, KL: 16.84, MAE: 0.58014, Word Loss: 56.10, Topo Loss: 16.29, Assm Loss: 8.67, Pred Loss: 0.38, Word: 51.03, Topo: 87.72, Assm: 56.93, PNorm: 108.07, GNorm: 50.00\n",
      "[Train][400] Alpha: 265.000, Beta: 0.126, Loss: 134.62, KL: 18.29, MAE: 0.43410, Word Loss: 53.64, Topo Loss: 12.31, Assm Loss: 8.30, Pred Loss: 0.22, Word: 52.74, Topo: 91.03, Assm: 59.49, PNorm: 111.15, GNorm: 50.00\n",
      "[Train][500] Alpha: 265.000, Beta: 0.126, Loss: 119.79, KL: 15.77, MAE: 0.37938, Word Loss: 51.80, Topo Loss: 13.13, Assm Loss: 8.41, Pred Loss: 0.17, Word: 53.71, Topo: 90.31, Assm: 59.22, PNorm: 114.49, GNorm: 50.00\n",
      "[Train][600] Alpha: 265.000, Beta: 0.126, Loss: 108.20, KL: 16.02, MAE: 0.34023, Word Loss: 49.69, Topo Loss: 11.65, Assm Loss: 7.85, Pred Loss: 0.14, Word: 55.58, Topo: 91.74, Assm: 61.81, PNorm: 117.76, GNorm: 50.00\n",
      "[Train][700] Alpha: 265.000, Beta: 0.126, Loss: 102.46, KL: 17.47, MAE: 0.32687, Word Loss: 47.86, Topo Loss: 10.53, Assm Loss: 8.40, Pred Loss: 0.13, Word: 56.87, Topo: 92.34, Assm: 58.42, PNorm: 120.64, GNorm: 50.00\n",
      "[Train][800] Alpha: 265.000, Beta: 0.126, Loss: 96.97, KL: 17.20, MAE: 0.29968, Word Loss: 47.15, Topo Loss: 10.44, Assm Loss: 8.05, Pred Loss: 0.11, Word: 58.03, Topo: 92.52, Assm: 60.84, PNorm: 123.17, GNorm: 50.00\n",
      "[Train][900] Alpha: 265.000, Beta: 0.126, Loss: 94.93, KL: 17.94, MAE: 0.29896, Word Loss: 46.49, Topo Loss: 10.31, Assm Loss: 8.02, Pred Loss: 0.11, Word: 58.21, Topo: 92.45, Assm: 61.50, PNorm: 125.69, GNorm: 50.00\n",
      "[Train][1000] Alpha: 265.000, Beta: 0.126, Loss: 96.33, KL: 18.14, MAE: 0.31360, Word Loss: 45.25, Topo Loss: 10.76, Assm Loss: 7.78, Pred Loss: 0.11, Word: 58.96, Topo: 92.28, Assm: 62.35, PNorm: 127.66, GNorm: 50.00\n",
      "[Train][1100] Alpha: 265.000, Beta: 0.126, Loss: 90.66, KL: 18.62, MAE: 0.27649, Word Loss: 45.91, Topo Loss: 10.16, Assm Loss: 7.75, Pred Loss: 0.09, Word: 58.30, Topo: 92.68, Assm: 61.77, PNorm: 130.50, GNorm: 50.00\n",
      "[Train][1200] Alpha: 265.000, Beta: 0.126, Loss: 86.93, KL: 19.09, MAE: 0.27387, Word Loss: 44.18, Topo Loss: 8.96, Assm Loss: 7.78, Pred Loss: 0.09, Word: 60.32, Topo: 93.61, Assm: 62.55, PNorm: 132.60, GNorm: 50.00\n",
      "[Train][1300] Alpha: 265.000, Beta: 0.126, Loss: 83.38, KL: 20.33, MAE: 0.25938, Word Loss: 43.38, Topo Loss: 8.79, Assm Loss: 7.42, Pred Loss: 0.08, Word: 60.56, Topo: 93.50, Assm: 62.82, PNorm: 134.76, GNorm: 50.00\n",
      "[Train][1400] Alpha: 265.000, Beta: 0.126, Loss: 79.54, KL: 19.72, MAE: 0.23414, Word Loss: 43.28, Topo Loss: 8.97, Assm Loss: 7.54, Pred Loss: 0.07, Word: 61.01, Topo: 93.50, Assm: 64.20, PNorm: 136.80, GNorm: 50.00\n",
      "[Train][1500] Alpha: 265.000, Beta: 0.126, Loss: 81.64, KL: 20.32, MAE: 0.25989, Word Loss: 42.24, Topo Loss: 8.56, Assm Loss: 7.61, Pred Loss: 0.08, Word: 61.29, Topo: 93.84, Assm: 63.02, PNorm: 138.34, GNorm: 50.00\n",
      "[Train][1600] Alpha: 265.000, Beta: 0.126, Loss: 77.25, KL: 21.06, MAE: 0.21974, Word Loss: 42.81, Topo Loss: 8.83, Assm Loss: 7.38, Pred Loss: 0.06, Word: 61.36, Topo: 93.79, Assm: 64.78, PNorm: 141.06, GNorm: 50.00\n",
      "[Validation][175] Alpha: 265.000, Beta: 0.126, Loss: 41.09, KL: 11.23, MAE: 0.18813, Word Loss: 20.52, Topo Loss: 4.06, Assm Loss: 3.56, Pred Loss: 0.043549, Word: 62.67, Topo: 94.19, Assm: 64.59\n",
      "[Train][1700] Alpha: 265.000, Beta: 0.126, Loss: 75.30, KL: 21.65, MAE: 0.21564, Word Loss: 41.90, Topo Loss: 8.56, Assm Loss: 7.49, Pred Loss: 0.06, Word: 61.93, Topo: 93.72, Assm: 64.18, PNorm: 143.09, GNorm: 50.00\n",
      "[Train][1800] Alpha: 265.000, Beta: 0.126, Loss: 72.87, KL: 22.39, MAE: 0.21881, Word Loss: 40.24, Topo Loss: 8.02, Assm Loss: 7.14, Pred Loss: 0.06, Word: 63.20, Topo: 94.18, Assm: 65.08, PNorm: 144.99, GNorm: 50.00\n",
      "[Train][1900] Alpha: 265.000, Beta: 0.126, Loss: 72.11, KL: 22.98, MAE: 0.20260, Word Loss: 40.84, Topo Loss: 7.96, Assm Loss: 7.30, Pred Loss: 0.05, Word: 62.23, Topo: 94.26, Assm: 65.16, PNorm: 147.13, GNorm: 50.00\n",
      "[Train][2000] Alpha: 265.000, Beta: 0.126, Loss: 72.86, KL: 23.26, MAE: 0.22242, Word Loss: 39.68, Topo Loss: 8.00, Assm Loss: 7.15, Pred Loss: 0.06, Word: 63.82, Topo: 94.18, Assm: 65.68, PNorm: 148.85, GNorm: 50.00\n",
      "[Train][2100] Alpha: 265.000, Beta: 0.126, Loss: 69.35, KL: 24.81, MAE: 0.18981, Word Loss: 39.41, Topo Loss: 7.60, Assm Loss: 6.99, Pred Loss: 0.05, Word: 63.72, Topo: 94.51, Assm: 65.54, PNorm: 150.80, GNorm: 50.00\n",
      "[Train][2200] Alpha: 265.000, Beta: 0.126, Loss: 69.90, KL: 24.54, MAE: 0.20033, Word Loss: 38.95, Topo Loss: 7.52, Assm Loss: 7.09, Pred Loss: 0.05, Word: 64.52, Topo: 94.53, Assm: 65.07, PNorm: 152.45, GNorm: 50.00\n",
      "[Train][2400] Alpha: 265.000, Beta: 0.126, Loss: 67.55, KL: 26.09, MAE: 0.18588, Word Loss: 38.61, Topo Loss: 7.63, Assm Loss: 7.00, Pred Loss: 0.04, Word: 65.11, Topo: 94.49, Assm: 66.49, PNorm: 156.32, GNorm: 50.00\n",
      "[Train][2500] Alpha: 265.000, Beta: 0.126, Loss: 68.92, KL: 27.02, MAE: 0.20554, Word Loss: 37.91, Topo Loss: 7.53, Assm Loss: 7.07, Pred Loss: 0.05, Word: 65.12, Topo: 94.49, Assm: 66.20, PNorm: 157.89, GNorm: 50.00\n",
      "[Train][2600] Alpha: 265.000, Beta: 0.126, Loss: 66.70, KL: 26.98, MAE: 0.17789, Word Loss: 37.57, Topo Loss: 7.38, Assm Loss: 6.83, Pred Loss: 0.04, Word: 66.13, Topo: 94.66, Assm: 67.06, PNorm: 159.65, GNorm: 50.00\n",
      "[Train][7100] Alpha: 265.000, Beta: 0.126, Loss: 46.13, KL: 44.58, MAE: 0.11773, Word Loss: 25.38, Topo Loss: 5.19, Assm Loss: 5.63, Pred Loss: 0.02, Word: 75.22, Topo: 96.30, Assm: 72.14, PNorm: 227.04, GNorm: 50.00\n",
      "[Train][7200] Alpha: 265.000, Beta: 0.126, Loss: 46.24, KL: 44.85, MAE: 0.11759, Word Loss: 25.29, Topo Loss: 5.10, Assm Loss: 5.77, Pred Loss: 0.02, Word: 75.32, Topo: 96.36, Assm: 71.83, PNorm: 228.46, GNorm: 50.00\n",
      "[Train][7300] Alpha: 265.000, Beta: 0.126, Loss: 47.65, KL: 45.12, MAE: 0.14156, Word Loss: 24.78, Topo Loss: 5.04, Assm Loss: 5.53, Pred Loss: 0.02, Word: 76.10, Topo: 96.46, Assm: 73.43, PNorm: 229.37, GNorm: 50.00\n",
      "[Train][7400] Alpha: 265.000, Beta: 0.126, Loss: 48.02, KL: 45.37, MAE: 0.14388, Word Loss: 25.04, Topo Loss: 4.97, Assm Loss: 5.57, Pred Loss: 0.03, Word: 75.78, Topo: 96.57, Assm: 72.78, PNorm: 230.50, GNorm: 50.00\n",
      "[Train][7500] Alpha: 265.000, Beta: 0.126, Loss: 46.24, KL: 45.74, MAE: 0.12051, Word Loss: 25.28, Topo Loss: 5.04, Assm Loss: 5.56, Pred Loss: 0.02, Word: 75.54, Topo: 96.50, Assm: 72.91, PNorm: 231.97, GNorm: 50.00\n",
      "[Train][7600] Alpha: 265.000, Beta: 0.126, Loss: 46.05, KL: 45.62, MAE: 0.11861, Word Loss: 24.99, Topo Loss: 5.10, Assm Loss: 5.82, Pred Loss: 0.02, Word: 75.74, Topo: 96.39, Assm: 72.52, PNorm: 233.44, GNorm: 50.00\n",
      "[Train][7700] Alpha: 265.000, Beta: 0.126, Loss: 45.76, KL: 46.22, MAE: 0.12231, Word Loss: 24.95, Topo Loss: 4.75, Assm Loss: 5.52, Pred Loss: 0.02, Word: 75.79, Topo: 96.58, Assm: 73.50, PNorm: 234.81, GNorm: 50.00\n",
      "[Train][7800] Alpha: 265.000, Beta: 0.126, Loss: 45.38, KL: 46.10, MAE: 0.11920, Word Loss: 24.51, Topo Loss: 4.98, Assm Loss: 5.58, Pred Loss: 0.02, Word: 76.16, Topo: 96.41, Assm: 73.86, PNorm: 235.95, GNorm: 50.00\n",
      "[Train][7900] Alpha: 265.000, Beta: 0.126, Loss: 45.96, KL: 46.28, MAE: 0.11823, Word Loss: 24.88, Topo Loss: 5.01, Assm Loss: 5.71, Pred Loss: 0.02, Word: 75.67, Topo: 96.42, Assm: 72.07, PNorm: 237.46, GNorm: 50.00\n",
      "[Train][8000] Alpha: 265.000, Beta: 0.126, Loss: 45.21, KL: 47.04, MAE: 0.11647, Word Loss: 24.22, Topo Loss: 5.14, Assm Loss: 5.67, Pred Loss: 0.02, Word: 76.37, Topo: 96.39, Assm: 72.47, PNorm: 238.89, GNorm: 50.00\n",
      "[Train][8100] Alpha: 265.000, Beta: 0.126, Loss: 45.88, KL: 48.13, MAE: 0.13016, Word Loss: 24.23, Topo Loss: 4.78, Assm Loss: 5.45, Pred Loss: 0.02, Word: 76.36, Topo: 96.55, Assm: 73.02, PNorm: 240.18, GNorm: 50.00\n",
      "[Train][8200] Alpha: 265.000, Beta: 0.126, Loss: 45.23, KL: 47.29, MAE: 0.11946, Word Loss: 24.23, Topo Loss: 5.04, Assm Loss: 5.46, Pred Loss: 0.02, Word: 76.66, Topo: 96.42, Assm: 73.10, PNorm: 241.55, GNorm: 50.00\n",
      "[Train][8300] Alpha: 265.000, Beta: 0.126, Loss: 45.38, KL: 47.51, MAE: 0.12565, Word Loss: 24.03, Topo Loss: 4.98, Assm Loss: 5.41, Pred Loss: 0.02, Word: 76.51, Topo: 96.32, Assm: 73.64, PNorm: 242.79, GNorm: 50.00\n",
      "[Validation][175] Alpha: 265.000, Beta: 0.126, Loss: 25.73, KL: 25.12, MAE: 0.10957, Word Loss: 12.74, Topo Loss: 2.73, Assm Loss: 2.71, Pred Loss: 0.016544, Word: 76.56, Topo: 96.28, Assm: 73.52\n",
      "[Train][8400] Alpha: 265.000, Beta: 0.126, Loss: 45.04, KL: 48.99, MAE: 0.13486, Word Loss: 22.80, Topo Loss: 4.78, Assm Loss: 5.25, Pred Loss: 0.02, Word: 77.69, Topo: 96.71, Assm: 74.75, PNorm: 243.72, GNorm: 50.00\n",
      "[Train][8500] Alpha: 265.000, Beta: 0.126, Loss: 42.44, KL: 48.85, MAE: 0.11947, Word Loss: 22.41, Topo Loss: 4.33, Assm Loss: 5.06, Pred Loss: 0.02, Word: 78.21, Topo: 97.00, Assm: 75.24, PNorm: 244.96, GNorm: 50.00\n",
      "[Train][8600] Alpha: 265.000, Beta: 0.126, Loss: 41.54, KL: 49.56, MAE: 0.09778, Word Loss: 22.75, Topo Loss: 4.49, Assm Loss: 5.07, Pred Loss: 0.01, Word: 77.57, Topo: 96.88, Assm: 75.04, PNorm: 246.56, GNorm: 50.00\n",
      "[Train][8700] Alpha: 265.000, Beta: 0.126, Loss: 42.71, KL: 49.37, MAE: 0.11787, Word Loss: 22.50, Topo Loss: 4.56, Assm Loss: 5.07, Pred Loss: 0.02, Word: 77.95, Topo: 96.80, Assm: 74.86, PNorm: 247.83, GNorm: 50.00\n",
      "[Train][8800] Alpha: 265.000, Beta: 0.126, Loss: 41.87, KL: 49.45, MAE: 0.11112, Word Loss: 22.06, Topo Loss: 4.55, Assm Loss: 5.13, Pred Loss: 0.01, Word: 78.30, Topo: 96.75, Assm: 74.98, PNorm: 249.08, GNorm: 50.00\n",
      "[Train][8900] Alpha: 265.000, Beta: 0.126, Loss: 42.58, KL: 50.35, MAE: 0.11040, Word Loss: 22.23, Topo Loss: 4.68, Assm Loss: 5.41, Pred Loss: 0.01, Word: 78.32, Topo: 96.68, Assm: 74.41, PNorm: 250.30, GNorm: 50.00\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred(\n",
    "    X_train,\n",
    "    L_train,\n",
    "    X_test,\n",
    "    L_test,\n",
    "    X_Val,\n",
    "    L_Val,\n",
    "    load_epoch= 0,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=conf[\"alpha\"],\n",
    "    max_alpha=conf[\"max_alpha\"],\n",
    "    step_alpha=conf[\"step_alpha\"],\n",
    "    beta=conf[\"beta\"],\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    alpha_anneal_iter=conf[\"alpha_anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter=conf[\"save_iter\"],\n",
    "    batch_size=conf[\"batch_size\"],\n",
    "    num_workers=conf[\"num_workers\"],\n",
    "    label_pct=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred_supervised(\n",
    "    loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    load_epoch=0,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=conf[\"alpha\"],\n",
    "    max_alpha=conf[\"max_alpha\"],\n",
    "    step_alpha=conf[\"step_alpha\"],\n",
    "    beta=conf[\"beta\"],\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    alpha_anneal_iter=conf[\"alpha_anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter=conf[\"save_iter\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of google_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
