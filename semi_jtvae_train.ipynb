{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr:0.001\n",
    "# anneal_rate:0.9\n",
    "# batch_size:32\n",
    "# clip_norm:50\n",
    "# num_epochs:5\n",
    "# alpha:250\n",
    "# beta:0\n",
    "# max_beta:1\n",
    "# step_beta:0.002\n",
    "# anneal_iter:40000\n",
    "# kl_anneal_iter:2000\n",
    "# print_iter:100\n",
    "# save_iter:5000\n",
    "# num_workers:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "gradient": {
     "editing": false,
     "id": "18de9aeb-6551-42f6-8c11-a0e30bd207d6",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "JDinHdioUZRH",
    "outputId": "1a824742-d528-46a2-8d89-7697a166c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu116.html\n",
    "!pip install -q dive-into-graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q toolz\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f728113a-ff43-466d-b34d-77c9b2e11478",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "_RKgd8MsYOYh"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pickle \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from molecule_optimizer.externals.fast_jtnn.datautils import SemiMolTreeFolder, SemiMolTreeFolderTest\n",
    "from molecule_optimizer.runner.semi_jtvae import SemiJTVAEGeneratorPredictor\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "\n",
    "import rdkit\n",
    "\n",
    "lg = rdkit.RDLogger.logger() \n",
    "lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6e85f4e2-eab6-4eae-b452-7f089e176039",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "2C4erValhPS-"
   },
   "outputs": [],
   "source": [
    "conf = json.load(open(\"training/configs/rand_gen_zinc250k_config_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": true,
     "id": "30eb7f71-c38e-49f6-8d84-715ff3d80e08",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "SuzGX7ClhKaf",
    "outputId": "ab764f6b-4dd7-4ffc-b6cf-bae47d6cedf7"
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"ZINC_310k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = csv['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = smiles[:60000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(csv['QED'][:60000]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'runner.xml' not in os.listdir(\".\"):\n",
    "#     runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "#     with open('runner.xml', 'wb') as f:\n",
    "#         pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'runner_20.xml' not in os.listdir(\".\"):\n",
    "    runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "    with open('runner_20.xml', 'wb') as f:\n",
    "        pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fc02ae3d-696b-4c2e-b557-760e6a1a75a9",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "uTNMpfWD7b7Q"
   },
   "outputs": [],
   "source": [
    "# with open('runner.xml', 'rb') as f:\n",
    "#     runner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fc02ae3d-696b-4c2e-b557-760e6a1a75a9",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "uTNMpfWD7b7Q"
   },
   "outputs": [],
   "source": [
    "with open('runner_20.xml', 'rb') as f:\n",
    "    runner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "4ccc9136-0e87-470b-90eb-8e0b92da52bc",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "0-1xXVU15z6N",
    "outputId": "c6c328fe-72e4-4401-bce6-8613e7e9319f"
   },
   "outputs": [],
   "source": [
    "runner.get_model(\n",
    "    \"rand_gen\",\n",
    "    {\n",
    "        \"hidden_size\": conf[\"model\"][\"hidden_size\"],\n",
    "        \"latent_size\": conf[\"model\"][\"latent_size\"],\n",
    "        \"depthT\": conf[\"model\"][\"depthT\"],\n",
    "        \"depthG\": conf[\"model\"][\"depthG\"],\n",
    "        \"label_size\": 1,\n",
    "        \"label_mean\": float(torch.mean(labels)),\n",
    "        \"label_var\": float(torch.var(labels)),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c177219a-298a-4994-98b9-de76833e14fe",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "IGnpfkM_KQXi"
   },
   "outputs": [],
   "source": [
    "labels = runner.get_processed_labels(labels)\n",
    "preprocessed = runner.processed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_TEST = 10000\n",
    "N_TEST = 200\n",
    "VAL_FRAC = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_id=np.random.permutation(len(labels))\n",
    "X_train = preprocessed[perm_id[N_TEST:]]\n",
    "L_train = torch.tensor(labels.numpy()[perm_id[N_TEST:]])\n",
    "\n",
    "\n",
    "X_test = preprocessed[perm_id[:N_TEST]]\n",
    "L_test = torch.tensor(labels.numpy()[perm_id[:N_TEST]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cut = math.floor(len(X_train) * VAL_FRAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Val = X_train[:val_cut]\n",
    "L_Val = L_train[:val_cut]\n",
    "\n",
    "X_train = X_train[val_cut :]\n",
    "L_train = L_train[val_cut :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = SemiMolTreeFolder(\n",
    "    X_train,\n",
    "    L_train,\n",
    "    runner.vocab,\n",
    "    conf[\"batch_size\"],\n",
    "    label_pct=0.05,\n",
    "    num_workers=conf[\"num_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "25d5a0be-b1f8-454e-bc3a-82d8489ac3ca",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "k19uZMZN9H05"
   },
   "outputs": [],
   "source": [
    "test_loader = SemiMolTreeFolderTest(\n",
    "    X_test,\n",
    "    L_test,\n",
    "    runner.vocab,\n",
    "    conf[\"batch_size\"],\n",
    "    num_workers=conf[\"num_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_val = math.floor(len(X_Val) / 10)\n",
    "\n",
    "val_loader = SemiMolTreeFolderTest(\n",
    "    X_Val,\n",
    "    L_Val,\n",
    "    runner.vocab,\n",
    "    batch_size_val,\n",
    "    num_workers=conf[\"num_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7cb9e705-09fa-4075-a487-21887ecaeb95",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "TMxgCK1Y20mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model #Params: 4732K\n",
      "[Train][100] Alpha: 66.503, Beta: 0.000, Loss: 177.01, KL: 87.75, MAE: 0.11276, Word Loss: 92.26, Topo Loss: 25.66, Assm Loss: 9.05, Pred Loss: 0.80, Word: 27.78, Topo: 79.16, Assm: 54.35, PNorm: 101.68, GNorm: 50.00\n",
      "[Train][200] Alpha: 61.574, Beta: 0.000, Loss: 111.62, KL: 119.35, MAE: 0.09095, Word Loss: 58.46, Topo Loss: 14.27, Assm Loss: 8.59, Pred Loss: 0.54, Word: 47.80, Topo: 89.93, Assm: 58.65, PNorm: 105.76, GNorm: 50.00\n",
      "[Train][300] Alpha: 77.440, Beta: 0.000, Loss: 99.73, KL: 147.30, MAE: 0.08573, Word Loss: 50.65, Topo Loss: 12.07, Assm Loss: 8.30, Pred Loss: 0.48, Word: 54.54, Topo: 91.30, Assm: 58.27, PNorm: 108.72, GNorm: 50.00\n",
      "[Train][400] Alpha: 28.909, Beta: 0.000, Loss: 95.02, KL: 179.85, MAE: 0.08182, Word Loss: 47.61, Topo Loss: 11.54, Assm Loss: 8.16, Pred Loss: 0.43, Word: 57.59, Topo: 91.76, Assm: 60.09, PNorm: 111.56, GNorm: 50.00\n",
      "[Train][500] Alpha: 28.293, Beta: 0.000, Loss: 86.89, KL: 202.06, MAE: 0.08202, Word Loss: 44.76, Topo Loss: 10.13, Assm Loss: 7.80, Pred Loss: 0.44, Word: 59.90, Topo: 92.74, Assm: 61.64, PNorm: 114.22, GNorm: 50.00\n",
      "[Train][600] Alpha: 77.616, Beta: 0.000, Loss: 83.22, KL: 228.98, MAE: 0.07326, Word Loss: 41.95, Topo Loss: 10.24, Assm Loss: 7.54, Pred Loss: 0.35, Word: 62.36, Topo: 92.46, Assm: 62.64, PNorm: 117.09, GNorm: 50.00\n",
      "[Train][700] Alpha: 72.741, Beta: 0.000, Loss: 81.40, KL: 244.18, MAE: 0.07239, Word Loss: 40.82, Topo Loss: 9.24, Assm Loss: 7.50, Pred Loss: 0.35, Word: 63.85, Topo: 93.37, Assm: 64.09, PNorm: 119.34, GNorm: 50.00\n",
      "[Train][800] Alpha: 57.677, Beta: 0.000, Loss: 76.16, KL: 271.24, MAE: 0.06904, Word Loss: 38.38, Topo Loss: 8.04, Assm Loss: 7.41, Pred Loss: 0.32, Word: 65.38, Topo: 94.29, Assm: 62.64, PNorm: 121.73, GNorm: 50.00\n",
      "[Train][900] Alpha: 89.186, Beta: 0.000, Loss: 73.53, KL: 292.19, MAE: 0.06474, Word Loss: 37.50, Topo Loss: 7.85, Assm Loss: 7.29, Pred Loss: 0.28, Word: 66.44, Topo: 94.44, Assm: 65.12, PNorm: 124.11, GNorm: 50.00\n",
      "[Train][1000] Alpha: 30.282, Beta: 0.000, Loss: 71.31, KL: 306.55, MAE: 0.06630, Word Loss: 36.12, Topo Loss: 7.75, Assm Loss: 7.02, Pred Loss: 0.29, Word: 67.12, Topo: 94.44, Assm: 65.66, PNorm: 126.69, GNorm: 50.00\n",
      "[Train][1100] Alpha: 94.266, Beta: 0.000, Loss: 69.60, KL: 314.90, MAE: 0.06122, Word Loss: 34.66, Topo Loss: 7.52, Assm Loss: 6.78, Pred Loss: 0.25, Word: 68.03, Topo: 94.46, Assm: 66.69, PNorm: 129.08, GNorm: 50.00\n",
      "[Train][1200] Alpha: 70.044, Beta: 0.000, Loss: 66.22, KL: 346.75, MAE: 0.05964, Word Loss: 33.68, Topo Loss: 7.01, Assm Loss: 6.81, Pred Loss: 0.24, Word: 68.94, Topo: 95.05, Assm: 66.88, PNorm: 131.40, GNorm: 50.00\n",
      "[Train][1300] Alpha: 52.560, Beta: 0.000, Loss: 64.58, KL: 361.34, MAE: 0.05619, Word Loss: 32.39, Topo Loss: 6.75, Assm Loss: 6.71, Pred Loss: 0.21, Word: 69.50, Topo: 95.07, Assm: 66.71, PNorm: 133.87, GNorm: 50.00\n",
      "[Train][1400] Alpha: 139.312, Beta: 0.000, Loss: 63.18, KL: 379.55, MAE: 0.05582, Word Loss: 31.43, Topo Loss: 6.43, Assm Loss: 6.56, Pred Loss: 0.21, Word: 70.41, Topo: 95.35, Assm: 68.43, PNorm: 136.18, GNorm: 50.00\n",
      "[Train][1500] Alpha: 159.363, Beta: 0.000, Loss: 59.98, KL: 392.59, MAE: 0.05653, Word Loss: 29.92, Topo Loss: 6.31, Assm Loss: 6.27, Pred Loss: 0.21, Word: 71.52, Topo: 95.53, Assm: 69.51, PNorm: 138.48, GNorm: 50.00\n",
      "[Train][1600] Alpha: 135.952, Beta: 0.000, Loss: 59.62, KL: 401.88, MAE: 0.05570, Word Loss: 29.40, Topo Loss: 6.30, Assm Loss: 6.20, Pred Loss: 0.21, Word: 71.69, Topo: 95.54, Assm: 70.22, PNorm: 140.90, GNorm: 50.00\n",
      "[Train][1700] Alpha: 78.371, Beta: 0.000, Loss: 56.93, KL: 414.14, MAE: 0.05200, Word Loss: 28.42, Topo Loss: 6.36, Assm Loss: 6.07, Pred Loss: 0.18, Word: 72.88, Topo: 95.59, Assm: 70.56, PNorm: 143.17, GNorm: 50.00\n",
      "[Train][1800] Alpha: 76.677, Beta: 0.000, Loss: 53.91, KL: 419.02, MAE: 0.05135, Word Loss: 26.51, Topo Loss: 5.60, Assm Loss: 6.04, Pred Loss: 0.18, Word: 73.79, Topo: 95.96, Assm: 70.73, PNorm: 145.53, GNorm: 50.00\n",
      "[Train][1900] Alpha: 74.053, Beta: 0.000, Loss: 51.34, KL: 449.62, MAE: 0.04661, Word Loss: 25.72, Topo Loss: 5.47, Assm Loss: 5.83, Pred Loss: 0.14, Word: 74.98, Topo: 96.12, Assm: 72.13, PNorm: 147.99, GNorm: 50.00\n",
      "[Train][2000] Alpha: 114.467, Beta: 0.000, Loss: 49.21, KL: 461.99, MAE: 0.04664, Word Loss: 23.75, Topo Loss: 5.37, Assm Loss: 5.52, Pred Loss: 0.15, Word: 76.26, Topo: 96.20, Assm: 73.95, PNorm: 150.61, GNorm: 50.00\n",
      "[Train][2100] Alpha: 78.972, Beta: 0.000, Loss: 49.21, KL: 476.87, MAE: 0.04630, Word Loss: 23.59, Topo Loss: 5.19, Assm Loss: 5.63, Pred Loss: 0.14, Word: 76.15, Topo: 96.33, Assm: 73.05, PNorm: 153.13, GNorm: 50.00\n",
      "[Train][2200] Alpha: 115.666, Beta: 0.000, Loss: 47.54, KL: 491.05, MAE: 0.04303, Word Loss: 22.94, Topo Loss: 5.73, Assm Loss: 5.36, Pred Loss: 0.13, Word: 77.26, Topo: 95.99, Assm: 74.51, PNorm: 155.59, GNorm: 50.00\n",
      "[Train][2300] Alpha: 92.752, Beta: 0.000, Loss: 42.97, KL: 492.61, MAE: 0.04117, Word Loss: 20.87, Topo Loss: 4.57, Assm Loss: 5.24, Pred Loss: 0.11, Word: 78.84, Topo: 96.92, Assm: 74.49, PNorm: 157.84, GNorm: 50.00\n",
      "[Train][2400] Alpha: 95.756, Beta: 0.000, Loss: 42.69, KL: 530.29, MAE: 0.04044, Word Loss: 20.47, Topo Loss: 4.61, Assm Loss: 5.00, Pred Loss: 0.11, Word: 79.52, Topo: 96.75, Assm: 76.48, PNorm: 159.88, GNorm: 50.00\n",
      "[Train][2500] Alpha: 64.784, Beta: 0.000, Loss: 39.50, KL: 536.58, MAE: 0.03864, Word Loss: 19.00, Topo Loss: 4.26, Assm Loss: 4.55, Pred Loss: 0.10, Word: 80.65, Topo: 97.06, Assm: 78.87, PNorm: 162.34, GNorm: 50.00\n",
      "[Train][2600] Alpha: 155.702, Beta: 0.000, Loss: 38.92, KL: 545.59, MAE: 0.04420, Word Loss: 19.07, Topo Loss: 4.37, Assm Loss: 4.64, Pred Loss: 0.13, Word: 80.36, Topo: 97.01, Assm: 78.06, PNorm: 164.47, GNorm: 50.00\n",
      "[Train][2700] Alpha: 72.532, Beta: 0.000, Loss: 35.81, KL: 558.19, MAE: 0.03777, Word Loss: 17.51, Topo Loss: 4.06, Assm Loss: 4.18, Pred Loss: 0.09, Word: 81.88, Topo: 97.20, Assm: 80.38, PNorm: 166.55, GNorm: 50.00\n",
      "[Train][2800] Alpha: 83.381, Beta: 0.000, Loss: 35.67, KL: 565.22, MAE: 0.03829, Word Loss: 16.82, Topo Loss: 3.88, Assm Loss: 4.31, Pred Loss: 0.10, Word: 82.31, Topo: 97.38, Assm: 79.89, PNorm: 168.57, GNorm: 50.00\n",
      "[Train][2900] Alpha: 136.004, Beta: 0.000, Loss: 32.97, KL: 593.65, MAE: 0.03867, Word Loss: 15.30, Topo Loss: 3.71, Assm Loss: 4.16, Pred Loss: 0.10, Word: 84.19, Topo: 97.48, Assm: 81.32, PNorm: 170.58, GNorm: 50.00\n",
      "[Train][3000] Alpha: 97.830, Beta: 0.000, Loss: 29.38, KL: 612.40, MAE: 0.03357, Word Loss: 13.63, Topo Loss: 3.86, Assm Loss: 3.56, Pred Loss: 0.07, Word: 85.59, Topo: 97.51, Assm: 83.90, PNorm: 172.66, GNorm: 50.00\n",
      "[Train][3100] Alpha: 56.356, Beta: 0.000, Loss: 27.25, KL: 625.40, MAE: 0.03687, Word Loss: 12.66, Topo Loss: 3.00, Assm Loss: 3.58, Pred Loss: 0.09, Word: 86.88, Topo: 97.89, Assm: 83.74, PNorm: 174.60, GNorm: 50.00\n",
      "[Validation][10] Alpha: 109.174, Beta: 0.000, Loss: 57.91, KL: 305.47, MAE: 0.06267, Word Loss: 19.76, Topo Loss: 5.78, Assm Loss: 3.68, Pred Loss: 0.26, Word: 70.31, Topo: 94.25, Assm: 72.16\n",
      "[Test][12] Alpha: 109.174, Beta: 0.000, Loss: 57.53, KL: 306.27, MAE: 0.06288, Word Loss: 19.83, Topo Loss: 5.45, Assm Loss: 3.15, Pred Loss: 0.27, Word: 69.90, Topo: 94.13, Assm: 73.21\n",
      "[Train][3200] Alpha: 66.939, Beta: 0.000, Loss: 3482631.72, KL: 2147368.08, MAE: 0.96784, Word Loss: 47.93, Topo Loss: 24.78, Assm Loss: 9825.75, Pred Loss: 51863.14, Word: 87.29, Topo: 97.54, Assm: 84.89, PNorm: 176.67, GNorm: 50.00\n",
      "[Train][3300] Alpha: 63.568, Beta: 0.000, Loss: 1439322.34, KL: 3751905.50, MAE: 0.86572, Word Loss: 79.53, Topo Loss: 18.79, Assm Loss: 1210.58, Pred Loss: 20310.77, Word: 87.41, Topo: 97.77, Assm: 82.04, PNorm: 178.75, GNorm: 50.00\n",
      "[Train][3400] Alpha: 28.481, Beta: 0.000, Loss: 108703921.61, KL: 628094195.00, MAE: 3.91505, Word Loss: 2444.88, Topo Loss: 188.59, Assm Loss: 48390.26, Pred Loss: 954179.89, Word: 89.48, Topo: 98.18, Assm: 84.91, PNorm: 180.41, GNorm: 50.00\n",
      "[Train][3500] Alpha: 63.327, Beta: 0.000, Loss: 20.78, KL: 687.44, MAE: 0.03204, Word Loss: 9.64, Topo Loss: 2.53, Assm Loss: 2.75, Pred Loss: 0.07, Word: 89.76, Topo: 98.39, Assm: 88.13, PNorm: 181.99, GNorm: 50.00\n",
      "[Train][3600] Alpha: 49.233, Beta: 0.000, Loss: 5932509.97, KL: 34208087.68, MAE: 1.92502, Word Loss: 80.22, Topo Loss: 72.87, Assm Loss: 34812.83, Pred Loss: 103498.47, Word: 90.45, Topo: 98.39, Assm: 86.01, PNorm: 183.50, GNorm: 50.00\n",
      "[Train][3700] Alpha: 48.143, Beta: 0.000, Loss: 46689.51, KL: 223454.74, MAE: 0.17658, Word Loss: 12.52, Topo Loss: 19.21, Assm Loss: 2156.51, Pred Loss: 580.79, Word: 90.71, Topo: 98.50, Assm: 87.47, PNorm: 185.11, GNorm: 50.00\n",
      "[Train][3800] Alpha: 82.014, Beta: 0.000, Loss: 11822638.62, KL: 39898759.13, MAE: 1.53700, Word Loss: 298.02, Topo Loss: 2.44, Assm Loss: 186419.18, Pred Loss: 140609.27, Word: 92.27, Topo: 98.41, Assm: 88.54, PNorm: 186.58, GNorm: 50.00\n",
      "[Train][3900] Alpha: 116.679, Beta: 0.000, Loss: 204914.94, KL: 7804590.17, MAE: 0.32500, Word Loss: 111.15, Topo Loss: 37.99, Assm Loss: 2796.96, Pred Loss: 1798.07, Word: 91.85, Topo: 98.45, Assm: 84.83, PNorm: 188.08, GNorm: 50.00\n",
      "[Train][4000] Alpha: 71.876, Beta: 0.000, Loss: 10485860256.01, KL: 11796533376.00, MAE: 47.33109, Word Loss: 702.02, Topo Loss: 47.42, Assm Loss: 3160.13, Pred Loss: 142474497.71, Word: 91.72, Topo: 98.39, Assm: 87.09, PNorm: 189.63, GNorm: 50.00\n",
      "[Train][4100] Alpha: 65.764, Beta: 0.000, Loss: 228732.61, KL: 2006190.43, MAE: 0.49134, Word Loss: 78.36, Topo Loss: 16.01, Assm Loss: 7729.94, Pred Loss: 5750.15, Word: 92.71, Topo: 98.65, Assm: 87.84, PNorm: 190.95, GNorm: 50.00\n",
      "[Train][4200] Alpha: 69.448, Beta: 0.000, Loss: 2234.96, KL: 41818.35, MAE: 0.06684, Word Loss: 18.90, Topo Loss: 8.25, Assm Loss: 83.87, Pred Loss: 37.51, Word: 93.43, Topo: 98.95, Assm: 89.87, PNorm: 192.29, GNorm: 50.00\n",
      "[Train][4300] Alpha: 59.294, Beta: 0.000, Loss: 27588.17, KL: 1435635.49, MAE: 0.13335, Word Loss: 56.22, Topo Loss: 4.65, Assm Loss: 712.67, Pred Loss: 304.52, Word: 93.94, Topo: 99.00, Assm: 89.15, PNorm: 193.54, GNorm: 50.00\n",
      "[Train][4400] Alpha: 79.048, Beta: 0.000, Loss: 94810.28, KL: 2195475.80, MAE: 0.18582, Word Loss: 94.18, Topo Loss: 65.99, Assm Loss: 1338.46, Pred Loss: 1368.52, Word: 94.03, Topo: 98.91, Assm: 88.61, PNorm: 194.91, GNorm: 50.00\n",
      "[Train][4500] Alpha: 71.561, Beta: 0.000, Loss: 4851.07, KL: 21848.87, MAE: 0.06592, Word Loss: 10.44, Topo Loss: 3.30, Assm Loss: 26.75, Pred Loss: 97.19, Word: 94.45, Topo: 99.03, Assm: 91.99, PNorm: 196.11, GNorm: 50.00\n",
      "[Train][4600] Alpha: 107.807, Beta: 0.000, Loss: 5113.46, KL: 48081.58, MAE: 0.07954, Word Loss: 18.26, Topo Loss: 2.88, Assm Loss: 66.51, Pred Loss: 99.39, Word: 94.46, Topo: 98.98, Assm: 90.27, PNorm: 197.35, GNorm: 50.00\n",
      "[Train][4700] Alpha: 53.925, Beta: 0.000, Loss: 327.99, KL: 27963.42, MAE: 0.03648, Word Loss: 7.55, Topo Loss: 3.17, Assm Loss: 103.22, Pred Loss: 3.82, Word: 94.80, Topo: 98.94, Assm: 91.26, PNorm: 198.62, GNorm: 50.00\n",
      "[Train][4800] Alpha: 75.700, Beta: 0.000, Loss: 3793.94, KL: 1923316.77, MAE: 0.07504, Word Loss: 88.80, Topo Loss: 13.98, Assm Loss: 519.40, Pred Loss: 59.00, Word: 94.94, Topo: 98.91, Assm: 90.88, PNorm: 199.86, GNorm: 50.00\n",
      "[Train][4900] Alpha: 112.500, Beta: 0.000, Loss: 3311021.13, KL: 19543630.39, MAE: 1.12281, Word Loss: 4.84, Topo Loss: 1.49, Assm Loss: 31467.19, Pred Loss: 75428.79, Word: 95.55, Topo: 99.23, Assm: 91.98, PNorm: 200.81, GNorm: 50.00\n",
      "[Train][5000] Alpha: 70.122, Beta: 0.000, Loss: 16210.50, KL: 175129.23, MAE: 0.10203, Word Loss: 17.46, Topo Loss: 4.57, Assm Loss: 117.27, Pred Loss: 223.38, Word: 95.70, Topo: 99.19, Assm: 92.45, PNorm: 201.84, GNorm: 50.00\n",
      "[Train][5100] Alpha: 78.995, Beta: 0.000, Loss: 109231.58, KL: 812164073.85, MAE: 0.28627, Word Loss: 1481.32, Topo Loss: 506.05, Assm Loss: 791.26, Pred Loss: 2061.69, Word: 95.48, Topo: 99.08, Assm: 90.54, PNorm: 203.06, GNorm: 50.00\n",
      "[Train][5200] Alpha: 68.321, Beta: 0.000, Loss: 3176043.90, KL: 9926866.81, MAE: 1.20176, Word Loss: 14.41, Topo Loss: 2.88, Assm Loss: 35083.73, Pred Loss: 85611.78, Word: 96.42, Topo: 99.18, Assm: 93.38, PNorm: 204.00, GNorm: 50.00\n",
      "[Train][5300] Alpha: 95.322, Beta: 0.000, Loss: 305628.45, KL: 3077391.27, MAE: 0.43432, Word Loss: 127.03, Topo Loss: 7.12, Assm Loss: 2760.17, Pred Loss: 5341.36, Word: 95.78, Topo: 99.18, Assm: 89.03, PNorm: 205.12, GNorm: 50.00\n",
      "[Train][5400] Alpha: 78.538, Beta: 0.000, Loss: 7507.29, KL: 356906.66, MAE: 0.10730, Word Loss: 7.08, Topo Loss: 9.48, Assm Loss: 207.42, Pred Loss: 194.95, Word: 96.18, Topo: 99.01, Assm: 93.54, PNorm: 206.24, GNorm: 50.00\n",
      "[Train][5500] Alpha: 118.121, Beta: 0.000, Loss: 421.33, KL: 3102.45, MAE: 0.02895, Word Loss: 7.76, Topo Loss: 2.21, Assm Loss: 7.81, Pred Loss: 3.44, Word: 96.48, Topo: 99.25, Assm: 94.89, PNorm: 207.20, GNorm: 50.00\n",
      "[Train][5600] Alpha: 75.633, Beta: 0.000, Loss: 5095837.65, KL: 84171467.77, MAE: 1.22170, Word Loss: 277.80, Topo Loss: 2.32, Assm Loss: 10900.85, Pred Loss: 65070.09, Word: 96.34, Topo: 99.25, Assm: 90.33, PNorm: 208.19, GNorm: 50.00\n",
      "[Train][5700] Alpha: 38.004, Beta: 0.000, Loss: 500.51, KL: 8285.70, MAE: 0.03358, Word Loss: 4.09, Topo Loss: 1.15, Assm Loss: 91.68, Pred Loss: 6.73, Word: 96.59, Topo: 99.41, Assm: 93.36, PNorm: 209.17, GNorm: 50.00\n",
      "[Train][5800] Alpha: 110.896, Beta: 0.000, Loss: 922.81, KL: 41129.51, MAE: 0.04245, Word Loss: 4.86, Topo Loss: 3.25, Assm Loss: 372.82, Pred Loss: 11.60, Word: 97.12, Topo: 99.33, Assm: 93.86, PNorm: 210.18, GNorm: 50.00\n",
      "[Train][5900] Alpha: 25.873, Beta: 0.000, Loss: 9456.32, KL: 1401117.22, MAE: 0.07580, Word Loss: 44.56, Topo Loss: 1.97, Assm Loss: 260.49, Pred Loss: 104.53, Word: 97.37, Topo: 99.20, Assm: 94.96, PNorm: 211.14, GNorm: 42.87\n",
      "[Train][6000] Alpha: 74.832, Beta: 0.000, Loss: 987.75, KL: 123410.17, MAE: 0.03462, Word Loss: 2.98, Topo Loss: 4.12, Assm Loss: 677.11, Pred Loss: 5.79, Word: 97.15, Topo: 99.42, Assm: 94.56, PNorm: 212.17, GNorm: 50.00\n",
      "[Train][6100] Alpha: 59.534, Beta: 0.000, Loss: 229154.30, KL: 10166522.24, MAE: 0.41320, Word Loss: 58.67, Topo Loss: 26.65, Assm Loss: 2526.73, Pred Loss: 3245.77, Word: 96.76, Topo: 99.41, Assm: 90.23, PNorm: 213.07, GNorm: 50.00\n",
      "[Train][6200] Alpha: 78.595, Beta: 0.000, Loss: 20558.36, KL: 1451411.22, MAE: 0.14423, Word Loss: 51.12, Topo Loss: 1.15, Assm Loss: 1237.69, Pred Loss: 434.30, Word: 96.93, Topo: 99.50, Assm: 94.23, PNorm: 214.03, GNorm: 50.00\n",
      "[Train][6300] Alpha: 64.041, Beta: 0.000, Loss: 12289.94, KL: 3077802.74, MAE: 0.14466, Word Loss: 45.09, Topo Loss: 1.16, Assm Loss: 1666.13, Pred Loss: 415.42, Word: 97.38, Topo: 99.50, Assm: 94.00, PNorm: 214.92, GNorm: 50.00\n",
      "[Validation][10] Alpha: 46.297, Beta: 0.000, Loss: 56.00, KL: 463.16, MAE: 0.05850, Word Loss: 29.75, Topo Loss: 9.11, Assm Loss: 6.23, Pred Loss: 0.24, Word: 72.51, Topo: 94.80, Assm: 74.06\n",
      "[Test][12] Alpha: 46.297, Beta: 0.000, Loss: 56.03, KL: 467.46, MAE: 0.05914, Word Loss: 29.97, Topo Loss: 8.77, Assm Loss: 5.11, Pred Loss: 0.26, Word: 72.42, Topo: 95.08, Assm: 78.10\n",
      "[Train][6400] Alpha: 49.187, Beta: 0.000, Loss: 6102.00, KL: 693358.75, MAE: 0.08764, Word Loss: 57.29, Topo Loss: 0.98, Assm Loss: 527.80, Pred Loss: 91.26, Word: 97.60, Topo: 99.56, Assm: 94.21, PNorm: 215.80, GNorm: 50.00\n",
      "[Train][6500] Alpha: 15.448, Beta: 0.000, Loss: 174236.47, KL: 507793.47, MAE: 0.21785, Word Loss: 17.53, Topo Loss: 1.07, Assm Loss: 261.02, Pred Loss: 2427.05, Word: 97.44, Topo: 99.44, Assm: 95.14, PNorm: 216.67, GNorm: 50.00\n",
      "[Train][6600] Alpha: 52.750, Beta: 0.000, Loss: 506137.36, KL: 8825150.47, MAE: 0.37252, Word Loss: 156.55, Topo Loss: 0.98, Assm Loss: 72.31, Pred Loss: 7886.78, Word: 97.74, Topo: 99.58, Assm: 95.22, PNorm: 217.49, GNorm: 50.00\n",
      "[Train][6700] Alpha: 51.373, Beta: 0.000, Loss: 1529.35, KL: 577684.62, MAE: 0.03958, Word Loss: 25.43, Topo Loss: 24.80, Assm Loss: 114.62, Pred Loss: 24.05, Word: 97.88, Topo: 99.56, Assm: 95.96, PNorm: 218.44, GNorm: 50.00\n",
      "[Train][6800] Alpha: 18.163, Beta: 0.000, Loss: 6615230.00, KL: 139755132.70, MAE: 1.05249, Word Loss: 39.45, Topo Loss: 1.00, Assm Loss: 76465.21, Pred Loss: 66331.98, Word: 97.16, Topo: 99.57, Assm: 94.81, PNorm: 219.44, GNorm: 50.00\n",
      "[Train][6900] Alpha: 89.083, Beta: 0.000, Loss: 2152104.61, KL: 25377050.39, MAE: 0.64818, Word Loss: 181.80, Topo Loss: 56.86, Assm Loss: 3719.53, Pred Loss: 24114.29, Word: 97.49, Topo: 99.57, Assm: 93.04, PNorm: 220.12, GNorm: 50.00\n",
      "[Train][7000] Alpha: 95.887, Beta: 0.000, Loss: 84269991.58, KL: 1033359922.00, MAE: 5.96951, Word Loss: 218.05, Topo Loss: 2.61, Assm Loss: 1847.25, Pred Loss: 2102057.31, Word: 97.34, Topo: 99.55, Assm: 92.88, PNorm: 220.96, GNorm: 50.00\n",
      "[Train][7100] Alpha: 98.375, Beta: 0.000, Loss: 649.55, KL: 137669.12, MAE: 0.05036, Word Loss: 16.79, Topo Loss: 2.85, Assm Loss: 72.14, Pred Loss: 13.96, Word: 97.76, Topo: 99.42, Assm: 94.15, PNorm: 221.83, GNorm: 50.00\n",
      "[Train][7200] Alpha: 31.983, Beta: 0.000, Loss: 4710001.55, KL: 295653518.59, MAE: 1.66130, Word Loss: 91.35, Topo Loss: 1.38, Assm Loss: 297970.79, Pred Loss: 159827.43, Word: 97.65, Topo: 99.50, Assm: 93.04, PNorm: 222.61, GNorm: 50.00\n",
      "[Train][7300] Alpha: 29.752, Beta: 0.000, Loss: 2071842.15, KL: 195740600.50, MAE: 1.03021, Word Loss: 251.74, Topo Loss: 2.65, Assm Loss: 72792.60, Pred Loss: 51860.36, Word: 97.59, Topo: 99.51, Assm: 94.66, PNorm: 223.44, GNorm: 50.00\n",
      "[Train][7400] Alpha: 29.690, Beta: 0.000, Loss: 66528.63, KL: 1713493.18, MAE: 0.14270, Word Loss: 165.68, Topo Loss: 2.76, Assm Loss: 725.62, Pred Loss: 954.75, Word: 98.03, Topo: 99.64, Assm: 95.95, PNorm: 224.20, GNorm: 50.00\n",
      "[Train][7500] Alpha: 57.569, Beta: 0.000, Loss: 213.40, KL: 3500.66, MAE: 0.02403, Word Loss: 2.35, Topo Loss: 1.26, Assm Loss: 169.74, Pred Loss: 1.17, Word: 97.86, Topo: 99.62, Assm: 96.52, PNorm: 225.00, GNorm: 50.00\n",
      "[Train][7600] Alpha: 55.045, Beta: 0.000, Loss: 622761.07, KL: 11538510.39, MAE: 0.65859, Word Loss: 187.64, Topo Loss: 1.92, Assm Loss: 8532.02, Pred Loss: 10038.87, Word: 97.47, Topo: 99.59, Assm: 94.40, PNorm: 225.80, GNorm: 50.00\n",
      "[Train][7700] Alpha: 31.627, Beta: 0.000, Loss: 4142.30, KL: 153153.41, MAE: 0.05794, Word Loss: 2.39, Topo Loss: 1.05, Assm Loss: 205.63, Pred Loss: 77.70, Word: 98.14, Topo: 99.45, Assm: 94.99, PNorm: 226.67, GNorm: 35.30\n",
      "[Train][7800] Alpha: 54.645, Beta: 0.000, Loss: 59.55, KL: 2927.41, MAE: 0.01870, Word Loss: 4.19, Topo Loss: 0.76, Assm Loss: 48.40, Pred Loss: 0.10, Word: 98.26, Topo: 99.62, Assm: 97.07, PNorm: 227.52, GNorm: 50.00\n",
      "[Train][7900] Alpha: 102.806, Beta: 0.000, Loss: 3844.83, KL: 322272.39, MAE: 0.05114, Word Loss: 27.30, Topo Loss: 0.81, Assm Loss: 297.97, Pred Loss: 45.43, Word: 98.42, Topo: 99.64, Assm: 95.99, PNorm: 228.28, GNorm: 50.00\n",
      "[Train][8000] Alpha: 45.072, Beta: 0.000, Loss: 1397.53, KL: 241373.90, MAE: 0.05185, Word Loss: 16.35, Topo Loss: 4.82, Assm Loss: 167.13, Pred Loss: 29.85, Word: 98.50, Topo: 99.64, Assm: 93.89, PNorm: 229.13, GNorm: 50.00\n",
      "[Train][8100] Alpha: 36.357, Beta: 0.000, Loss: 170.69, KL: 46747.22, MAE: 0.02422, Word Loss: 3.98, Topo Loss: 6.71, Assm Loss: 26.88, Pred Loss: 1.16, Word: 98.51, Topo: 99.57, Assm: 96.34, PNorm: 229.95, GNorm: 50.00\n",
      "[Train][8200] Alpha: 127.536, Beta: 0.000, Loss: 49803.31, KL: 43319919.97, MAE: 0.16088, Word Loss: 309.90, Topo Loss: 2.24, Assm Loss: 2518.50, Pred Loss: 595.67, Word: 98.21, Topo: 99.61, Assm: 94.97, PNorm: 230.74, GNorm: 50.00\n",
      "[Train][8300] Alpha: 52.389, Beta: 0.000, Loss: 169904.50, KL: 15247981.00, MAE: 0.34598, Word Loss: 138.88, Topo Loss: 115.11, Assm Loss: 5633.09, Pred Loss: 6214.27, Word: 98.14, Topo: 99.64, Assm: 94.55, PNorm: 231.41, GNorm: 43.99\n",
      "[Train][8400] Alpha: 71.429, Beta: 0.000, Loss: 4694856.44, KL: 743778765.46, MAE: 1.56583, Word Loss: 1611.36, Topo Loss: 1.43, Assm Loss: 84793.68, Pred Loss: 150454.17, Word: 98.24, Topo: 99.63, Assm: 95.40, PNorm: 232.04, GNorm: 50.00\n",
      "[Train][8500] Alpha: 32.021, Beta: 0.000, Loss: 93096.67, KL: 30222266.65, MAE: 0.34836, Word Loss: 257.11, Topo Loss: 14.79, Assm Loss: 2835.13, Pred Loss: 1864.07, Word: 97.68, Topo: 99.57, Assm: 94.06, PNorm: 232.78, GNorm: 35.76\n",
      "[Train][8600] Alpha: 89.154, Beta: 0.000, Loss: 99086159.35, KL: 13489293239.18, MAE: 5.44465, Word Loss: 19.72, Topo Loss: 2.20, Assm Loss: 14458727.29, Pred Loss: 1825253.87, Word: 97.78, Topo: 99.56, Assm: 95.01, PNorm: 233.48, GNorm: 50.00\n",
      "[Train][8700] Alpha: 32.019, Beta: 0.000, Loss: 122.93, KL: 15042.24, MAE: 0.02429, Word Loss: 6.63, Topo Loss: 0.64, Assm Loss: 75.39, Pred Loss: 1.04, Word: 98.18, Topo: 99.67, Assm: 97.37, PNorm: 234.11, GNorm: 50.00\n",
      "[Train][8800] Alpha: 53.648, Beta: 0.000, Loss: 58.23, KL: 329970.35, MAE: 0.01907, Word Loss: 3.46, Topo Loss: 3.51, Assm Loss: 45.79, Pred Loss: 0.16, Word: 98.43, Topo: 99.70, Assm: 97.15, PNorm: 234.74, GNorm: 50.00\n",
      "[Train][8900] Alpha: 56.506, Beta: 0.000, Loss: 33916.70, KL: 3990885.83, MAE: 0.16352, Word Loss: 7.94, Topo Loss: 3.60, Assm Loss: 1170.38, Pred Loss: 954.89, Word: 98.18, Topo: 99.62, Assm: 93.53, PNorm: 235.46, GNorm: 50.00\n",
      "[Train][9000] Alpha: 59.265, Beta: 0.000, Loss: 1892.54, KL: 182180.53, MAE: 0.04743, Word Loss: 4.87, Topo Loss: 6.26, Assm Loss: 56.27, Pred Loss: 53.44, Word: 98.14, Topo: 99.58, Assm: 97.54, PNorm: 236.15, GNorm: 50.00\n",
      "[Train][9100] Alpha: 43.833, Beta: 0.000, Loss: 141102.36, KL: 12573922.86, MAE: 0.22152, Word Loss: 7.88, Topo Loss: 7.70, Assm Loss: 5459.82, Pred Loss: 2166.31, Word: 98.05, Topo: 99.74, Assm: 95.16, PNorm: 236.84, GNorm: 50.00\n",
      "[Train][9200] Alpha: 45.687, Beta: 0.000, Loss: 196165.18, KL: 3492238.22, MAE: 0.23586, Word Loss: 105.37, Topo Loss: 1.02, Assm Loss: 7264.53, Pred Loss: 2951.49, Word: 98.22, Topo: 99.72, Assm: 95.07, PNorm: 237.51, GNorm: 50.00\n",
      "[Train][9300] Alpha: 26.006, Beta: 0.000, Loss: 177.20, KL: 16940.55, MAE: 0.02583, Word Loss: 2.71, Topo Loss: 3.14, Assm Loss: 43.56, Pred Loss: 2.90, Word: 98.56, Topo: 99.77, Assm: 97.11, PNorm: 238.21, GNorm: 50.00\n",
      "[Train][9400] Alpha: 37.383, Beta: 0.000, Loss: 3862248.82, KL: 383630487.12, MAE: 1.52342, Word Loss: 607.54, Topo Loss: 1.19, Assm Loss: 226416.42, Pred Loss: 108939.79, Word: 98.10, Topo: 99.70, Assm: 93.04, PNorm: 238.93, GNorm: 50.00\n",
      "[Train][9500] Alpha: 80.743, Beta: 0.000, Loss: 6892.48, KL: 4164818.99, MAE: 0.10923, Word Loss: 91.04, Topo Loss: 2.41, Assm Loss: 1762.26, Pred Loss: 199.62, Word: 98.42, Topo: 99.68, Assm: 94.30, PNorm: 239.62, GNorm: 50.00\n",
      "[Validation][10] Alpha: 145.938, Beta: 0.000, Loss: 87.22, KL: 515.85, MAE: 0.06047, Word Loss: 34.90, Topo Loss: 9.71, Assm Loss: 6.48, Pred Loss: 0.25, Word: 72.46, Topo: 94.78, Assm: 75.29\n",
      "[Test][12] Alpha: 145.938, Beta: 0.000, Loss: 89.47, KL: 525.46, MAE: 0.06077, Word Loss: 35.92, Topo Loss: 9.32, Assm Loss: 5.64, Pred Loss: 0.26, Word: 72.31, Topo: 94.90, Assm: 79.25\n",
      "[Train][9600] Alpha: 56.633, Beta: 0.000, Loss: 23755.89, KL: 2381149.60, MAE: 0.10812, Word Loss: 57.58, Topo Loss: 51.57, Assm Loss: 1490.12, Pred Loss: 417.97, Word: 98.40, Topo: 99.78, Assm: 95.76, PNorm: 240.31, GNorm: 50.00\n",
      "[Train][9700] Alpha: 36.961, Beta: 0.000, Loss: 255763.31, KL: 6232870.80, MAE: 0.34642, Word Loss: 43.74, Topo Loss: 0.64, Assm Loss: 9820.52, Pred Loss: 3611.83, Word: 98.39, Topo: 99.75, Assm: 95.12, PNorm: 241.01, GNorm: 50.00\n",
      "[Train][9800] Alpha: 37.310, Beta: 0.000, Loss: 989.91, KL: 49957.87, MAE: 0.04544, Word Loss: 1.28, Topo Loss: 0.56, Assm Loss: 166.62, Pred Loss: 26.96, Word: 98.87, Topo: 99.74, Assm: 96.40, PNorm: 241.72, GNorm: 50.00\n",
      "[Train][9900] Alpha: 83.504, Beta: 0.000, Loss: 3951.67, KL: 887047.13, MAE: 0.05883, Word Loss: 42.93, Topo Loss: 0.47, Assm Loss: 647.74, Pred Loss: 60.20, Word: 98.60, Topo: 99.78, Assm: 95.32, PNorm: 242.44, GNorm: 50.00\n",
      "[Train][10000] Alpha: 35.655, Beta: 0.000, Loss: 10.47, KL: 7463.76, MAE: 0.01713, Word Loss: 3.68, Topo Loss: 0.62, Assm Loss: 4.59, Pred Loss: 0.03, Word: 98.62, Topo: 99.67, Assm: 97.98, PNorm: 243.10, GNorm: 48.72\n",
      "[Train][10100] Alpha: 22.113, Beta: 0.000, Loss: 7.20, KL: 1418.89, MAE: 0.01786, Word Loss: 1.33, Topo Loss: 0.53, Assm Loss: 3.19, Pred Loss: 0.09, Word: 98.67, Topo: 99.76, Assm: 98.27, PNorm: 243.75, GNorm: 45.38\n",
      "[Train][10200] Alpha: 82.030, Beta: 0.000, Loss: 28.65, KL: 13206.56, MAE: 0.01852, Word Loss: 1.86, Topo Loss: 0.77, Assm Loss: 13.29, Pred Loss: 0.33, Word: 98.74, Topo: 99.84, Assm: 97.14, PNorm: 244.33, GNorm: 50.00\n",
      "[Train][10300] Alpha: 30.919, Beta: 0.000, Loss: 352917.37, KL: 13212089.77, MAE: 0.51024, Word Loss: 102.25, Topo Loss: 22.12, Assm Loss: 2623.30, Pred Loss: 10017.24, Word: 98.63, Topo: 99.75, Assm: 95.15, PNorm: 244.94, GNorm: 50.00\n",
      "[Train][10400] Alpha: 51.303, Beta: 0.000, Loss: 910.30, KL: 219137.03, MAE: 0.03947, Word Loss: 5.62, Topo Loss: 1.52, Assm Loss: 170.57, Pred Loss: 28.44, Word: 98.82, Topo: 99.82, Assm: 95.90, PNorm: 245.53, GNorm: 46.51\n",
      "[Train][10500] Alpha: 36.293, Beta: 0.000, Loss: 2514.15, KL: 107060.15, MAE: 0.04445, Word Loss: 3.53, Topo Loss: 0.68, Assm Loss: 40.25, Pred Loss: 33.02, Word: 98.62, Topo: 99.68, Assm: 97.02, PNorm: 246.18, GNorm: 50.00\n",
      "[Train][10600] Alpha: 51.178, Beta: 0.000, Loss: 88.12, KL: 15615.41, MAE: 0.02456, Word Loss: 8.74, Topo Loss: 3.27, Assm Loss: 16.35, Pred Loss: 2.04, Word: 98.80, Topo: 99.77, Assm: 96.39, PNorm: 246.67, GNorm: 50.00\n",
      "[Train][10700] Alpha: 54.304, Beta: 0.000, Loss: 1434.46, KL: 175799.14, MAE: 0.04665, Word Loss: 19.86, Topo Loss: 3.30, Assm Loss: 105.79, Pred Loss: 53.74, Word: 98.70, Topo: 99.83, Assm: 97.05, PNorm: 247.22, GNorm: 50.00\n",
      "[Train][10800] Alpha: 32.522, Beta: 0.000, Loss: 106226.76, KL: 5856133.52, MAE: 0.26182, Word Loss: 9.74, Topo Loss: 0.53, Assm Loss: 949.22, Pred Loss: 3700.10, Word: 98.65, Topo: 99.74, Assm: 95.81, PNorm: 247.83, GNorm: 50.00\n",
      "[Train][10900] Alpha: 76.593, Beta: 0.000, Loss: 34.94, KL: 4212.58, MAE: 0.01817, Word Loss: 4.06, Topo Loss: 0.54, Assm Loss: 17.21, Pred Loss: 0.18, Word: 98.71, Topo: 99.73, Assm: 97.73, PNorm: 248.41, GNorm: 50.00\n",
      "[Train][11000] Alpha: 52.936, Beta: 0.000, Loss: 4528.12, KL: 639907.54, MAE: 0.08727, Word Loss: 45.15, Topo Loss: 1.03, Assm Loss: 529.66, Pred Loss: 114.68, Word: 98.73, Topo: 99.68, Assm: 95.75, PNorm: 249.08, GNorm: 50.00\n",
      "[Train][11100] Alpha: 65.627, Beta: 0.000, Loss: 1605.51, KL: 231725.80, MAE: 0.04681, Word Loss: 5.72, Topo Loss: 2.33, Assm Loss: 55.07, Pred Loss: 35.08, Word: 98.71, Topo: 99.77, Assm: 93.47, PNorm: 249.71, GNorm: 50.00\n",
      "[Train][11200] Alpha: 21.226, Beta: 0.000, Loss: 1340.85, KL: 428110.67, MAE: 0.05607, Word Loss: 33.49, Topo Loss: 0.79, Assm Loss: 28.76, Pred Loss: 62.02, Word: 98.95, Topo: 99.81, Assm: 97.22, PNorm: 250.34, GNorm: 31.32\n",
      "[Train][11300] Alpha: 48.997, Beta: 0.000, Loss: 41.13, KL: 8322.16, MAE: 0.01821, Word Loss: 12.15, Topo Loss: 0.75, Assm Loss: 21.21, Pred Loss: 0.16, Word: 98.49, Topo: 99.72, Assm: 96.29, PNorm: 250.91, GNorm: 50.00\n",
      "[Train][11400] Alpha: 119.339, Beta: 0.000, Loss: 944.09, KL: 126450.85, MAE: 0.04113, Word Loss: 27.66, Topo Loss: 7.51, Assm Loss: 108.29, Pred Loss: 21.28, Word: 98.76, Topo: 99.79, Assm: 97.62, PNorm: 251.52, GNorm: 50.00\n",
      "[Train][11500] Alpha: 34.415, Beta: 0.000, Loss: 1426.15, KL: 336503.41, MAE: 0.05921, Word Loss: 33.45, Topo Loss: 0.75, Assm Loss: 115.31, Pred Loss: 53.44, Word: 98.68, Topo: 99.83, Assm: 95.83, PNorm: 252.12, GNorm: 42.84\n",
      "[Train][11600] Alpha: 18.439, Beta: 0.000, Loss: 108043.53, KL: 8545432.56, MAE: 0.21290, Word Loss: 54.08, Topo Loss: 2.83, Assm Loss: 9280.63, Pred Loss: 2273.24, Word: 98.55, Topo: 99.82, Assm: 95.46, PNorm: 252.72, GNorm: 26.29\n",
      "[Train][11700] Alpha: 76.407, Beta: 0.000, Loss: 132853.18, KL: 10109514.15, MAE: 0.15856, Word Loss: 59.47, Topo Loss: 4.31, Assm Loss: 101003.75, Pred Loss: 1004.55, Word: 98.69, Topo: 99.71, Assm: 94.95, PNorm: 253.33, GNorm: 50.00\n",
      "[Train][11800] Alpha: 26.891, Beta: 0.000, Loss: 10798.83, KL: 9379151.20, MAE: 0.13174, Word Loss: 103.21, Topo Loss: 0.78, Assm Loss: 3220.26, Pred Loss: 223.69, Word: 98.54, Topo: 99.68, Assm: 95.11, PNorm: 253.91, GNorm: 50.00\n",
      "[Train][11900] Alpha: 84.688, Beta: 0.000, Loss: 899232.93, KL: 61558436.29, MAE: 0.72503, Word Loss: 256.62, Topo Loss: 0.86, Assm Loss: 22663.75, Pred Loss: 14454.25, Word: 98.61, Topo: 99.67, Assm: 95.67, PNorm: 254.55, GNorm: 50.00\n",
      "[Train][12000] Alpha: 43.840, Beta: 0.000, Loss: 1003.26, KL: 335903.74, MAE: 0.03892, Word Loss: 18.17, Topo Loss: 3.82, Assm Loss: 112.78, Pred Loss: 21.75, Word: 98.54, Topo: 99.78, Assm: 96.94, PNorm: 255.22, GNorm: 50.00\n",
      "[Train][12100] Alpha: 75.644, Beta: 0.000, Loss: 33.55, KL: 9232.55, MAE: 0.01913, Word Loss: 2.81, Topo Loss: 1.50, Assm Loss: 2.81, Pred Loss: 0.51, Word: 98.75, Topo: 99.77, Assm: 98.73, PNorm: 255.78, GNorm: 50.00\n",
      "[Train][12200] Alpha: 46.979, Beta: 0.000, Loss: 30746.58, KL: 5286714.70, MAE: 0.26642, Word Loss: 57.39, Topo Loss: 0.89, Assm Loss: 2677.66, Pred Loss: 1117.79, Word: 98.68, Topo: 99.72, Assm: 94.81, PNorm: 256.30, GNorm: 50.00\n",
      "[Train][12300] Alpha: 62.277, Beta: 0.000, Loss: 2536407.86, KL: 122115545807.01, MAE: 1.62037, Word Loss: 3773.53, Topo Loss: 187.11, Assm Loss: 229454.27, Pred Loss: 93547.35, Word: 98.84, Topo: 99.75, Assm: 95.62, PNorm: 256.83, GNorm: 50.00\n",
      "[Train][12400] Alpha: 55.815, Beta: 0.000, Loss: 395305.97, KL: 12436664.12, MAE: 0.63215, Word Loss: 165.07, Topo Loss: 9.13, Assm Loss: 8885.27, Pred Loss: 8851.79, Word: 98.79, Topo: 99.73, Assm: 96.46, PNorm: 257.40, GNorm: 50.00\n",
      "[Train][12500] Alpha: 20.241, Beta: 0.000, Loss: 130268.54, KL: 2824261.24, MAE: 0.21055, Word Loss: 68.05, Topo Loss: 1.59, Assm Loss: 2792.53, Pred Loss: 2386.38, Word: 99.03, Topo: 99.82, Assm: 96.16, PNorm: 257.87, GNorm: 42.59\n",
      "[Train][12600] Alpha: 21.661, Beta: 0.000, Loss: 669.91, KL: 6566861.22, MAE: 0.04067, Word Loss: 162.48, Topo Loss: 49.25, Assm Loss: 238.33, Pred Loss: 7.73, Word: 98.52, Topo: 99.84, Assm: 94.62, PNorm: 258.43, GNorm: 48.65\n",
      "[Train][12700] Alpha: 57.751, Beta: 0.000, Loss: 737.43, KL: 259803.57, MAE: 0.04574, Word Loss: 5.29, Topo Loss: 0.28, Assm Loss: 84.32, Pred Loss: 25.79, Word: 98.72, Topo: 99.87, Assm: 96.06, PNorm: 259.00, GNorm: 50.00\n",
      "[Validation][10] Alpha: 28.652, Beta: 0.000, Loss: 62.35, KL: 561.24, MAE: 0.06077, Word Loss: 38.18, Topo Loss: 10.42, Assm Loss: 6.72, Pred Loss: 0.25, Word: 72.49, Topo: 94.99, Assm: 76.38\n",
      "[Test][12] Alpha: 28.652, Beta: 0.000, Loss: 61.98, KL: 568.56, MAE: 0.06136, Word Loss: 39.05, Topo Loss: 9.90, Assm Loss: 5.63, Pred Loss: 0.26, Word: 71.49, Topo: 95.09, Assm: 79.02\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred(\n",
    "    loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    load_epoch=0,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=conf[\"alpha\"],\n",
    "    beta=conf[\"beta\"],\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter=conf[\"save_iter\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred_supervised(\n",
    "    loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    load_epoch=0,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=conf[\"alpha\"],\n",
    "    beta=conf[\"beta\"],\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter=conf[\"save_iter\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of google_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
