{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr:0.001\n",
    "# anneal_rate:0.9\n",
    "# batch_size:32\n",
    "# clip_norm:50\n",
    "# num_epochs:5\n",
    "# alpha:250\n",
    "# beta:0\n",
    "# max_beta:1\n",
    "# step_beta:0.002\n",
    "# anneal_iter:40000\n",
    "# kl_anneal_iter:2000\n",
    "# print_iter:100\n",
    "# save_iter:5000\n",
    "# num_workers:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "gradient": {
     "editing": false,
     "id": "18de9aeb-6551-42f6-8c11-a0e30bd207d6",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "JDinHdioUZRH",
    "outputId": "1a824742-d528-46a2-8d89-7697a166c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.12.0+cu116.html\n",
    "!pip install -q dive-into-graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q toolz\n",
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "f728113a-ff43-466d-b34d-77c9b2e11478",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "_RKgd8MsYOYh"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import pickle \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from molecule_optimizer.externals.fast_jtnn.datautils import SemiMolTreeFolder, SemiMolTreeFolderTest\n",
    "from molecule_optimizer.runner.semi_jtvae import SemiJTVAEGeneratorPredictor\n",
    "from torch_geometric.data import DenseDataLoader\n",
    "\n",
    "import rdkit\n",
    "\n",
    "lg = rdkit.RDLogger.logger() \n",
    "lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "6e85f4e2-eab6-4eae-b452-7f089e176039",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "2C4erValhPS-"
   },
   "outputs": [],
   "source": [
    "conf = json.load(open(\"training/configs/rand_gen_zinc250k_config_dict.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": true,
     "id": "30eb7f71-c38e-49f6-8d84-715ff3d80e08",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "SuzGX7ClhKaf",
    "outputId": "ab764f6b-4dd7-4ffc-b6cf-bae47d6cedf7"
   },
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"ZINC_310k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = csv['SMILES']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'runner.xml' not in os.listdir(\".\"):\n",
    "    runner = SemiJTVAEGeneratorPredictor(smiles)\n",
    "    with open('runner.xml', 'wb') as f:\n",
    "        pickle.dump(runner, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "fc02ae3d-696b-4c2e-b557-760e6a1a75a9",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "uTNMpfWD7b7Q"
   },
   "outputs": [],
   "source": [
    "with open('runner.xml', 'rb') as f:\n",
    "    runner = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(csv['LogP']).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "gradient": {
     "editing": false,
     "id": "4ccc9136-0e87-470b-90eb-8e0b92da52bc",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "0-1xXVU15z6N",
    "outputId": "c6c328fe-72e4-4401-bce6-8613e7e9319f"
   },
   "outputs": [],
   "source": [
    "runner.get_model(\n",
    "    \"rand_gen\",\n",
    "    {\n",
    "        \"hidden_size\": conf[\"model\"][\"hidden_size\"],\n",
    "        \"latent_size\": conf[\"model\"][\"latent_size\"],\n",
    "        \"depthT\": conf[\"model\"][\"depthT\"],\n",
    "        \"depthG\": conf[\"model\"][\"depthG\"],\n",
    "        \"label_size\": 1,\n",
    "        \"label_mean\": float(torch.mean(labels)),\n",
    "        \"label_var\": float(torch.var(labels)),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "c177219a-298a-4994-98b9-de76833e14fe",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "IGnpfkM_KQXi"
   },
   "outputs": [],
   "source": [
    "labels = runner.get_processed_labels(labels)\n",
    "preprocessed = runner.processed_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TEST = 10000\n",
    "VAL_FRAC = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_id=np.random.permutation(len(labels))\n",
    "X_train = preprocessed[perm_id[N_TEST:]]\n",
    "L_train = torch.tensor(labels.numpy()[perm_id[N_TEST:]])\n",
    "\n",
    "\n",
    "X_test = preprocessed[perm_id[:N_TEST]]\n",
    "L_test = torch.tensor(labels.numpy()[perm_id[:N_TEST]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = SemiMolTreeFolder(\n",
    "    X_train,\n",
    "    L_train,\n",
    "    runner.vocab,\n",
    "    conf[\"batch_size\"],\n",
    "    label_pct=0.5,\n",
    "    num_workers=conf[\"num_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "25d5a0be-b1f8-454e-bc3a-82d8489ac3ca",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "k19uZMZN9H05"
   },
   "outputs": [],
   "source": [
    "test_loader = SemiMolTreeFolderTest(\n",
    "    X_test,\n",
    "    L_test,\n",
    "    runner.vocab,\n",
    "    conf[\"batch_size\"],\n",
    "    num_workers=conf[\"num_workers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "7cb9e705-09fa-4075-a487-21887ecaeb95",
     "kernelId": "dcb13b96-53c0-4a94-b7ff-08f48ec3f7ee"
    },
    "id": "TMxgCK1Y20mu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model #Params: 5207K\n",
      "[100] Alpha: 250.000, Beta: 0.000, Loss: 286.09, KL: 69.47, MAE: 0.00712, Word Loss: 96.08, Topo Loss: 24.40, Assm Loss: 8.69, Pred Loss: 0.63, Word: 0.27, Topo: 0.80, Assm: 0.57, PNorm: 103.91, GNorm: 50.00\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 101.36, KL: 57.47, MAE: 0.00400, Word Loss: 36.76, Topo Loss: 7.62, Assm Loss: 4.76, Pred Loss: 0.21, Word: 0.41, Topo: 0.91, Assm: 0.51\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 116.44, KL: 57.59, MAE: 0.00551, Word Loss: 32.33, Topo Loss: 6.82, Assm Loss: 4.09, Pred Loss: 0.29, Word: 0.42, Topo: 0.90, Assm: 0.52\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 110.60, KL: 57.04, MAE: 0.00490, Word Loss: 32.87, Topo Loss: 8.03, Assm Loss: 4.30, Pred Loss: 0.26, Word: 0.40, Topo: 0.89, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 92.10, KL: 58.80, MAE: 0.00415, Word Loss: 34.60, Topo Loss: 6.75, Assm Loss: 3.68, Pred Loss: 0.19, Word: 0.42, Topo: 0.91, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 111.86, KL: 55.82, MAE: 0.00498, Word Loss: 34.32, Topo Loss: 6.28, Assm Loss: 4.16, Pred Loss: 0.27, Word: 0.39, Topo: 0.92, Assm: 0.55\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 125.54, KL: 54.90, MAE: 0.00513, Word Loss: 34.45, Topo Loss: 7.60, Assm Loss: 4.41, Pred Loss: 0.32, Word: 0.37, Topo: 0.89, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 88.72, KL: 52.83, MAE: 0.00423, Word Loss: 31.11, Topo Loss: 6.56, Assm Loss: 3.27, Pred Loss: 0.19, Word: 0.37, Topo: 0.89, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 84.83, KL: 58.17, MAE: 0.00413, Word Loss: 30.95, Topo Loss: 6.99, Assm Loss: 4.12, Pred Loss: 0.17, Word: 0.42, Topo: 0.90, Assm: 0.51\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 111.11, KL: 56.84, MAE: 0.00502, Word Loss: 33.29, Topo Loss: 8.40, Assm Loss: 3.80, Pred Loss: 0.26, Word: 0.39, Topo: 0.89, Assm: 0.61\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 127.13, KL: 58.28, MAE: 0.00489, Word Loss: 35.76, Topo Loss: 6.72, Assm Loss: 4.17, Pred Loss: 0.32, Word: 0.37, Topo: 0.90, Assm: 0.54\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 106.83, KL: 57.51, MAE: 0.00467, Word Loss: 32.58, Topo Loss: 6.96, Assm Loss: 4.14, Pred Loss: 0.25, Word: 0.42, Topo: 0.91, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 141.34, KL: 59.99, MAE: 0.00541, Word Loss: 35.67, Topo Loss: 7.50, Assm Loss: 3.91, Pred Loss: 0.38, Word: 0.41, Topo: 0.91, Assm: 0.66\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 108.10, KL: 58.00, MAE: 0.00443, Word Loss: 35.24, Topo Loss: 7.00, Assm Loss: 4.33, Pred Loss: 0.25, Word: 0.39, Topo: 0.91, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 140.60, KL: 55.58, MAE: 0.00636, Word Loss: 33.77, Topo Loss: 7.50, Assm Loss: 3.90, Pred Loss: 0.38, Word: 0.43, Topo: 0.89, Assm: 0.66\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 112.52, KL: 57.81, MAE: 0.00494, Word Loss: 31.85, Topo Loss: 8.25, Assm Loss: 4.37, Pred Loss: 0.27, Word: 0.42, Topo: 0.89, Assm: 0.59\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 118.62, KL: 58.41, MAE: 0.00484, Word Loss: 34.70, Topo Loss: 7.53, Assm Loss: 3.87, Pred Loss: 0.29, Word: 0.40, Topo: 0.89, Assm: 0.69\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 146.11, KL: 59.83, MAE: 0.00604, Word Loss: 34.50, Topo Loss: 6.83, Assm Loss: 4.15, Pred Loss: 0.40, Word: 0.44, Topo: 0.91, Assm: 0.59\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 92.11, KL: 57.20, MAE: 0.00404, Word Loss: 35.57, Topo Loss: 6.87, Assm Loss: 3.80, Pred Loss: 0.18, Word: 0.40, Topo: 0.90, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 140.62, KL: 56.66, MAE: 0.00617, Word Loss: 32.26, Topo Loss: 6.60, Assm Loss: 3.85, Pred Loss: 0.39, Word: 0.42, Topo: 0.91, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 117.29, KL: 55.77, MAE: 0.00506, Word Loss: 35.41, Topo Loss: 7.06, Assm Loss: 3.44, Pred Loss: 0.29, Word: 0.39, Topo: 0.90, Assm: 0.71\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 106.42, KL: 58.25, MAE: 0.00471, Word Loss: 34.58, Topo Loss: 7.08, Assm Loss: 4.74, Pred Loss: 0.24, Word: 0.37, Topo: 0.89, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 141.75, KL: 56.29, MAE: 0.00590, Word Loss: 34.78, Topo Loss: 6.96, Assm Loss: 4.29, Pred Loss: 0.38, Word: 0.39, Topo: 0.89, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 103.59, KL: 58.12, MAE: 0.00467, Word Loss: 32.67, Topo Loss: 6.93, Assm Loss: 3.96, Pred Loss: 0.24, Word: 0.44, Topo: 0.91, Assm: 0.61\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 112.68, KL: 57.94, MAE: 0.00447, Word Loss: 35.36, Topo Loss: 8.95, Assm Loss: 4.76, Pred Loss: 0.25, Word: 0.40, Topo: 0.89, Assm: 0.56\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 122.70, KL: 58.18, MAE: 0.00509, Word Loss: 36.80, Topo Loss: 7.65, Assm Loss: 3.87, Pred Loss: 0.30, Word: 0.38, Topo: 0.90, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 119.41, KL: 56.74, MAE: 0.00519, Word Loss: 31.75, Topo Loss: 6.36, Assm Loss: 3.95, Pred Loss: 0.31, Word: 0.41, Topo: 0.91, Assm: 0.66\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 74.03, KL: 57.83, MAE: 0.00343, Word Loss: 32.69, Topo Loss: 6.68, Assm Loss: 4.61, Pred Loss: 0.12, Word: 0.39, Topo: 0.90, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 135.28, KL: 53.84, MAE: 0.00528, Word Loss: 32.17, Topo Loss: 6.31, Assm Loss: 4.65, Pred Loss: 0.37, Word: 0.37, Topo: 0.90, Assm: 0.49\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 106.05, KL: 55.73, MAE: 0.00493, Word Loss: 32.56, Topo Loss: 6.66, Assm Loss: 4.77, Pred Loss: 0.25, Word: 0.41, Topo: 0.91, Assm: 0.53\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 113.84, KL: 58.72, MAE: 0.00492, Word Loss: 33.91, Topo Loss: 7.11, Assm Loss: 3.64, Pred Loss: 0.28, Word: 0.42, Topo: 0.91, Assm: 0.63\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 135.05, KL: 59.70, MAE: 0.00577, Word Loss: 35.67, Topo Loss: 7.46, Assm Loss: 4.01, Pred Loss: 0.35, Word: 0.41, Topo: 0.90, Assm: 0.59\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 134.51, KL: 55.78, MAE: 0.00599, Word Loss: 34.13, Topo Loss: 6.75, Assm Loss: 4.11, Pred Loss: 0.36, Word: 0.41, Topo: 0.90, Assm: 0.56\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 130.87, KL: 55.02, MAE: 0.00590, Word Loss: 35.73, Topo Loss: 7.51, Assm Loss: 4.28, Pred Loss: 0.33, Word: 0.37, Topo: 0.89, Assm: 0.59\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 121.45, KL: 54.81, MAE: 0.00554, Word Loss: 31.61, Topo Loss: 6.09, Assm Loss: 4.01, Pred Loss: 0.32, Word: 0.38, Topo: 0.91, Assm: 0.51\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 127.91, KL: 57.17, MAE: 0.00523, Word Loss: 31.35, Topo Loss: 8.23, Assm Loss: 4.09, Pred Loss: 0.34, Word: 0.43, Topo: 0.90, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 108.88, KL: 54.56, MAE: 0.00498, Word Loss: 33.46, Topo Loss: 6.04, Assm Loss: 3.33, Pred Loss: 0.26, Word: 0.39, Topo: 0.92, Assm: 0.66\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 174.46, KL: 57.51, MAE: 0.00685, Word Loss: 34.78, Topo Loss: 7.31, Assm Loss: 3.73, Pred Loss: 0.51, Word: 0.38, Topo: 0.91, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 102.32, KL: 58.32, MAE: 0.00446, Word Loss: 35.62, Topo Loss: 7.15, Assm Loss: 4.46, Pred Loss: 0.22, Word: 0.39, Topo: 0.91, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 148.17, KL: 57.90, MAE: 0.00637, Word Loss: 33.52, Topo Loss: 6.99, Assm Loss: 3.88, Pred Loss: 0.42, Word: 0.38, Topo: 0.91, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 115.63, KL: 58.43, MAE: 0.00539, Word Loss: 33.45, Topo Loss: 8.16, Assm Loss: 3.38, Pred Loss: 0.28, Word: 0.42, Topo: 0.89, Assm: 0.63\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 120.35, KL: 58.28, MAE: 0.00527, Word Loss: 34.93, Topo Loss: 6.98, Assm Loss: 3.78, Pred Loss: 0.30, Word: 0.37, Topo: 0.91, Assm: 0.70\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 112.76, KL: 58.37, MAE: 0.00487, Word Loss: 33.98, Topo Loss: 6.39, Assm Loss: 3.80, Pred Loss: 0.27, Word: 0.42, Topo: 0.91, Assm: 0.64\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 117.22, KL: 54.21, MAE: 0.00546, Word Loss: 34.50, Topo Loss: 6.80, Assm Loss: 3.55, Pred Loss: 0.29, Word: 0.40, Topo: 0.91, Assm: 0.63\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 148.17, KL: 58.06, MAE: 0.00612, Word Loss: 34.94, Topo Loss: 7.62, Assm Loss: 3.98, Pred Loss: 0.41, Word: 0.40, Topo: 0.90, Assm: 0.56\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 121.28, KL: 55.75, MAE: 0.00538, Word Loss: 34.69, Topo Loss: 6.94, Assm Loss: 3.73, Pred Loss: 0.30, Word: 0.38, Topo: 0.91, Assm: 0.70\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 102.24, KL: 55.37, MAE: 0.00446, Word Loss: 30.73, Topo Loss: 6.47, Assm Loss: 3.89, Pred Loss: 0.24, Word: 0.41, Topo: 0.91, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 110.87, KL: 55.84, MAE: 0.00477, Word Loss: 32.77, Topo Loss: 6.78, Assm Loss: 4.24, Pred Loss: 0.27, Word: 0.38, Topo: 0.91, Assm: 0.63\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 105.66, KL: 59.42, MAE: 0.00490, Word Loss: 34.50, Topo Loss: 8.30, Assm Loss: 4.45, Pred Loss: 0.23, Word: 0.38, Topo: 0.90, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 103.28, KL: 55.28, MAE: 0.00431, Word Loss: 35.85, Topo Loss: 8.04, Assm Loss: 4.35, Pred Loss: 0.22, Word: 0.40, Topo: 0.89, Assm: 0.63\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 117.43, KL: 55.26, MAE: 0.00534, Word Loss: 34.20, Topo Loss: 5.92, Assm Loss: 3.78, Pred Loss: 0.29, Word: 0.38, Topo: 0.91, Assm: 0.64\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 122.42, KL: 54.88, MAE: 0.00516, Word Loss: 32.76, Topo Loss: 8.30, Assm Loss: 4.81, Pred Loss: 0.31, Word: 0.37, Topo: 0.90, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 129.93, KL: 57.58, MAE: 0.00538, Word Loss: 33.19, Topo Loss: 6.72, Assm Loss: 3.81, Pred Loss: 0.34, Word: 0.39, Topo: 0.89, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 122.52, KL: 59.15, MAE: 0.00564, Word Loss: 31.92, Topo Loss: 6.29, Assm Loss: 3.47, Pred Loss: 0.32, Word: 0.42, Topo: 0.90, Assm: 0.61\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 100.46, KL: 55.70, MAE: 0.00438, Word Loss: 32.80, Topo Loss: 6.60, Assm Loss: 4.40, Pred Loss: 0.23, Word: 0.41, Topo: 0.90, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 132.14, KL: 56.44, MAE: 0.00594, Word Loss: 33.19, Topo Loss: 7.79, Assm Loss: 4.18, Pred Loss: 0.35, Word: 0.39, Topo: 0.90, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 90.32, KL: 60.29, MAE: 0.00391, Word Loss: 35.17, Topo Loss: 8.71, Assm Loss: 4.16, Pred Loss: 0.17, Word: 0.41, Topo: 0.88, Assm: 0.59\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 115.21, KL: 57.40, MAE: 0.00524, Word Loss: 33.51, Topo Loss: 6.87, Assm Loss: 3.78, Pred Loss: 0.28, Word: 0.43, Topo: 0.91, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 120.43, KL: 55.26, MAE: 0.00514, Word Loss: 36.07, Topo Loss: 7.37, Assm Loss: 3.73, Pred Loss: 0.29, Word: 0.37, Topo: 0.90, Assm: 0.55\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 100.04, KL: 54.38, MAE: 0.00465, Word Loss: 30.86, Topo Loss: 7.10, Assm Loss: 4.37, Pred Loss: 0.23, Word: 0.42, Topo: 0.90, Assm: 0.56\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 83.14, KL: 56.58, MAE: 0.00411, Word Loss: 31.43, Topo Loss: 6.53, Assm Loss: 4.41, Pred Loss: 0.16, Word: 0.40, Topo: 0.90, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 146.31, KL: 54.32, MAE: 0.00577, Word Loss: 33.12, Topo Loss: 8.11, Assm Loss: 4.78, Pred Loss: 0.40, Word: 0.36, Topo: 0.88, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 99.81, KL: 56.63, MAE: 0.00433, Word Loss: 31.64, Topo Loss: 6.91, Assm Loss: 3.90, Pred Loss: 0.23, Word: 0.43, Topo: 0.90, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 133.40, KL: 56.07, MAE: 0.00611, Word Loss: 32.67, Topo Loss: 7.66, Assm Loss: 4.40, Pred Loss: 0.35, Word: 0.41, Topo: 0.88, Assm: 0.56\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 119.72, KL: 58.24, MAE: 0.00494, Word Loss: 34.84, Topo Loss: 7.31, Assm Loss: 3.60, Pred Loss: 0.30, Word: 0.41, Topo: 0.91, Assm: 0.67\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 136.06, KL: 56.28, MAE: 0.00552, Word Loss: 33.98, Topo Loss: 7.02, Assm Loss: 3.36, Pred Loss: 0.37, Word: 0.42, Topo: 0.92, Assm: 0.63\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 124.54, KL: 60.53, MAE: 0.00550, Word Loss: 33.58, Topo Loss: 7.73, Assm Loss: 4.23, Pred Loss: 0.32, Word: 0.45, Topo: 0.89, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 103.11, KL: 56.67, MAE: 0.00474, Word Loss: 33.15, Topo Loss: 7.98, Assm Loss: 3.30, Pred Loss: 0.23, Word: 0.40, Topo: 0.90, Assm: 0.64\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 97.88, KL: 54.16, MAE: 0.00447, Word Loss: 30.56, Topo Loss: 6.83, Assm Loss: 3.97, Pred Loss: 0.23, Word: 0.41, Topo: 0.91, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 96.77, KL: 57.01, MAE: 0.00452, Word Loss: 33.83, Topo Loss: 7.45, Assm Loss: 3.88, Pred Loss: 0.21, Word: 0.42, Topo: 0.89, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 150.98, KL: 58.57, MAE: 0.00659, Word Loss: 37.69, Topo Loss: 8.31, Assm Loss: 4.04, Pred Loss: 0.40, Word: 0.40, Topo: 0.90, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 109.68, KL: 60.99, MAE: 0.00485, Word Loss: 36.84, Topo Loss: 8.65, Assm Loss: 4.22, Pred Loss: 0.24, Word: 0.42, Topo: 0.90, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 154.73, KL: 57.36, MAE: 0.00574, Word Loss: 40.38, Topo Loss: 8.36, Assm Loss: 4.17, Pred Loss: 0.41, Word: 0.37, Topo: 0.90, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 102.93, KL: 58.20, MAE: 0.00445, Word Loss: 33.95, Topo Loss: 7.40, Assm Loss: 3.89, Pred Loss: 0.23, Word: 0.41, Topo: 0.90, Assm: 0.55\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 114.11, KL: 57.17, MAE: 0.00517, Word Loss: 32.11, Topo Loss: 7.75, Assm Loss: 4.21, Pred Loss: 0.28, Word: 0.44, Topo: 0.89, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 144.81, KL: 57.27, MAE: 0.00622, Word Loss: 33.05, Topo Loss: 7.83, Assm Loss: 4.32, Pred Loss: 0.40, Word: 0.41, Topo: 0.90, Assm: 0.56\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 105.18, KL: 58.39, MAE: 0.00434, Word Loss: 33.32, Topo Loss: 7.05, Assm Loss: 3.67, Pred Loss: 0.24, Word: 0.42, Topo: 0.90, Assm: 0.53\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 100.48, KL: 53.10, MAE: 0.00460, Word Loss: 34.80, Topo Loss: 6.68, Assm Loss: 4.41, Pred Loss: 0.22, Word: 0.37, Topo: 0.89, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 149.73, KL: 58.65, MAE: 0.00615, Word Loss: 35.64, Topo Loss: 8.23, Assm Loss: 5.34, Pred Loss: 0.40, Word: 0.38, Topo: 0.90, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 101.34, KL: 55.26, MAE: 0.00452, Word Loss: 31.04, Topo Loss: 7.16, Assm Loss: 5.09, Pred Loss: 0.23, Word: 0.38, Topo: 0.89, Assm: 0.48\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 124.04, KL: 57.08, MAE: 0.00533, Word Loss: 32.68, Topo Loss: 6.79, Assm Loss: 3.60, Pred Loss: 0.32, Word: 0.41, Topo: 0.90, Assm: 0.53\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 130.07, KL: 59.17, MAE: 0.00580, Word Loss: 34.42, Topo Loss: 7.16, Assm Loss: 3.81, Pred Loss: 0.34, Word: 0.39, Topo: 0.91, Assm: 0.64\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 132.46, KL: 57.75, MAE: 0.00524, Word Loss: 34.28, Topo Loss: 8.02, Assm Loss: 3.79, Pred Loss: 0.35, Word: 0.38, Topo: 0.89, Assm: 0.54\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 116.03, KL: 57.82, MAE: 0.00536, Word Loss: 34.54, Topo Loss: 7.87, Assm Loss: 4.09, Pred Loss: 0.28, Word: 0.37, Topo: 0.89, Assm: 0.49\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 108.75, KL: 54.94, MAE: 0.00491, Word Loss: 32.63, Topo Loss: 6.32, Assm Loss: 4.29, Pred Loss: 0.26, Word: 0.38, Topo: 0.90, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 95.51, KL: 58.33, MAE: 0.00418, Word Loss: 33.97, Topo Loss: 6.99, Assm Loss: 4.32, Pred Loss: 0.20, Word: 0.41, Topo: 0.91, Assm: 0.61\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 101.23, KL: 57.60, MAE: 0.00436, Word Loss: 35.50, Topo Loss: 7.15, Assm Loss: 4.72, Pred Loss: 0.22, Word: 0.42, Topo: 0.89, Assm: 0.59\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 105.27, KL: 54.59, MAE: 0.00476, Word Loss: 32.24, Topo Loss: 6.17, Assm Loss: 4.19, Pred Loss: 0.25, Word: 0.39, Topo: 0.91, Assm: 0.54\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 102.20, KL: 56.89, MAE: 0.00484, Word Loss: 31.88, Topo Loss: 7.04, Assm Loss: 3.65, Pred Loss: 0.24, Word: 0.42, Topo: 0.91, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 123.00, KL: 58.53, MAE: 0.00548, Word Loss: 34.25, Topo Loss: 6.90, Assm Loss: 3.77, Pred Loss: 0.31, Word: 0.42, Topo: 0.92, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 96.06, KL: 53.80, MAE: 0.00419, Word Loss: 33.50, Topo Loss: 6.85, Assm Loss: 4.28, Pred Loss: 0.21, Word: 0.35, Topo: 0.89, Assm: 0.59\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 118.54, KL: 56.64, MAE: 0.00470, Word Loss: 35.98, Topo Loss: 9.54, Assm Loss: 4.11, Pred Loss: 0.28, Word: 0.37, Topo: 0.88, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 172.69, KL: 56.78, MAE: 0.00610, Word Loss: 34.77, Topo Loss: 6.65, Assm Loss: 3.43, Pred Loss: 0.51, Word: 0.41, Topo: 0.91, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 145.54, KL: 54.39, MAE: 0.00620, Word Loss: 32.82, Topo Loss: 6.89, Assm Loss: 4.30, Pred Loss: 0.41, Word: 0.41, Topo: 0.91, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 118.23, KL: 54.61, MAE: 0.00549, Word Loss: 30.69, Topo Loss: 7.45, Assm Loss: 4.13, Pred Loss: 0.30, Word: 0.40, Topo: 0.89, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 165.41, KL: 56.03, MAE: 0.00652, Word Loss: 34.90, Topo Loss: 7.76, Assm Loss: 3.99, Pred Loss: 0.48, Word: 0.37, Topo: 0.90, Assm: 0.63\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 121.12, KL: 57.95, MAE: 0.00524, Word Loss: 33.89, Topo Loss: 8.58, Assm Loss: 3.83, Pred Loss: 0.30, Word: 0.42, Topo: 0.89, Assm: 0.53\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 112.78, KL: 54.74, MAE: 0.00492, Word Loss: 33.38, Topo Loss: 7.02, Assm Loss: 4.72, Pred Loss: 0.27, Word: 0.36, Topo: 0.90, Assm: 0.51\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 138.25, KL: 58.34, MAE: 0.00588, Word Loss: 33.70, Topo Loss: 7.35, Assm Loss: 4.29, Pred Loss: 0.37, Word: 0.42, Topo: 0.88, Assm: 0.56\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 116.08, KL: 56.77, MAE: 0.00505, Word Loss: 32.86, Topo Loss: 6.96, Assm Loss: 3.88, Pred Loss: 0.29, Word: 0.41, Topo: 0.90, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 122.86, KL: 58.07, MAE: 0.00554, Word Loss: 33.62, Topo Loss: 7.30, Assm Loss: 3.94, Pred Loss: 0.31, Word: 0.40, Topo: 0.91, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 111.09, KL: 58.23, MAE: 0.00446, Word Loss: 33.37, Topo Loss: 8.87, Assm Loss: 4.49, Pred Loss: 0.26, Word: 0.42, Topo: 0.87, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 115.52, KL: 56.11, MAE: 0.00525, Word Loss: 32.89, Topo Loss: 6.67, Assm Loss: 4.82, Pred Loss: 0.28, Word: 0.39, Topo: 0.90, Assm: 0.52\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 120.39, KL: 56.22, MAE: 0.00509, Word Loss: 33.11, Topo Loss: 7.24, Assm Loss: 4.19, Pred Loss: 0.30, Word: 0.41, Topo: 0.89, Assm: 0.61\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 128.02, KL: 55.92, MAE: 0.00547, Word Loss: 34.60, Topo Loss: 6.87, Assm Loss: 4.76, Pred Loss: 0.33, Word: 0.38, Topo: 0.90, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 128.76, KL: 56.82, MAE: 0.00554, Word Loss: 34.97, Topo Loss: 7.56, Assm Loss: 4.96, Pred Loss: 0.33, Word: 0.39, Topo: 0.88, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 95.98, KL: 55.66, MAE: 0.00449, Word Loss: 31.24, Topo Loss: 6.52, Assm Loss: 3.85, Pred Loss: 0.22, Word: 0.43, Topo: 0.91, Assm: 0.58\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 125.20, KL: 56.18, MAE: 0.00526, Word Loss: 31.57, Topo Loss: 6.57, Assm Loss: 3.87, Pred Loss: 0.33, Word: 0.39, Topo: 0.89, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 163.82, KL: 58.23, MAE: 0.00698, Word Loss: 31.55, Topo Loss: 6.99, Assm Loss: 3.25, Pred Loss: 0.49, Word: 0.44, Topo: 0.90, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 101.59, KL: 59.98, MAE: 0.00431, Word Loss: 32.78, Topo Loss: 7.71, Assm Loss: 3.43, Pred Loss: 0.23, Word: 0.42, Topo: 0.90, Assm: 0.68\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 124.42, KL: 59.15, MAE: 0.00477, Word Loss: 37.03, Topo Loss: 7.00, Assm Loss: 4.27, Pred Loss: 0.30, Word: 0.39, Topo: 0.92, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 115.74, KL: 59.12, MAE: 0.00511, Word Loss: 34.52, Topo Loss: 6.74, Assm Loss: 3.50, Pred Loss: 0.28, Word: 0.40, Topo: 0.90, Assm: 0.66\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 146.93, KL: 62.04, MAE: 0.00620, Word Loss: 33.18, Topo Loss: 7.46, Assm Loss: 3.66, Pred Loss: 0.41, Word: 0.44, Topo: 0.91, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 103.91, KL: 57.55, MAE: 0.00431, Word Loss: 37.66, Topo Loss: 7.94, Assm Loss: 4.03, Pred Loss: 0.22, Word: 0.42, Topo: 0.90, Assm: 0.63\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 94.43, KL: 57.27, MAE: 0.00420, Word Loss: 32.01, Topo Loss: 7.38, Assm Loss: 4.09, Pred Loss: 0.20, Word: 0.41, Topo: 0.89, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 155.83, KL: 58.17, MAE: 0.00655, Word Loss: 33.72, Topo Loss: 6.88, Assm Loss: 3.70, Pred Loss: 0.45, Word: 0.42, Topo: 0.91, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 120.41, KL: 56.09, MAE: 0.00529, Word Loss: 34.83, Topo Loss: 6.54, Assm Loss: 3.27, Pred Loss: 0.30, Word: 0.40, Topo: 0.91, Assm: 0.67\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 115.33, KL: 56.45, MAE: 0.00482, Word Loss: 34.63, Topo Loss: 8.22, Assm Loss: 4.37, Pred Loss: 0.27, Word: 0.41, Topo: 0.88, Assm: 0.59\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 92.88, KL: 57.65, MAE: 0.00439, Word Loss: 32.71, Topo Loss: 7.38, Assm Loss: 3.66, Pred Loss: 0.20, Word: 0.40, Topo: 0.90, Assm: 0.62\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 108.53, KL: 57.04, MAE: 0.00471, Word Loss: 33.63, Topo Loss: 7.89, Assm Loss: 4.92, Pred Loss: 0.25, Word: 0.41, Topo: 0.90, Assm: 0.54\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 112.85, KL: 54.57, MAE: 0.00515, Word Loss: 32.36, Topo Loss: 6.53, Assm Loss: 4.77, Pred Loss: 0.28, Word: 0.38, Topo: 0.90, Assm: 0.56\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 116.49, KL: 57.34, MAE: 0.00515, Word Loss: 32.38, Topo Loss: 6.97, Assm Loss: 3.57, Pred Loss: 0.29, Word: 0.40, Topo: 0.90, Assm: 0.64\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 119.03, KL: 55.72, MAE: 0.00490, Word Loss: 33.55, Topo Loss: 6.79, Assm Loss: 3.86, Pred Loss: 0.30, Word: 0.40, Topo: 0.90, Assm: 0.57\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 122.58, KL: 57.53, MAE: 0.00531, Word Loss: 34.63, Topo Loss: 7.55, Assm Loss: 4.82, Pred Loss: 0.30, Word: 0.37, Topo: 0.89, Assm: 0.55\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 102.04, KL: 61.77, MAE: 0.00433, Word Loss: 37.14, Topo Loss: 8.82, Assm Loss: 3.98, Pred Loss: 0.21, Word: 0.39, Topo: 0.88, Assm: 0.65\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 137.31, KL: 60.01, MAE: 0.00603, Word Loss: 37.02, Topo Loss: 7.35, Assm Loss: 3.53, Pred Loss: 0.36, Word: 0.39, Topo: 0.91, Assm: 0.68\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 121.32, KL: 58.32, MAE: 0.00536, Word Loss: 35.43, Topo Loss: 6.56, Assm Loss: 4.17, Pred Loss: 0.30, Word: 0.41, Topo: 0.93, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 112.24, KL: 57.54, MAE: 0.00464, Word Loss: 35.75, Topo Loss: 6.91, Assm Loss: 3.37, Pred Loss: 0.26, Word: 0.40, Topo: 0.92, Assm: 0.68\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 98.15, KL: 58.14, MAE: 0.00439, Word Loss: 32.16, Topo Loss: 7.13, Assm Loss: 3.69, Pred Loss: 0.22, Word: 0.42, Topo: 0.89, Assm: 0.60\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 107.82, KL: 59.65, MAE: 0.00501, Word Loss: 33.43, Topo Loss: 8.50, Assm Loss: 4.01, Pred Loss: 0.25, Word: 0.42, Topo: 0.89, Assm: 0.61\n",
      "[Test] Alpha: 250.000, Beta: 0.000, Loss: 93.32, KL: 56.84, MAE: 0.00419, Word Loss: 33.15, Topo Loss: 7.72, Assm Loss: 3.40, Pred Loss: 0.20, Word: 0.45, Topo: 0.89, Assm: 0.66\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "runner.train_gen_pred(\n",
    "    loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    load_epoch=0,\n",
    "    lr=conf[\"lr\"],\n",
    "    anneal_rate=conf[\"anneal_rate\"],\n",
    "    clip_norm=conf[\"clip_norm\"],\n",
    "    num_epochs=conf[\"num_epochs\"],\n",
    "    alpha=conf[\"alpha\"],\n",
    "    beta=conf[\"beta\"],\n",
    "    max_beta=conf[\"max_beta\"],\n",
    "    step_beta=conf[\"step_beta\"],\n",
    "    anneal_iter=conf[\"anneal_iter\"],\n",
    "    kl_anneal_iter=conf[\"kl_anneal_iter\"],\n",
    "    print_iter=100,\n",
    "    save_iter=conf[\"save_iter\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of google_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
